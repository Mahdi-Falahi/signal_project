{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "  <a href=\"http://www.sharif.edu/\">\n",
    "    <img src=\"https://cdn.freebiesupply.com/logos/large/2x/sharif-logo-png-transparent.png\" alt=\"SUT Logo\" width=\"140\">\n",
    "  </a>\n",
    "  \n",
    "  # Sharif University of Technology\n",
    "  ### Electrical Engineering Department\n",
    "\n",
    "  ## Signals and Systems\n",
    "  #### *Final Project - Spring 2025*\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "<div align=\"center\">\n",
    "  <h1>\n",
    "    <b>Object Tracker</b>\n",
    "  </h1>\n",
    "  <p>\n",
    "    An object tracking system using YOLO for detection and various algorithms (KCF, CSRT, MOSSE) for tracking.\n",
    "  </p>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "| Professor                  |\n",
    "| :-------------------------: |\n",
    "| Dr. Mohammad Mehdi Mojahedian |\n",
    "\n",
    "<br>\n",
    "\n",
    "| Contributors              |\n",
    "| :-----------------------: |\n",
    "| **Amirreza Mousavi** |\n",
    "| **Mahdi Falahi** |\n",
    "| **Zahra Miladipour** |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1: Preparing The Materials\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 : Calculating HOG ( return the hog of the image)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hog (image):\n",
    "    # w , h = image.shape()\n",
    "    filter = cv2.HOGDescriptor()\n",
    "    result = filter.compute(image)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 : Checking The Scale (return the scale of the image in the current frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_check (image ,h, source_hog):\n",
    "    w , h = image.shape()\n",
    "    n = np.arange(0.9 , 1.2 , 0.05)\n",
    "    source = np.fft.fft2(source_hog)\n",
    "    h_fft = np.fft.fft2(h)\n",
    "    x = h_fft * source\n",
    "    scale = 1\n",
    "    min = 500\n",
    "    for coefficient in n :\n",
    "        new_img = cv2.resize(image , coefficient * w , coefficient * h)\n",
    "        new_fft = np.fft.fft2(new_img)\n",
    "        new_result = new_fft * h_fft\n",
    "        diff = new_result - x\n",
    "        sum = np.sum(diff ** 2)\n",
    "        if (sum < min ):\n",
    "            min = sum\n",
    "            scale = coefficient \n",
    "    return scale    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3 : Prediction The Next Frame's Center ( Kalman Filter )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kalman_prediction( F , X_k_1 , P_k_1 , Q_k):\n",
    "    x_k = np.dot(F , X_k_1)\n",
    "    p_k = np.dot( F , np.dot(P_k_1 , F.T)) + Q_k\n",
    "    return x_k , p_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kalman_updating(x_k , p_k , H_k , z_k , R_k):\n",
    "    k_1 = np.dot(np.dot(H_k , p_k) , H_k.T) + R_k\n",
    "    k_2 = np.dot(p_k , H_k.T)\n",
    "    K = np.dot(k_2 , np.linalg.inv(k_1))\n",
    "    P_k_new = p_k - np.dot(np.dot(K , H_k) , p_k)\n",
    "    x_k_new = x_k + np.dot(K , (z_k - np.dot(H_k , x_k)))\n",
    "    return x_k_new , P_k_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.4 : Updating Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_updating(H_new , H_old , alpha):\n",
    "    result = alpha * H_new + (1-alpha) * H_old\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.5 : Finding The Channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_channels(image):\n",
    "    colors = np.array([\n",
    "    [0.00, 0.00, 0.00], [45.37, -4.33, -33.43], [43.08, 17.51, 37.53],\n",
    "    [53.59, 0.00, 0.00], [47.31, -45.33, 41.35], [65.75, 71.45, 63.32],\n",
    "    [76.08, 22.25, -21.46], [32.30, 79.19, -107.86], [52.23, 75.43, 37.36],\n",
    "    [100.00, 0.00, 0.00], [92.13, -16.53, 93.35]\n",
    "], dtype=np.float32)\n",
    "    image_lab = cv2.cvtColor(image, cv2.COLOR_BGR2Lab)\n",
    "    h, w, _ = image_lab.shape\n",
    "    num_colors = colors.shape[0]\n",
    "    \n",
    "    new = image_lab.reshape(-1, 3).astype(np.float32)\n",
    "    distances = np.sum((new[:, np.newaxis, :] - colors[np.newaxis, :, :]) ** 2, axis=2)\n",
    "    closest_color_indices = np.argmin(distances, axis=1)\n",
    "    \n",
    "    cn_features_flat = np.zeros((new.shape[0], num_colors), dtype=np.float32)\n",
    "    cn_features_flat[np.arange(new.shape[0]), closest_color_indices] = 1\n",
    "    \n",
    "    cn_features = cn_features_flat.reshape(h, w, num_colors)\n",
    "    return cn_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.6 : Teaching The Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def teaching ( f , g , lambda_trust):\n",
    "    X = np.fft.fft2(f , axis = (0 ,1 ))\n",
    "    G = np.fft.fft2(g)\n",
    "    num = np.dot(X , G)\n",
    "    denom = np.sum(np.dot(np.conj(X) , X) , axis =2) + lambda_trust\n",
    "    H = num / denom \n",
    "    return H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 : Main Detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 : Uploading Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video has ended or failed to read frame.\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture('person1.mp4')\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        cv2.imshow(\"capture\", frame)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "    key = cv2.waitKey(4)\n",
    "    if key == 27 :\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 : The Main Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cap' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m i = \u001b[32m0\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     ret , frame = \u001b[43mcap\u001b[49m.read()\n\u001b[32m      4\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ( i == \u001b[32m0\u001b[39m):\n\u001b[32m      5\u001b[39m         result = model(frame)[\u001b[32m0\u001b[39m]\n",
      "\u001b[31mNameError\u001b[39m: name 'cap' is not defined"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "while True:\n",
    "    ret , frame = cap.read()\n",
    "    if ( i == 0):\n",
    "        result = model(frame)[0]\n",
    "        for box in result.boxes:\n",
    "            class_name = model_n.names[int(box.cls[0].item())]\n",
    "            coords = box.xyxy[0].tolist()\n",
    "            x1, y1, x2, y2 = map(int, coords)\n",
    "            width = x2 - x1\n",
    "            height = y2 - y1\n",
    "            conf = box.conf[0].item()\n",
    "            if(class_name == \"person\"):\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (100, 200, 100), 3)\n",
    "            #  now we detected the first image\n",
    "    i += 1\n",
    "    \n",
    "    #//// the rest will be here  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
