{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "  <a href=\"http://www.sharif.edu/\">\n",
    "    <img src=\"https://cdn.freebiesupply.com/logos/large/2x/sharif-logo-png-transparent.png\" alt=\"SUT Logo\" width=\"140\">\n",
    "  </a>\n",
    "  \n",
    "  # Sharif University of Technology\n",
    "  ### Electrical Engineering Department\n",
    "\n",
    "  ## Signals and Systems\n",
    "  #### *Final Project - Spring 2025*\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "<div align=\"center\">\n",
    "  <h1>\n",
    "    <b>Object Tracker</b>\n",
    "  </h1>\n",
    "  <p>\n",
    "    An object tracking system using YOLO for detection and various algorithms (KCF, CSRT, MOSSE) for tracking.\n",
    "  </p>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "| Professor                  |\n",
    "| :-------------------------: |\n",
    "| Dr. Mohammad Mehdi Mojahedian |\n",
    "\n",
    "<br>\n",
    "\n",
    "| Contributors              |\n",
    "| :-----------------------: |\n",
    "| **Amirreza Mousavi** |\n",
    "| **Mahdi Falahi** |\n",
    "| **Zahra Miladipour** |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1: Preparing The Materials\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 : Calculating HOG ( return the hog of the image)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hog_scaling (image):\n",
    "    img = cv2.cvtColor(image ,cv2.COLOR_BGR2GRAY)\n",
    "    filter = cv2.HOGDescriptor((64, 64), (16, 16), (8, 8), (8, 8), 9)\n",
    "    resized_image = cv2.resize(img, (64, 64))\n",
    "    result = filter.compute(resized_image)\n",
    "    return result if result is not None else np.zeros((hog_descriptor.getDescriptorSize(1764),))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hog_channel(image):\n",
    "    img = cv2.cvtColor(image , cv2.COLOR_BGR2GRAY)\n",
    "    win_size = (image.shape[1], image.shape[0])\n",
    "    filter = cv2.HOGDescriptor(win_size , (16 ,16 ) , (8,8) , (8,8) , 9)\n",
    "    result = filter.compute(img)\n",
    "    height =  (win_size[1] - 16) // 8 + 1    # 8 is the block strid(x) we can change that consider the trade off of time _ accuracy\n",
    "    width = (win_size[0] - 16) // 8 + 1   # 8 is the block strid(y) we can change that consider the trade off of time _ accuracy\n",
    "    features_per_block = 2 * 2 * 9\n",
    "    hog_features = result.reshape((height, width, features_per_block))\n",
    "    return cv2.resize(hog_features, (win_size[0] // 8, win_size[1] // 8))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 : Checking The Scale (return the scale of the image in the current frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_check(frame, pos, base_size, scale_factors, scale_model_A, scale_model_B, lambda_trust=0.01):\n",
    "    scale_features = []\n",
    "    for scale in scale_factors:\n",
    "        w_s, h_s = int(base_size[0] * scale), int(base_size[1] * scale)\n",
    "        x_s, y_s = int(pos[0] - w_s / 2), int(pos[1] - h_s / 2)\n",
    "        patch_s = frame[y_s:y_s+h_s, x_s:x_s+w_s]\n",
    "        if patch_s.shape[0] < 16 or patch_s.shape[1] < 16:\n",
    "            scale_features.append(np.zeros_like(scale_features[0]) if len(scale_features) > 0 else np.zeros((1764,)))\n",
    "            continue\n",
    "        resized = cv2.resize(patch_s, (64, 64))\n",
    "        scale_features.append(hog_scaling(resized))\n",
    "    \n",
    "    SF = np.fft.fft(np.array(scale_features), axis=0)\n",
    "    scale_H = scale_model_A / (scale_model_B[:, np.newaxis] + lambda_trust)\n",
    "    response_f = np.sum(np.conj(scale_H) * SF, axis=1)\n",
    "    response = np.real(np.fft.ifft(response_f))\n",
    "    \n",
    "    best_scale_idx = np.argmax(response)\n",
    "    return scale_factors[best_scale_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3 : Prediction The Next Frame's Center ( Kalman Filter )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kalman_prediction( F , X_k_1 , P_k_1 , Q_k):\n",
    "    x_k = np.dot(F , X_k_1)\n",
    "    p_k = np.dot( F , np.dot(P_k_1 , F.T)) + Q_k\n",
    "    return x_k , p_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kalman_updating(x_k , p_k , H_k , z_k , R_k):\n",
    "    k_1 = np.dot(np.dot(H_k , p_k) , H_k.T) + R_k\n",
    "    k_2 = np.dot(p_k , H_k.T)\n",
    "    K = np.dot(k_2 , np.linalg.inv(k_1))\n",
    "    P_k_new = p_k - np.dot(np.dot(K , H_k) , p_k)\n",
    "    x_k_new = x_k + np.dot(K , (z_k - np.dot(H_k , x_k)))\n",
    "    return x_k_new , P_k_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.4 : Updating Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_updating(H_new , H_old , alpha):\n",
    "    result = alpha * H_new + (1-alpha) * H_old\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.5 : Finding The Channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_channels(image):\n",
    "    hog_features = hog_channel(image)\n",
    "    colors = np.array([\n",
    "        [0.00, 0.00, 0.00], [45.37, -4.33, -33.43], [43.08, 17.51, 37.53],\n",
    "        [53.59, 0.00, 0.00], [47.31, -45.33, 41.35], [65.75, 71.45, 63.32],\n",
    "        [76.08, 22.25, -21.46], [32.30, 79.19, -107.86], [52.23, 75.43, 37.36],\n",
    "        [100.00, 0.00, 0.00], [92.13, -16.53, 93.35]\n",
    "    ], dtype=np.float32)\n",
    "    \n",
    "    image_lab = cv2.cvtColor(image, cv2.COLOR_BGR2Lab)\n",
    "    pixels = image_lab.reshape(-1, 3).astype(np.float32)\n",
    "    distances = np.sum((pixels[:, np.newaxis, :] - colors[np.newaxis, :, :]) ** 2, axis=2)\n",
    "    closest_color_indices = np.argmin(distances, axis=1)\n",
    "    \n",
    "    cn_features_flat = np.zeros((pixels.shape[0], colors.shape[0]), dtype=np.float32)\n",
    "    cn_features_flat[np.arange(pixels.shape[0]), closest_color_indices] = 1\n",
    "    cn_features = cn_features_flat.reshape(image.shape[0], image.shape[1], -1)\n",
    "\n",
    "    hog_resized = cv2.resize(hog_features, (image.shape[1], image.shape[0]))\n",
    "    return np.dstack((hog_resized, cn_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.6 : Teaching The Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def teaching ( f , g , lambda_trust):\n",
    "    X = np.fft.fft2(f , axes = (0 ,1 ))\n",
    "    G = np.fft.fft2(g)\n",
    "    G1 = np.expand_dims(G , axis = 2)\n",
    "    num = np.conj(X) * G1\n",
    "    denom = np.sum(np.conj(X) * X , axis =2) + lambda_trust\n",
    "    return num, denom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 : Main Detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 : Uploading Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture('person1.mp4')\n",
    "model = YOLO('yolo11n.pt')\n",
    "if not cap.isOpened():\n",
    "    print(\"wrong video\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 : The Main Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 1 motorcycle, 148.6ms\n",
      "Speed: 2.8ms preprocess, 148.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "tracking = False\n",
    "    \n",
    "model_A, model_B = None, None\n",
    "scale_model_A, scale_model_B = None, None\n",
    "current_pos, current_size = (0, 0), (0, 0)\n",
    "fixed_roi_size = (64, 128)\n",
    "\n",
    "dt = 1/30.0 \n",
    "F = np.array([[1, dt], [0, 1]])\n",
    "H_kalman = np.array([[1, 0]])\n",
    "Q = np.array([[(dt**4)/4, (dt**3)/2], [(dt**3)/2, dt**2]]) * 1.0\n",
    "R = np.array([[25.0]])\n",
    "kf_x_state, kf_y_state = np.zeros((2, 1)), np.zeros((2, 1))\n",
    "kf_x_p, kf_y_p = np.eye(2) * 500, np.eye(2) * 500\n",
    "\n",
    "scale_factors = np.array([0.95, 0.98, 1.0, 1.02, 1.05])\n",
    "\n",
    "last_time = 0\n",
    "fps_start_time = 0\n",
    "fps_frame_count = 0\n",
    "fps = 0\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    if fps_start_time == 0:\n",
    "        fps_start_time = time.time()\n",
    "    current_time = time.time()\n",
    "    dt = current_time - last_time if last_time > 0 else 1/30.0\n",
    "    last_time = current_time\n",
    "    F[0, 1] = dt\n",
    "    if not tracking:\n",
    "        results = model(frame)[0]\n",
    "        for box in results.boxes:\n",
    "            class_name = model.names[int(box.cls[0].item())]\n",
    "            conf = box.conf[0].item()\n",
    "            if class_name == \"person\" and conf > 0.7:\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())\n",
    "                w, h = x2 - x1, y2 - y1\n",
    "                \n",
    "                current_pos = (x1 + w/2, y1 + h/2)\n",
    "                current_size = (w, h)\n",
    "                \n",
    "                kf_x_state[0], kf_y_state[0] = current_pos[0], current_pos[1]\n",
    "                \n",
    "                patch = cv2.resize(frame[y1:y2, x1:x2], fixed_roi_size)\n",
    "                features = extract_channels(patch)\n",
    "                target_y_2d = np.fft.ifft2(np.fft.fft2(cv2.getGaussianKernel(fixed_roi_size[1], 18, cv2.CV_32F)) * np.fft.fft2(cv2.getGaussianKernel(fixed_roi_size[0], 18, cv2.CV_32F).T)).real\n",
    "                model_A, model_B = teaching(features, target_y_2d, 0.01)\n",
    "                target_y_1d = np.fft.ifft(np.fft.fft(cv2.getGaussianKernel(len(scale_factors), 1, cv2.CV_32F))).real.flatten()\n",
    "                \n",
    "                hog_desc_scale = cv2.HOGDescriptor((64, 64), (16, 16), (8, 8), (8, 8), 9)\n",
    "                descriptor_size_scale = hog_desc_scale.getDescriptorSize()\n",
    "                scale_features_init = []\n",
    "                for scale in scale_factors:\n",
    "                    w_s, h_s = int(w * scale), int(h * scale)\n",
    "                    x_s, y_s = int(current_pos[0] - w_s / 2), int(current_pos[1] - h_s / 2)\n",
    "                    patch_s = frame[y_s:y_s+h_s, x_s:x_s+w_s]\n",
    "                    if patch_s.shape[0] < 16 or patch_s.shape[1] < 16:\n",
    "                        scale_features_init.append(np.zeros((descriptor_size_scale,)))\n",
    "                        continue\n",
    "                    resized = cv2.resize(patch_s, (64, 64))\n",
    "                    scale_features_init.append(hog_scaling(resized))\n",
    "                \n",
    "                SF = np.fft.fft(np.array(scale_features_init), axis=0)\n",
    "                scale_model_A = np.conj(SF) * target_y_1d[:, np.newaxis]\n",
    "                scale_model_B = np.sum(np.conj(SF) * SF, axis=1)\n",
    "                tracking = True\n",
    "                cv2.rectangle(frame, (x1 , y1 ), (x2, y2), (0, 0, 255), 2)\n",
    "                # break\n",
    "    else:\n",
    "        kf_x_state, kf_x_p = kalman_prediction(F, kf_x_state, kf_x_p, Q)\n",
    "        kf_y_state, kf_y_p = kalman_prediction(F, kf_y_state, kf_y_p, Q)\n",
    "        pred_pos = (kf_x_state[0, 0], kf_y_state[0, 0])\n",
    "        current_scale = scaled_check(frame, pred_pos, current_size, scale_factors, scale_model_A, scale_model_B, 0.01)\n",
    "        current_size = (current_size[0] * current_scale, current_size[1] * current_scale)\n",
    "        \n",
    "        search_area_scale = 1.5\n",
    "        w_search, h_search = int(current_size[0] * search_area_scale), int(current_size[1] * search_area_scale)\n",
    "        x_search, y_search = int(pred_pos[0] - w_search/2), int(pred_pos[1] - h_search/2)\n",
    "        \n",
    "        search_patch = frame[y_search:y_search+h_search, x_search:x_search+w_search]\n",
    "        if search_patch.shape[0] > 0 and search_patch.shape[1] > 0:\n",
    "            resized_patch = cv2.resize(search_patch, fixed_roi_size)\n",
    "            features = extract_channels(resized_patch)\n",
    "            Z = np.fft.fft2(features, axes=(0, 1))\n",
    "            \n",
    "            H_filter = model_A / (np.expand_dims(model_B, axis=2) + 0.01)\n",
    "            response_f = np.sum(np.conj(H_filter) * Z, axis=2)\n",
    "            response = np.real(np.fft.ifft2(response_f))\n",
    "            \n",
    "            peak_y, peak_x = np.unravel_index(np.argmax(response), response.shape)\n",
    "            \n",
    "            if peak_y > fixed_roi_size[1] / 2:\n",
    "                peak_y -= fixed_roi_size[1]\n",
    "            if peak_x > fixed_roi_size[0] / 2:\n",
    "                peak_x -= fixed_roi_size[0]\n",
    "            \n",
    "            dx = (peak_x / fixed_roi_size[0]) * w_search\n",
    "            dy = (peak_y / fixed_roi_size[1]) * h_search\n",
    "            \n",
    "            measured_pos = (pred_pos[0] + dx, pred_pos[1] + dy)\n",
    "            kf_x_state, kf_x_p = kalman_updating(kf_x_state, kf_x_p, H_kalman, measured_pos[0], R)\n",
    "            kf_y_state, kf_y_p = kalman_updating(kf_y_state, kf_y_p, H_kalman, measured_pos[1], R)\n",
    "            \n",
    "            current_pos = (kf_x_state[0, 0], kf_y_state[0, 0])\n",
    "            \n",
    "            x1_up, y1_up = int(current_pos[0] - current_size[0]/2), int(current_pos[1] - current_size[1]/2)\n",
    "            w1_up, h1_up = int(current_size[0]), int(current_size[1])\n",
    "            update_patch = frame[y1_up:y1_up+h1_up, x1_up:x1_up+w1_up]\n",
    "            if update_patch.shape[0] > 0 and update_patch.shape[1] > 0:\n",
    "                resized_patch_up = cv2.resize(update_patch, fixed_roi_size)\n",
    "                features_new = extract_channels(resized_patch_up)\n",
    "                target_y_2d = np.fft.ifft2(np.fft.fft2(cv2.getGaussianKernel(fixed_roi_size[1], 18, cv2.CV_32F)) * np.fft.fft2(cv2.getGaussianKernel(fixed_roi_size[0], 18, cv2.CV_32F).T)).real\n",
    "                new_A, new_B = teaching(features_new, target_y_2d, 0.01)\n",
    "                \n",
    "                model_A = filter_updating(new_A, model_A, 0.02)\n",
    "                model_B = filter_updating(new_B, model_B, 0.02)\n",
    "                target_y_1d_up = np.fft.ifft(np.fft.fft(cv2.getGaussianKernel(len(scale_factors), 1, cv2.CV_32F))).real.flatten()\n",
    "                scale_features_new = []\n",
    "                for scale in scale_factors:\n",
    "                    w_s, h_s = int(current_size[0] * scale), int(current_size[1] * scale)\n",
    "                    x_s, y_s = int(current_pos[0] - w_s / 2), int(current_pos[1] - h_s / 2)\n",
    "                    patch_s = frame[y_s:y_s+h_s, x_s:x_s+w_s]\n",
    "                    if patch_s.shape[0] < 16 or patch_s.shape[1] < 16:\n",
    "                        scale_features_new.append(np.zeros((1764,)))\n",
    "                        continue\n",
    "                    resized = cv2.resize(patch_s, (64, 64))\n",
    "                    scale_features_new.append(hog_scaling(resized))\n",
    "                SF_new = np.fft.fft(np.array(scale_features_new), axis=0)\n",
    "                new_scale_A = np.conj(SF_new) * target_y_1d_up[:, np.newaxis]\n",
    "                new_scale_B = np.sum(np.conj(SF_new) * SF_new, axis=1)\n",
    "                scale_model_A = filter_updating(new_scale_A, scale_model_A, 0.02)\n",
    "                scale_model_B = filter_updating(new_scale_B, scale_model_B, 0.02)\n",
    "        x_draw, y_draw, w_draw, h_draw = int(current_pos[0]-current_size[0]/2), int(current_pos[1]-current_size[1]/2), int(current_size[0]), int(current_size[1])\n",
    "        cv2.rectangle(frame, (x_draw, y_draw), (x_draw + w_draw, y_draw + h_draw), (0, 255, 0), 2)\n",
    "        cv2.putText(frame, \"person\", (x_draw, y_draw - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "    fps_frame_count += 1\n",
    "    if (time.time() - fps_start_time) > 1:\n",
    "        fps = fps_frame_count / (time.time() - fps_start_time)\n",
    "        fps_frame_count = 0\n",
    "        fps_start_time = time.time()\n",
    "    fps_text = f\"FPS: {fps:.2f}\"\n",
    "    cv2.putText(frame, fps_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "    cv2.imshow(\"Advanced Tracker\", frame)\n",
    "    if cv2.waitKey(1000) & 0xFF == 27:\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
