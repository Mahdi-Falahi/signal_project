{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c52d8f99",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "  <a href=\"http://www.sharif.edu/\">\n",
    "    <img src=\"https://cdn.freebiesupply.com/logos/large/2x/sharif-logo-png-transparent.png\" alt=\"SUT Logo\" width=\"140\">\n",
    "  </a>\n",
    "  \n",
    "  # Sharif University of Technology\n",
    "  ### Electrical Engineering Department\n",
    "\n",
    "  ## Signals and Systems\n",
    "  #### *Final Project - Spring 2025*\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "<div align=\"center\">\n",
    "  <h1>\n",
    "    <b>Object Tracker</b>\n",
    "  </h1>\n",
    "  <p>\n",
    "    An object tracking system using YOLO for detection and various algorithms (KCF, CSRT, MOSSE) for tracking.\n",
    "  </p>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "| Professor                  |\n",
    "| :-------------------------: |\n",
    "| Dr. Mohammad Mehdi Mojahedian |\n",
    "\n",
    "<br>\n",
    "\n",
    "| Contributors              |\n",
    "| :-----------------------: |\n",
    "| **Amirreza Mousavi** |\n",
    "| **Mahdi Falahi** |\n",
    "| **Zahra Miladipour** |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6017895",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e8b3de",
   "metadata": {},
   "source": [
    "# 0: Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3b8cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import time\n",
    "import torch\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import base64\n",
    "import zlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427bd61d",
   "metadata": {},
   "source": [
    "# 1: Object Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8186abf2",
   "metadata": {},
   "source": [
    "## Preparing Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5a1c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"ObjectTracker using device: {device}\")\n",
    "model = YOLO('./yolo11n.pt').to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e83919",
   "metadata": {},
   "source": [
    "## numpy csrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63d81d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumpyCSRT:\n",
    "    \"\"\"\n",
    "    An advanced, NumPy-optimized implementation of a CSRT-like tracker with DSST.\n",
    "\n",
    "    This version is upgraded with a discriminative scale filter (DSST) to handle\n",
    "    size variations of the target object, in addition to previous upgrades.\n",
    "\n",
    "    Upgrades:\n",
    "    1.  **DSST Scale Estimation**: A secondary, 1D correlation filter is added to\n",
    "        explicitly track changes in the target's scale, making the tracker\n",
    "        robust to objects moving closer or further away.\n",
    "    2.  **Advanced Features (HOG & Color Names)**: Uses a powerful combination of\n",
    "        features for the translation filter. HOG is used for the scale filter.\n",
    "    3.  **Adaptive Model Update**: Skips expensive model updates on frames with\n",
    "        high tracking confidence (PSR), boosting FPS.\n",
    "    \"\"\"\n",
    "    def __init__(self, padding=2.5, sigma=0.1, lambda_reg=1e-4, learning_rate=0.02, \n",
    "                 psr_threshold=5.0, high_confidence_psr=11.0, hog_cell_size=4,\n",
    "                 n_scales=33, scale_step=1.02, scale_sigma=0.25, scale_lr=0.025):\n",
    "        # --- Hyperparameters ---\n",
    "        self.padding = padding\n",
    "        self.sigma = sigma\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.lr = learning_rate\n",
    "        self.psr_threshold = psr_threshold\n",
    "        self.high_confidence_psr = high_confidence_psr\n",
    "        self.hog_cell_size = hog_cell_size\n",
    "        \n",
    "        # --- DSST Scale Filter Hyperparameters ---\n",
    "        self.n_scales = n_scales\n",
    "        self.scale_step = scale_step\n",
    "        self.scale_sigma = scale_sigma\n",
    "        self.scale_lr = scale_lr\n",
    "\n",
    "        # --- Internal State ---\n",
    "        self.pos = None\n",
    "        self.target_sz = None\n",
    "        self.initial_target_sz = None # Store original size for scaling\n",
    "        self.window_sz = None\n",
    "        self.hann_window = None\n",
    "        self.yf = None\n",
    "        self.target_hsv_hist = None\n",
    "        self.w2c = self._load_w2c_table()\n",
    "\n",
    "        # --- Translation Model (Fourier Domain) ---\n",
    "        self.model_alphaf = None\n",
    "        self.model_xf = None\n",
    "        \n",
    "        # --- Scale Model (Fourier Domain) ---\n",
    "        self.current_scale_factor = 1.0\n",
    "        self.scale_factors = None\n",
    "        self.scale_window_sz = None\n",
    "        self.scale_hann = None\n",
    "        self.scale_yf = None\n",
    "        self.model_scale_alphaf = None\n",
    "        self.model_scale_xf = None\n",
    "\n",
    "\n",
    "    def init(self, frame, bbox):\n",
    "        \"\"\"\n",
    "        Initializes the tracker with the first frame and bounding box.\n",
    "\n",
    "        Args:\n",
    "            frame (np.ndarray): The first frame of the video (BGR).\n",
    "            bbox (tuple): The initial bounding box (x, y, w, h).\n",
    "        \"\"\"\n",
    "        x, y, w, h = [int(v) for v in bbox]\n",
    "\n",
    "        # --- ROBUSTNESS CHECK ---\n",
    "        # Add a check to ensure the initial bounding box is large enough.\n",
    "        # The HOG feature extractor needs at least a 2x2 block of cells.\n",
    "        # With cell_size=4, this means a minimum patch size of 8x8 pixels.\n",
    "        min_size = 2 * self.hog_cell_size\n",
    "        if w < min_size or h < min_size:\n",
    "            raise ValueError(f\"Initial bounding box is too small. \"\n",
    "                             f\"Minimum size is {min_size}x{min_size} pixels, \"\n",
    "                             f\"but got {w}x{h}.\")\n",
    "\n",
    "        self.target_sz = np.array([w, h])\n",
    "        self.pos = np.array([x + w / 2, y + h / 2])\n",
    "        \n",
    "        # Adjust window size to be a multiple of the HOG cell size\n",
    "        base_window_sz = np.floor(self.target_sz * (1 + self.padding))\n",
    "        self.window_sz = np.round(base_window_sz / (2 * self.hog_cell_size)) * (2 * self.hog_cell_size)\n",
    "        \n",
    "        # Pre-compute ideal Gaussian response and Hann window\n",
    "        self.yf = self._create_gaussian_peak((self.window_sz // self.hog_cell_size).astype(int))\n",
    "        self.hann_window = self._create_hann_window((self.window_sz // self.hog_cell_size).astype(int))\n",
    "\n",
    "        # --- Initial Model Training ---\n",
    "        # 1. Extract initial patch and create color histogram for spatial mask\n",
    "        init_patch_hsv = self._get_subwindow(frame, self.pos, self.window_sz, to_hsv=True)\n",
    "        target_roi = cv2.getRectSubPix(init_patch_hsv, (w, h), (self.window_sz[0]/2, self.window_sz[1]/2))\n",
    "        self.target_hsv_hist = cv2.calcHist([target_roi], [0, 1], None, [180, 256], [0, 180, 0, 256])\n",
    "        cv2.normalize(self.target_hsv_hist, self.target_hsv_hist, 0, 255, cv2.NORM_MINMAX)\n",
    "        spatial_mask = self._get_spatial_reliability_mask(init_patch_hsv)\n",
    "\n",
    "        # 2. Extract advanced features (HOG+CN)\n",
    "        init_patch_rgb = self._get_subwindow(frame, self.pos, self.window_sz)\n",
    "        features = self._get_features(init_patch_rgb)\n",
    "        \n",
    "        # 3. Apply spatial mask to the features\n",
    "        resized_mask = cv2.resize(spatial_mask, (features.shape[1], features.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "        features_masked = features * resized_mask[:, :, np.newaxis]\n",
    "        \n",
    "        # 4. Train the initial filter model\n",
    "        xf = np.fft.fft2(features_masked, axes=(0, 1))\n",
    "        self.model_alphaf = self.yf[:, :, np.newaxis] * np.conj(xf)\n",
    "        self.model_xf = xf\n",
    "\n",
    "\n",
    "    def update(self, frame):\n",
    "        \"\"\"\n",
    "        Tracks the object in the current frame and updates the model.\n",
    "        \"\"\"\n",
    "        # --- 1. Localize Translation ---\n",
    "        # Get patch at current scale and position\n",
    "        scaled_window_sz = self.window_sz * self.current_scale_factor\n",
    "        search_patch = self._get_subwindow(frame, self.pos, scaled_window_sz)\n",
    "        \n",
    "        # Resize patch to base feature size, then extract features\n",
    "        resized_patch = cv2.resize(search_patch, (int(self.window_sz[0]), int(self.window_sz[1])))\n",
    "        features_z = self._get_features(resized_patch)\n",
    "        zf = np.fft.fft2(features_z, axes=(0, 1))\n",
    "        \n",
    "        # Calculate translation response\n",
    "        kzf = np.sum(self.model_xf * np.conj(self.model_xf), axis=2)\n",
    "        filter_f = self.model_alphaf / (kzf[:, :, np.newaxis] + self.lambda_reg)\n",
    "        response_f = np.sum(np.conj(filter_f) * zf, axis=2)\n",
    "        response = np.real(np.fft.ifft2(response_f))\n",
    "        \n",
    "        psr = self._calculate_psr(response)\n",
    "        if psr < self.psr_threshold:\n",
    "            return False, None\n",
    "\n",
    "        disp = np.unravel_index(np.argmax(response), response.shape)\n",
    "        dy = disp[0] - response.shape[0] // 2\n",
    "        dx = disp[1] - response.shape[1] // 2\n",
    "        self.pos += [dx * self.hog_cell_size * self.current_scale_factor, dy * self.hog_cell_size * self.current_scale_factor]\n",
    "        \n",
    "        # --- 2. Estimate Scale ---\n",
    "        scale_features = self._get_scale_features(frame, self.pos, self.initial_target_sz, self.current_scale_factor, self.scale_factors)\n",
    "        scale_features_hann = self.scale_hann * scale_features\n",
    "        sf = np.fft.fft(scale_features_hann, axis=1)\n",
    "        \n",
    "        scale_filter_f = np.sum(self.model_scale_alphaf * sf, axis=1) / (np.sum(self.model_scale_xf * np.conj(self.model_scale_xf), axis=1) + self.lambda_reg)\n",
    "        scale_response = np.real(np.fft.ifft(scale_filter_f))\n",
    "        \n",
    "        best_scale_idx = np.argmax(scale_response)\n",
    "        self.current_scale_factor *= self.scale_factors[best_scale_idx]\n",
    "        \n",
    "        # Clamp scale factor to prevent extreme drift\n",
    "        self.current_scale_factor = np.clip(self.current_scale_factor, 0.5, 2.0)\n",
    "\n",
    "        # --- 3. Adaptive Model Update ---\n",
    "        if psr < self.high_confidence_psr:\n",
    "            # --- Update Translation Model ---\n",
    "            update_patch_rgb = self._get_subwindow(frame, self.pos, self.window_sz * self.current_scale_factor)\n",
    "            resized_update_patch = cv2.resize(update_patch_rgb, (int(self.window_sz[0]), int(self.window_sz[1])))\n",
    "            update_patch_hsv = cv2.cvtColor(resized_update_patch, cv2.COLOR_RGB2HSV)\n",
    "            \n",
    "            spatial_mask = self._get_spatial_reliability_mask(update_patch_hsv)\n",
    "            features_x = self._get_features(resized_update_patch)\n",
    "            resized_mask = cv2.resize(spatial_mask, (features_x.shape[1], features_x.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "            features_masked = features_x * resized_mask[:, :, np.newaxis]\n",
    "            \n",
    "            xf = np.fft.fft2(features_masked, axes=(0, 1))\n",
    "            new_alphaf = self.yf[:, :, np.newaxis] * np.conj(xf)\n",
    "            self.model_alphaf = (1 - self.lr) * self.model_alphaf + self.lr * new_alphaf\n",
    "            self.model_xf = (1 - self.lr) * self.model_xf + self.lr * xf\n",
    "\n",
    "            # --- Update Scale Model ---\n",
    "            new_scale_features = self._get_scale_features(frame, self.pos, self.initial_target_sz, self.current_scale_factor, self.scale_factors)\n",
    "            new_scale_features_hann = self.scale_hann * new_scale_features\n",
    "            new_sf = np.fft.fft(new_scale_features_hann, axis=1)\n",
    "            \n",
    "            new_scale_alphaf = self.scale_yf * np.conj(new_sf)\n",
    "            self.model_scale_alphaf = (1 - self.scale_lr) * self.model_scale_alphaf + self.scale_lr * new_scale_alphaf\n",
    "            self.model_scale_xf = (1 - self.scale_lr) * self.model_scale_xf + self.scale_lr * new_sf\n",
    "\n",
    "        # --- 4. Return Final Bounding Box ---\n",
    "        final_target_sz = self.initial_target_sz * self.current_scale_factor\n",
    "        bbox = (self.pos[0] - final_target_sz[0]/2,\n",
    "                self.pos[1] - final_target_sz[1]/2,\n",
    "                final_target_sz[0],\n",
    "                final_target_sz[1])\n",
    "        return True, bbox\n",
    "\n",
    "    # --- Feature Extraction ---\n",
    "    def _get_features(self, patch):\n",
    "        hog = self._get_hog_features(patch)\n",
    "        cn = self._get_cn_features(patch)\n",
    "        return (np.concatenate((hog, cn), axis=2) * self.hann_window[:, :, np.newaxis]).astype(np.float32)\n",
    "\n",
    "    def _get_scale_features(self, frame, pos, base_sz, current_scale, scale_factors):\n",
    "        n_scales = len(scale_factors)\n",
    "        scale_feature_dim = 36 # HOG feature dimension\n",
    "        \n",
    "        # Use a fixed feature map size for scale features\n",
    "        scale_model_sz = np.maximum(base_sz / self.hog_cell_size, [2,2])\n",
    "        scale_model_sz = (int(scale_model_sz[0]), int(scale_model_sz[1]))\n",
    "        \n",
    "        scale_features = np.zeros((n_scales, scale_model_sz[0] * scale_model_sz[1] * scale_feature_dim))\n",
    "\n",
    "        for i, scale in enumerate(scale_factors):\n",
    "            patch_sz = base_sz * current_scale * scale\n",
    "            patch = self._get_subwindow(frame, pos, patch_sz)\n",
    "            \n",
    "            # Resize patch to a fixed size before HOG extraction\n",
    "            resized_patch = cv2.resize(patch, (scale_model_sz[0] * self.hog_cell_size, scale_model_sz[1] * self.hog_cell_size))\n",
    "            \n",
    "            hog_feats = self._get_hog_features(resized_patch)\n",
    "            scale_features[i, :] = hog_feats.flatten()\n",
    "            \n",
    "        return scale_features\n",
    "\n",
    "    def _get_cn_features(self, patch):\n",
    "        R, G, B = patch[:, :, 0], patch[:, :, 1], patch[:, :, 2]\n",
    "        index = np.floor(R/8).astype(int) * 32*32 + np.floor(G/8).astype(int) * 32 + np.floor(B/8).astype(int)\n",
    "        cn_features = self.w2c[index]\n",
    "        return cv2.resize(cn_features, (self.window_sz[0] // self.hog_cell_size, self.window_sz[1] // self.hog_cell_size))\n",
    "\n",
    "    def _get_hog_features(self, patch):\n",
    "        img = cv2.cvtColor(patch, cv2.COLOR_RGB2GRAY)\n",
    "        gx = cv2.Sobel(img, cv2.CV_32F, 1, 0, ksize=1)\n",
    "        gy = cv2.Sobel(img, cv2.CV_32F, 0, 1, ksize=1)\n",
    "        mag, ang = cv2.cartToPolar(gx, gy, angleInDegrees=True)\n",
    "        ang = ang % 180\n",
    "        n_bins = 9\n",
    "        h, w = img.shape\n",
    "        n_cells_y = h // self.hog_cell_size\n",
    "        n_cells_x = w // self.hog_cell_size\n",
    "        mag_cells = mag.reshape(n_cells_y, self.hog_cell_size, n_cells_x, self.hog_cell_size).transpose(0, 2, 1, 3)\n",
    "        ang_cells = ang.reshape(n_cells_y, self.hog_cell_size, n_cells_x, self.hog_cell_size).transpose(0, 2, 1, 3)\n",
    "        hist = np.zeros((n_cells_y, n_cells_x, n_bins))\n",
    "        for i in range(n_cells_y):\n",
    "            for j in range(n_cells_x):\n",
    "                cell_mag = mag_cells[i, j].ravel()\n",
    "                cell_ang = ang_cells[i, j].ravel()\n",
    "                bin_indices = np.floor(cell_ang / (180 / n_bins)).astype(int)\n",
    "                for k in range(len(cell_ang)):\n",
    "                    hist[i, j, bin_indices[k] % n_bins] += cell_mag[k]\n",
    "        hog_features = np.zeros((n_cells_y - 1, n_cells_x - 1, n_bins * 4))\n",
    "        for i in range(n_cells_y - 1):\n",
    "            for j in range(n_cells_x - 1):\n",
    "                block = hist[i:i+2, j:j+2].flatten()\n",
    "                norm = np.sqrt(np.sum(block**2) + 1e-6)\n",
    "                hog_features[i, j] = block / norm\n",
    "        return np.pad(hog_features, ((0, 1), (0, 1), (0, 0)), 'constant')\n",
    "\n",
    "    # --- Helper Methods ---\n",
    "    def _get_subwindow(self, img, center, sz, to_hsv=False):\n",
    "        w, h = int(sz[0]), int(sz[1])\n",
    "        if to_hsv:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "        else:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        return cv2.getRectSubPix(img, (w, h), (center[0], center[1]))\n",
    "\n",
    "    def _create_gaussian_peak(self, sz):\n",
    "        w, h = int(sz[0]), int(sz[1])\n",
    "        output_sigma = np.sqrt(w * h) / self.padding * self.sigma\n",
    "        syh, sxh = np.ogrid[-h//2:h//2, -w//2:w//2]\n",
    "        gauss = np.exp(-0.5 * (sxh**2 + syh**2) / output_sigma**2)\n",
    "        return np.fft.fft2(np.roll(gauss, (-h//2, -w//2), axis=(0, 1)))\n",
    "        \n",
    "    def _create_1d_gaussian_peak(self, n, sigma):\n",
    "        x = np.arange(n)\n",
    "        mid = n // 2\n",
    "        gauss = np.exp(-0.5 * ((x - mid) / sigma)**2)\n",
    "        return np.fft.fft(gauss)\n",
    "\n",
    "    def _create_hann_window(self, sz):\n",
    "        return np.outer(np.hanning(sz[1]), np.hanning(sz[0]))\n",
    "\n",
    "    def _get_spatial_reliability_mask(self, hsv_patch):\n",
    "        mask = cv2.calcBackProject([hsv_patch], [0, 1], self.target_hsv_hist, [0, 180, 0, 256], 1)\n",
    "        _, mask = cv2.threshold(mask, 25, 255, cv2.THRESH_BINARY)\n",
    "        kernel = np.ones((5, 5), np.uint8)\n",
    "        mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel, iterations=1)\n",
    "        return mask / 255.0\n",
    "\n",
    "    def _calculate_psr(self, response_map, peak_radius=5):\n",
    "        peak_val = np.max(response_map)\n",
    "        peak_pos = np.unravel_index(np.argmax(response_map), response_map.shape)\n",
    "        sidelobe_mask = np.ones_like(response_map, dtype=bool)\n",
    "        y_min = max(0, peak_pos[0] - peak_radius)\n",
    "        y_max = min(response_map.shape[0], peak_pos[0] + peak_radius + 1)\n",
    "        x_min = max(0, peak_pos[1] - peak_radius)\n",
    "        x_max = min(response_map.shape[1], peak_pos[1] + peak_radius + 1)\n",
    "        sidelobe_mask[y_min:y_max, x_min:x_max] = False\n",
    "        sidelobe_vals = response_map[sidelobe_mask]\n",
    "        if sidelobe_vals.size == 0: return 100.0\n",
    "        sidelobe_mean = np.mean(sidelobe_vals)\n",
    "        sidelobe_std = np.std(sidelobe_vals)\n",
    "        if sidelobe_std < 1e-6: return 100.0\n",
    "        return (peak_val - sidelobe_mean) / sidelobe_std\n",
    "    \n",
    "    def _load_w2c_table(self):\n",
    "        w2c_b64 = \"eNq1M1sHEAMYBOBPaAAVgAJQAApAASgABaAAFIACOA04DbgNuA24Dbhfgf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gT6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6B/gf4F+hfoX6-HwA=\"\n",
    "        data = zlib.decompress(base64.b64decode(w2c_b64))\n",
    "        return np.frombuffer(data, dtype=np.float32).reshape(32, 32, 32, 11)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153643a7",
   "metadata": {},
   "source": [
    "## ObjectTracker Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb41a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObjectTracker:\n",
    "    def __init__(self, model, tracker_type='KCF', detect_interval=48, conf_threshold=0.5):\n",
    "\n",
    "        self.model = model\n",
    "        self.detect_interval = detect_interval\n",
    "        self.conf_threshold = conf_threshold\n",
    "        self.frame_idx = 0\n",
    "        self.tracked_objects = []\n",
    "\n",
    "        self.tracker_constructors = {\n",
    "            'CSRT': cv2.legacy.TrackerCSRT_create,\n",
    "            'KCF': cv2.legacy.TrackerKCF_create,\n",
    "            'MOSSE': cv2.legacy.TrackerMOSSE_create,\n",
    "            'MEDIAN_FLOW': cv2.legacy.TrackerMedianFlow_create,\n",
    "            'NUMPY_CSRT' : NumpyCSRT\n",
    "        }\n",
    "        if tracker_type not in self.tracker_constructors:\n",
    "            raise ValueError(f\"Invalid tracker type: {tracker_type}. Choose from {list(self.tracker_constructors.keys())}\")\n",
    "        self.tracker_type = tracker_type\n",
    "        print(f\"Using tracker: {self.tracker_type}\")\n",
    "\n",
    "    def process_frame(self, frame, iou_threshold=0.7, max_lost_frames=5, max_objects=1):\n",
    "        height, width = frame.shape[:2]\n",
    "\n",
    "        # Lazily initialize a tracker ID counter\n",
    "        if not hasattr(self, 'next_track_id'):\n",
    "            self.next_track_id = 0\n",
    "            # print(frame.shape)\n",
    "\n",
    "        # --- PHASE 1: PREDICT with Kalman Filter ---\n",
    "        # Predict the new state for each track based on its motion model\n",
    "        for obj in self.tracked_objects:\n",
    "            obj['kf'].predict()\n",
    "            # Update bbox to the KF's prediction. This is our guess before we get new data.\n",
    "            predicted_state = obj['kf'].statePost\n",
    "            cx, cy, w, h = predicted_state[0], predicted_state[1], predicted_state[2], predicted_state[3]\n",
    "            obj['bbox'] = (int(cx - w/2), int(cy - h/2), int(cx + w/2), int(cy + h/2))\n",
    "\n",
    "        for obj in self.tracked_objects:\n",
    "            x1, y1, x2, y2 = obj['bbox']\n",
    "            # Check if fully out of frame\n",
    "            if x2 < 0 or y2 < 0 or x1 > width or y1 > height:\n",
    "                obj['lost_frames'] = max_lost_frames  # Force removal in cleanup\n",
    "            # Or, minimal overlap: compute intersection area with frame\n",
    "            elif max(0, min(x2, width) - max(x1, 0)) * max(0, min(y2, height) - max(y1, 0)) < 0.1 * (x2 - x1) * (y2 - y1):\n",
    "                obj['lost_frames'] += 1  # Gradual loss for partial exits\n",
    "            # Clip bbox to frame (optional, for drawing)\n",
    "            obj['bbox'] = (max(0, x1), max(0, y1), min(width, x2), min(height, y2))\n",
    "\n",
    "        # --- PHASE 2: UPDATE with Lightweight Tracker ---\n",
    "        lost_track_detected = False\n",
    "        for obj in self.tracked_objects:\n",
    "            success, bbox = obj['tracker'].update(frame)\n",
    "            if success:\n",
    "                # Measurement is successful. Correct the KF with the tracker's output.\n",
    "                x1, y1, w, h = [int(v) for v in bbox]\n",
    "                cx, cy = x1 + w/2, y1 + h/2\n",
    "                measurement = np.array([cx, cy, w, h], dtype=np.float32)\n",
    "                obj['kf'].correct(measurement)\n",
    "                obj['lost_frames'] = 0 # Reset lost counter\n",
    "            else:\n",
    "                # Tracker failed. Increment the lost counter.\n",
    "                obj['lost_frames'] += 1\n",
    "                lost_track_detected = True\n",
    "\n",
    "        # --- PHASE 3: CLEANUP & DETECT ---\n",
    "        # Remove tracks that have been lost for too long\n",
    "        survived_tracks = [t for t in self.tracked_objects if t['lost_frames'] < max_lost_frames]\n",
    "        \n",
    "        # Trigger detector if it's the first frame or a track was lost (even temporarily) or periodic\n",
    "        if self.frame_idx == 0 or lost_track_detected or (self.frame_idx % self.detect_interval == 0):\n",
    "            detections = self.detect(frame)\n",
    "            \n",
    "            # Associate detections with survived tracks\n",
    "            if detections:\n",
    "                tracked_bboxes = [t['bbox'] for t in survived_tracks]\n",
    "                detected_bboxes = [[d['x1'], d['y1'], d['x2'], d['y2']] for d in detections]\n",
    "                \n",
    "                iou_matrix = self._calculate_iou(tracked_bboxes, detected_bboxes)\n",
    "                matched_pairs, _, unmatched_detections = self._apply_matching(iou_matrix, iou_threshold)\n",
    "\n",
    "                # Update matched tracks with detector's more accurate data\n",
    "                for t_idx, d_idx in matched_pairs:\n",
    "                    track = survived_tracks[t_idx]\n",
    "                    det = detections[d_idx]\n",
    "                    x1, y1, x2, y2 = det['x1'], det['y1'], det['x2'], det['y2']\n",
    "                    w, h = x2 - x1, y2 - y1\n",
    "                    cx, cy = x1 + w/2, y1 + h/2\n",
    "                    \n",
    "                    # Correct the KF with the accurate detector measurement\n",
    "                    measurement = np.array([cx, cy, w, h], dtype=np.float32)\n",
    "                    track['kf'].correct(measurement)\n",
    "                    \n",
    "                    # Re-initialize the lightweight tracker to prevent drift\n",
    "                    track['tracker'].init(frame, (x1, y1, w, h))\n",
    "                    track['lost_frames'] = 0 # Reset lost counter as we found it\n",
    "                \n",
    "                # Create new tracks for unmatched detections\n",
    "                for d_idx in unmatched_detections:\n",
    "                    det = detections[d_idx]\n",
    "                    x1, y1, x2, y2 = det['x1'], det['y1'], det['x2'], det['y2']\n",
    "                    w, h = x2-x1, y2-y1\n",
    "                    \n",
    "                    new_kf = self._create_kalman_filter()\n",
    "                    new_kf.statePost = np.array([x1+w/2, y1+h/2, w, h, 0, 0, 0, 0], dtype=np.float32)\n",
    "                    \n",
    "                    tracker = self.tracker_constructors[self.tracker_type]()\n",
    "                    tracker.init(frame, (x1, y1, w, h))\n",
    "                    \n",
    "                    survived_tracks.append({\n",
    "                        'id': self.next_track_id, 'kf': new_kf, 'tracker': tracker,\n",
    "                        'class_name': det['class_name'], 'conf': det['conf'],\n",
    "                        'color': np.random.uniform(0, 255, 3).tolist(), 'bbox': (x1, y1, x2, y2),\n",
    "                        'lost_frames': 0\n",
    "                    })\n",
    "                    self.next_track_id += 1\n",
    "        \n",
    "        self.tracked_objects = survived_tracks[:min(max_objects, len(survived_tracks))]\n",
    "\n",
    "        # --- PHASE 4: DRAWING ---\n",
    "        # Final bboxes are taken from the corrected KF state for smoothness\n",
    "        objects_to_draw = []\n",
    "        for obj in self.tracked_objects:\n",
    "            # Get the smoothed bbox from the Kalman Filter's state\n",
    "            state = obj['kf'].statePost\n",
    "            cx, cy, w, h = state[0], state[1], state[2], state[3]\n",
    "            x1, y1, x2, y2 = int(cx-w/2), int(cy-h/2), int(cx+w/2), int(cy+h/2)\n",
    "            \n",
    "            objects_to_draw.append({\n",
    "                'x1': x1, 'y1': y1, 'x2': x2, 'y2': y2,\n",
    "                'class_name': f\"ID-{obj['id']} {obj['class_name']}\",\n",
    "                'conf': obj.get('conf', 1.0), 'color': obj['color']\n",
    "            })\n",
    "        \n",
    "        annotated_frame = self.draw_boxes(frame, objects_to_draw)\n",
    "\n",
    "        self.frame_idx += 1\n",
    "        return annotated_frame\n",
    "    \n",
    "    def _calculate_iou(self, bboxes1, bboxes2):\n",
    "        \"\"\"\n",
    "        Calculates the Intersection over Union (IoU) matrix between two sets of boxes.\n",
    "        - bboxes1: A NumPy array of shape (N, 4) for the first set of boxes.\n",
    "        - bboxes2: A NumPy array of shape (M, 4) for the second set of boxes.\n",
    "        Returns a NumPy array of shape (N, M) with the IoU scores.\n",
    "        \"\"\"\n",
    "        # Ensure we have NumPy arrays\n",
    "        bboxes1 = np.array(bboxes1)\n",
    "        bboxes2 = np.array(bboxes2)\n",
    "\n",
    "        # Return empty if either input is empty\n",
    "        if bboxes1.size == 0 or bboxes2.size == 0:\n",
    "            return np.empty((len(bboxes1), len(bboxes2)))\n",
    "\n",
    "        # Determine the coordinates of the intersection rectangles\n",
    "        xA = np.maximum(bboxes1[:, 0][:, np.newaxis], bboxes2[:, 0])\n",
    "        yA = np.maximum(bboxes1[:, 1][:, np.newaxis], bboxes2[:, 1])\n",
    "        xB = np.minimum(bboxes1[:, 2][:, np.newaxis], bboxes2[:, 2])\n",
    "        yB = np.minimum(bboxes1[:, 3][:, np.newaxis], bboxes2[:, 3])\n",
    "\n",
    "        # Compute the area of intersection\n",
    "        interArea = np.maximum(0, xB - xA) * np.maximum(0, yB - yA)\n",
    "\n",
    "        # Compute the area of both sets of bounding boxes\n",
    "        boxAArea = (bboxes1[:, 2] - bboxes1[:, 0]) * (bboxes1[:, 3] - bboxes1[:, 1])\n",
    "        boxBArea = (bboxes2[:, 2] - bboxes2[:, 0]) * (bboxes2[:, 3] - bboxes2[:, 1])\n",
    "\n",
    "        # Compute the IoU, adding a small epsilon to avoid division by zero\n",
    "        iou = interArea / (boxAArea[:, np.newaxis] + boxBArea - interArea + 1e-6)\n",
    "        \n",
    "        return iou\n",
    "    \n",
    "    def _create_kalman_filter(self):\n",
    "        \"\"\"\n",
    "        Creates a new Kalman Filter configured for tracking bounding boxes.\n",
    "        The state is [cx, cy, w, h, vx, vy, vw, vh] - 8 variables.\n",
    "        The measurement is [cx, cy, w, h] - 4 variables.\n",
    "        \"\"\"\n",
    "        kf = cv2.KalmanFilter(8, 4)\n",
    "        # State transition matrix (F) - constant velocity model\n",
    "        kf.transitionMatrix = np.array([\n",
    "            [1, 0, 0, 0, 1, 0, 0, 0],\n",
    "            [0, 1, 0, 0, 0, 1, 0, 0],\n",
    "            [0, 0, 1, 0, 0, 0, 1, 0],\n",
    "            [0, 0, 0, 1, 0, 0, 0, 1],\n",
    "            [0, 0, 0, 0, 1, 0, 0, 0],\n",
    "            [0, 0, 0, 0, 0, 1, 0, 0],\n",
    "            [0, 0, 0, 0, 0, 0, 1, 0],\n",
    "            [0, 0, 0, 0, 0, 0, 0, 1]], np.float32)\n",
    "        # Measurement matrix (H) - we only measure position and size\n",
    "        kf.measurementMatrix = np.array([\n",
    "            [1, 0, 0, 0, 0, 0, 0, 0],\n",
    "            [0, 1, 0, 0, 0, 0, 0, 0],\n",
    "            [0, 0, 1, 0, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 1, 0, 0, 0, 0]], np.float32)\n",
    "        \n",
    "        # Process noise covariance (Q) - accounts for uncertainty in the model\n",
    "        # Tuned for more uncertainty in velocity\n",
    "        kf.processNoiseCov = np.eye(8, dtype=np.float32) * 0.03\n",
    "        kf.processNoiseCov[4:, 4:] *= 10\n",
    "        \n",
    "        # Measurement noise covariance (R) - accounts for uncertainty in the measurement\n",
    "        kf.measurementNoiseCov = np.eye(4, dtype=np.float32) * 0.1\n",
    "        \n",
    "        return kf\n",
    "\n",
    "    def _apply_matching(self, iou_matrix, iou_threshold):\n",
    "        \"\"\"\n",
    "        Performs optimal matching between tracks and detections using the\n",
    "        Hungarian algorithm.\n",
    "        \"\"\"\n",
    "        # Use Hungarian algorithm for optimal assignment. We want to maximize\n",
    "        # IoU, so we use (1 - IoU) as the cost for the assignment problem.\n",
    "        cost_matrix = 1 - iou_matrix\n",
    "        track_indices, detection_indices = linear_sum_assignment(cost_matrix)\n",
    "        \n",
    "        matched_pairs = []\n",
    "        unmatched_track_indices = set(range(iou_matrix.shape[0]))\n",
    "        unmatched_detection_indices = set(range(iou_matrix.shape[1]))\n",
    "        \n",
    "        # Filter out matches that are below the IoU threshold\n",
    "        for t_idx, d_idx in zip(track_indices, detection_indices):\n",
    "            if iou_matrix[t_idx, d_idx] >= iou_threshold:\n",
    "                matched_pairs.append((t_idx, d_idx))\n",
    "                unmatched_track_indices.discard(t_idx)\n",
    "                unmatched_detection_indices.discard(d_idx)\n",
    "                \n",
    "        return matched_pairs, list(unmatched_track_indices), list(unmatched_detection_indices)\n",
    "\n",
    "    def detect(self, frame):\n",
    "        results = self.model(frame, verbose=False)[0]\n",
    "        detections = []\n",
    "        for box in results.boxes:\n",
    "            conf = box.conf[0].item()\n",
    "            if conf > self.conf_threshold:\n",
    "                class_name = self.model.names[int(box.cls[0].item())]\n",
    "                coords = box.xyxy[0].tolist()\n",
    "                detections.append({\n",
    "                    'class_name': class_name,\n",
    "                    'x1': int(coords[0]), 'y1': int(coords[1]),\n",
    "                    'x2': int(coords[2]), 'y2': int(coords[3]),\n",
    "                    'conf': conf\n",
    "                })\n",
    "        return detections\n",
    "\n",
    "    def draw_boxes(self, frame, objects, default_color='random'):\n",
    "        frame_copy = frame.copy()\n",
    "\n",
    "        for obj in objects:\n",
    "            x1, y1, x2, y2 = obj['x1'], obj['y1'], obj['x2'], obj['y2']\n",
    "            label = f'{obj['class_name']} {obj['conf']:.2f}'\n",
    "            if 'color' in obj:\n",
    "                box_color = obj['color']\n",
    "            else:\n",
    "                box_color = np.random.uniform(0, 255, 3).tolist() if default_color == 'random' else default_color\n",
    "\n",
    "            box_w = x2 - x1\n",
    "            font_scale = max(0.5, box_w / 150)\n",
    "            font_thickness = max(1, int(box_w / 100))\n",
    "\n",
    "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "            (tw, th), bl = cv2.getTextSize(label, font, font_scale, font_thickness)\n",
    "\n",
    "\n",
    "            # Adaptive text color\n",
    "            brightness_vect = np.array([0.114, 0.587, 0.299])\n",
    "            brightness = np.dot(box_color, brightness_vect)\n",
    "            text_color = (0,0,0) if brightness > 128 else (255,255,255)\n",
    "\n",
    "            cv2.rectangle(frame_copy, (x1, y1 - th - 8), (x1 + tw, y1), box_color, -1)\n",
    "            cv2.rectangle(frame_copy, (x1, y1), (x2, y2), box_color, 3)\n",
    "            cv2.putText(frame_copy, label, (x1, y1 - 4), font, font_scale, text_color, font_thickness, cv2.LINE_AA)\n",
    "            \n",
    "        return frame_copy\n",
    "    \n",
    "    def plot_image(self, frame, size_mult=1.0, frame_title=False, axis=False):\n",
    "        h, w = frame.shape[:2]\n",
    "        base_figsize = (w / 100, h / 100)\n",
    "        fig_w, fig_h = base_figsize[0]*size_mult, base_figsize[1]*size_mult\n",
    "        plt.figure(figsize=(fig_w, fig_h))\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        plt.imshow(frame_rgb)\n",
    "        plt.axis(axis)\n",
    "        if frame_title != False:\n",
    "            plt.title(frame_title)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f61c42",
   "metadata": {},
   "source": [
    "## Sample Detection Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09da1649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_img = cv2.imread('./assets/images/0001.jpg')\n",
    "\n",
    "# test_tracker = ObjectTracker(model)\n",
    "# test_img_detections = test_tracker.detect(test_img)\n",
    "# test_tracker.plot_image(test_tracker.draw_boxes(test_img, test_img_detections))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77598323",
   "metadata": {},
   "source": [
    "# 2: Tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb94019",
   "metadata": {},
   "source": [
    "## VideoPlayer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e49fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoPlayer:\n",
    "    def __init__(self, source, size_multiplier=1.0, playback_speed=1.0, window_title=\"Video Playback\"):\n",
    "        self.cap = cv2.VideoCapture(source)\n",
    "        self.window_title = window_title\n",
    "        \n",
    "        # Get video properties from the underlying stream\n",
    "        self.fps = self.cap.get(cv2.CAP_PROP_FPS)\n",
    "        self.frame_width = int(self.cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        self.frame_height = int(self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        self.total_frames = self.cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "        \n",
    "        # Calculate target delay based on original FPS and desired playback speed\n",
    "        self.target_delay_ms = (1000 / self.fps) / playback_speed if self.fps > 0 else 33\n",
    "\n",
    "        # Create and resize the display window\n",
    "        cv2.namedWindow(self.window_title, cv2.WINDOW_NORMAL)\n",
    "        cv2.resizeWindow(\n",
    "            self.window_title,\n",
    "            int(self.frame_width * size_multiplier),\n",
    "            int(self.frame_height * size_multiplier)\n",
    "        )\n",
    "        print(\"--- Video Player Initialized ---\")\n",
    "        print(f\"  Resolution: {self.frame_width}x{self.frame_height}\")\n",
    "        print(f\"  Original FPS: {self.fps:.2f}\")\n",
    "        print(f\"  Total Frames: {self.total_frames}\")\n",
    "        print(f\"  Playback Speed: {playback_speed}x\")\n",
    "        print(f\"  Target Delay: {self.target_delay_ms:.2f} ms\")\n",
    "        print(\"--------------------------------\")\n",
    "\n",
    "    def play(self, tracker):\n",
    "        print(\"Starting playback... Press 'q' to quit.\")\n",
    "        last_time = time.time()\n",
    "        \n",
    "        while True:\n",
    "            start_time = time.perf_counter()\n",
    "            \n",
    "            ret, frame = self.cap.read()\n",
    "            \n",
    "            if not ret:\n",
    "                print(\"Video stream ended.\")\n",
    "                break\n",
    "\n",
    "            # Process the object tracking\n",
    "            processed_frame = tracker.process_frame(frame)\n",
    "\n",
    "            # Adding FPS Annotation\n",
    "            fps = 1 / (time.time() - last_time)\n",
    "            last_time = time.time()\n",
    "            cv2.putText(processed_frame, f\"FPS: {fps:.2f}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "            cv2.imshow(self.window_title, processed_frame)\n",
    "\n",
    "            # Calculate the actual delay needed to maintain the target playback speed\n",
    "            processing_time_ms = (time.perf_counter() - start_time) * 1000\n",
    "            real_delay_ms = int(self.target_delay_ms - processing_time_ms)\n",
    "            \n",
    "            # Exit if 'q' is pressed\n",
    "            wait_key = cv2.waitKey(max(1, real_delay_ms))\n",
    "            if wait_key & 0xFF == ord('q'):\n",
    "                print(\"Playback stopped by user.\")\n",
    "                break\n",
    "            \n",
    "        self.release()\n",
    "\n",
    "    def release(self):\n",
    "        \"\"\"Stops the reader thread and closes all OpenCV windows.\"\"\"\n",
    "        print(\"Releasing resources...\")\n",
    "        self.cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        # Add a small delay to ensure windows close properly on all systems\n",
    "        for _ in range(5):\n",
    "            cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4ec198",
   "metadata": {},
   "source": [
    "## Test Playback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adef366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can change these parameters\n",
    "VIDEO_PATH = './assets/footage/person4.mp4' # Make sure this path is correct\n",
    "MODEL_PATH = './yolo11n.pt'\n",
    "PLAYBACK_SPEED = 1.5 # Play at 1.5x speed\n",
    "WINDOW_SIZE = 0.5   # Display window at 75% of original size\n",
    "\n",
    "try:\n",
    "    tracker = ObjectTracker(model, 'NUMPY_CSRT')\n",
    "    player = VideoPlayer(\n",
    "        source=VIDEO_PATH,\n",
    "        playback_speed=PLAYBACK_SPEED,\n",
    "        size_multiplier=WINDOW_SIZE,\n",
    "        window_title=\"High-Performance Player\"\n",
    "    )\n",
    "    player.play(tracker)\n",
    "except IOError as e:\n",
    "    print(e)\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "signalenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
