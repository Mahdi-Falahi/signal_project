{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "c52d8f99",
            "metadata": {},
            "source": [
                "<div align=\"center\">\n",
                "  <a href=\"http://www.sharif.edu/\">\n",
                "    <img src=\"https://cdn.freebiesupply.com/logos/large/2x/sharif-logo-png-transparent.png\" alt=\"SUT Logo\" width=\"140\">\n",
                "  </a>\n",
                "  \n",
                "  # Sharif University of Technology\n",
                "  ### Electrical Engineering Department\n",
                "\n",
                "  ## Signals and Systems\n",
                "  #### *Final Project - Spring 2025*\n",
                "</div>\n",
                "\n",
                "---\n",
                "\n",
                "<div align=\"center\">\n",
                "  <h1>\n",
                "    <b>Object Tracker</b>\n",
                "  </h1>\n",
                "  <p>\n",
                "    An object tracking system using YOLO for detection and various algorithms (KCF, CSRT, MOSSE) for tracking.\n",
                "  </p>\n",
                "</div>\n",
                "\n",
                "<br>\n",
                "\n",
                "| Professor                  |\n",
                "| :-------------------------: |\n",
                "| Dr. Mohammad Mehdi Mojahedian |\n",
                "\n",
                "<br>\n",
                "\n",
                "| Contributors              |\n",
                "| :-----------------------: |\n",
                "| **Amirreza Mousavi** |\n",
                "| **Mahdi Falahi** |\n",
                "| **Zahra Miladipour** |"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f6017895",
            "metadata": {},
            "source": [
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "70e8b3de",
            "metadata": {},
            "source": [
                "## Imports"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0d3b8cc9",
            "metadata": {},
            "outputs": [],
            "source": [
                "import cv2\n",
                "import numpy as np\n",
                "from ultralytics import YOLO\n",
                "import time\n",
                "import torch\n",
                "from scipy.optimize import linear_sum_assignment\n",
                "import os\n",
                "import torchreid\n",
                "from PIL import Image\n",
                "from sklearn.metrics.pairwise import cosine_similarity\n",
                "from collections import deque"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ac40b6c5",
            "metadata": {},
            "source": [
                "## Custom Kcf"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "dfd156f1",
            "metadata": {},
            "outputs": [],
            "source": [
                "class KCFParams:\n",
                "    \"\"\"\n",
                "    A data class to hold all configurable parameters for the KCF tracker.\n",
                "    \"\"\"\n",
                "    def __init__(self):\n",
                "        self.detect_thresh = 0.1\n",
                "        self.sigma = 0.1\n",
                "        self.lambda_ = 0.0001\n",
                "        self.interp_factor = 0.1\n",
                "        self.output_sigma_factor = 1.0 / 16.0\n",
                "        self.resize = True\n",
                "        self.max_patch_size = 80 * 80\n",
                "        self.split_coeff = True\n",
                "        self.wrap_kernel = False\n",
                "        self.desc_npca = 'GRAY'\n",
                "        self.desc_pca = 'CN'\n",
                "        self.compress_feature = True\n",
                "        self.compressed_size = 2\n",
                "        self.pca_learning_rate = 0.15\n",
                "\n",
                "class TrackerKCF:\n",
                "    \"\"\"\n",
                "    Python implementation of the Kernelized Correlation Filter (KCF) tracker.\n",
                "    \"\"\"\n",
                "    def __init__(self, parameters=KCFParams()):\n",
                "        self.params = parameters\n",
                "        self.roi = None\n",
                "        self.frame = 0\n",
                "        self.resize_image = False\n",
                "        self.output_sigma = 0.0\n",
                "        self.yf = None\n",
                "        self.alphaf = None\n",
                "        self.alphaf_den = None\n",
                "        self.z = None\n",
                "        self.x = None\n",
                "        self.hann = None\n",
                "        self.hann_cn = None\n",
                "        self.features_pca = []\n",
                "        self.features_npca = []\n",
                "        self.descriptors_pca = []\n",
                "        self.descriptors_npca = []\n",
                "        self.proj_mtx = None\n",
                "        self.old_cov_mtx = None\n",
                "        self.use_custom_extractor_pca = False\n",
                "        self.use_custom_extractor_npca = False\n",
                "        self.extractor_pca = []\n",
                "        self.extractor_npca = []\n",
                "        self.X = {}\n",
                "        self.Z = {}\n",
                "        self.Zc = {}\n",
                "        self.k = None\n",
                "        self.kf = None\n",
                "        self.kf_lambda = None\n",
                "        self.new_alphaf = None\n",
                "        self.response = None\n",
                "        self._color_names_table = None\n",
                "\n",
                "    def init(self, image, boundingBox):\n",
                "        if len(image.shape) == 2 or image.shape[2] == 1:\n",
                "            self.params.desc_pca = self.params.desc_pca.replace('CN', '')\n",
                "            self.params.desc_npca = self.params.desc_npca.replace('CN', '')\n",
                "\n",
                "        self.frame = 0\n",
                "        x, y, w, h = boundingBox\n",
                "        img_to_process = image\n",
                "        if self.params.resize and w * h > self.params.max_patch_size:\n",
                "            self.resize_image = True\n",
                "            x /= 2.0\n",
                "            y /= 2.0\n",
                "            w /= 2.0\n",
                "            h /= 2.0\n",
                "            img_to_process = cv2.resize(image, (image.shape[1] // 2, image.shape[0] // 2), interpolation=cv2.INTER_LINEAR_EXACT)\n",
                "\n",
                "        roi_x = x - w / 2\n",
                "        roi_y = y - h / 2\n",
                "        roi_w = w * 2\n",
                "        roi_h = h * 2\n",
                "        self.roi = (roi_x, roi_y, roi_w, roi_h)\n",
                "\n",
                "        output_sigma_val = np.sqrt(w * h) * self.params.output_sigma_factor\n",
                "        self.output_sigma = -0.5 / (output_sigma_val * output_sigma_val)\n",
                "\n",
                "        win_size = (int(roi_w), int(roi_h))\n",
                "        self.hann = self._createHanningWindow(win_size)\n",
                "        self.hann_cn = np.dstack([self.hann] * 10)\n",
                "\n",
                "        sz_y, sz_x = int(roi_h), int(roi_w)\n",
                "        j, i = np.meshgrid(np.arange(sz_x), np.arange(sz_y))\n",
                "        cy, cx = sz_y // 2, sz_x // 2\n",
                "        dist_sq = (i - cy + 1)**2 + (j - cx + 1)**2\n",
                "        y = np.exp(self.output_sigma * dist_sq)\n",
                "        self.yf = self._fft2(y)\n",
                "\n",
                "        if 'GRAY' in self.params.desc_npca.upper(): self.descriptors_npca.append('GRAY')\n",
                "        if 'CN' in self.params.desc_npca.upper(): self.descriptors_npca.append('CN')\n",
                "        if self.use_custom_extractor_npca: self.descriptors_npca.append('CUSTOM')\n",
                "\n",
                "        if 'GRAY' in self.params.desc_pca.upper(): self.descriptors_pca.append('GRAY')\n",
                "        if 'CN' in self.params.desc_pca.upper(): self.descriptors_pca.append('CN')\n",
                "        if self.use_custom_extractor_pca: self.descriptors_pca.append('CUSTOM')\n",
                "\n",
                "        features_pca, features_npca = self._extractFeatures(img_to_process, self.roi)\n",
                "        if not features_pca and not features_npca:\n",
                "            raise ValueError(\"No valid features extracted during initialization\")\n",
                "\n",
                "        if features_npca: self.Z[1] = np.dstack(features_npca)\n",
                "        if features_pca: self.Z[0] = np.dstack(features_pca)\n",
                "        \n",
                "        if self.params.compress_feature and self.Z.get(0) is not None:\n",
                "            self._updateProjectionMatrix(self.Z[0], self.params.pca_learning_rate, self.params.compressed_size)\n",
                "\n",
                "        x_parts = [self.Z.get(0), self.Z.get(1)]\n",
                "        if self.params.compress_feature and x_parts[0] is not None:\n",
                "            x_parts[0] = self._compress(x_parts[0])\n",
                "        self.x = np.dstack([p for p in x_parts if p is not None])\n",
                "\n",
                "        k = self._denseGaussKernel(self.params.sigma, self.x)\n",
                "        kf = self._fft2(k)\n",
                "        kf_lambda = kf + self.params.lambda_\n",
                "        if self.params.split_coeff:\n",
                "            self.alphaf = self._pixelWiseMult(self.yf, kf)\n",
                "            self.alphaf_den = self._pixelWiseMult(kf, kf_lambda)\n",
                "        else:\n",
                "            self.alphaf = self.yf / (kf_lambda + 1e-10)\n",
                "\n",
                "    def update(self, image):\n",
                "        img_to_process = image\n",
                "        if self.resize_image:\n",
                "            img_to_process = cv2.resize(image, (image.shape[1] // 2, image.shape[0] // 2), interpolation=cv2.INTER_LINEAR_EXACT)\n",
                "        \n",
                "        if self.frame > 0:\n",
                "            features_pca, features_npca = self._extractFeatures(img_to_process, self.roi)\n",
                "            if not features_pca and not features_npca:\n",
                "                return False, None\n",
                "\n",
                "            if features_npca: self.X[1] = np.dstack(features_npca)\n",
                "            if features_pca: self.X[0] = np.dstack(features_pca)\n",
                "\n",
                "            if self.params.compress_feature and self.X.get(0) is not None:\n",
                "                self.X[0] = self._compress(self.X[0])\n",
                "                self.Zc[0] = self._compress(self.Z[0])\n",
                "            else:\n",
                "                self.Zc[0] = self.Z.get(0)\n",
                "            self.Zc[1] = self.Z.get(1)\n",
                "\n",
                "            x_parts = [self.X.get(0), self.X.get(1)]\n",
                "            z_parts = [self.Zc.get(0), self.Zc.get(1)]\n",
                "            x = np.dstack([p for p in x_parts if p is not None])\n",
                "            z = np.dstack([p for p in z_parts if p is not None])\n",
                "            \n",
                "            k = self._denseGaussKernel(self.params.sigma, x, z)\n",
                "            \n",
                "            kf = self._fft2(k)\n",
                "            self.response = self._calcResponse(self.alphaf, kf)\n",
                "            \n",
                "            peak_y, peak_x = np.unravel_index(np.argmax(self.response), self.response.shape)\n",
                "            \n",
                "            if self.response[peak_y, peak_x] < self.params.detect_thresh:\n",
                "                return False, None\n",
                "\n",
                "            disp_y = peak_y - self.response.shape[0] // 2 + 1\n",
                "            disp_x = peak_x - self.response.shape[1] // 2 + 1\n",
                "\n",
                "            self.roi = (self.roi[0] + disp_x, self.roi[1] + disp_y, self.roi[2], self.roi[3])\n",
                "\n",
                "        features_pca, features_npca = self._extractFeatures(img_to_process, self.roi)\n",
                "        if not features_pca and not features_npca:\n",
                "            return False, None\n",
                "            \n",
                "        if features_npca: self.X[1] = np.dstack(features_npca)\n",
                "        if features_pca: self.X[0] = np.dstack(features_pca)\n",
                "        \n",
                "        interp = self.params.interp_factor\n",
                "        if self.X.get(0) is not None:\n",
                "            self.Z[0] = (1 - interp) * self.Z[0] + interp * self.X[0]\n",
                "        if self.X.get(1) is not None:\n",
                "            self.Z[1] = (1 - interp) * self.Z[1] + interp * self.X[1]\n",
                "\n",
                "        if self.params.compress_feature and self.Z.get(0) is not None:\n",
                "            self._updateProjectionMatrix(self.Z[0], self.params.pca_learning_rate, self.params.compressed_size)\n",
                "        \n",
                "        x_parts = [self.X.get(0), self.X.get(1)]\n",
                "        if self.params.compress_feature and x_parts[0] is not None:\n",
                "            x_parts[0] = self._compress(x_parts[0])\n",
                "\n",
                "        x = np.dstack([p for p in x_parts if p is not None])\n",
                "\n",
                "        k = self._denseGaussKernel(self.params.sigma, x)\n",
                "        kf = self._fft2(k)\n",
                "        kf_lambda = kf + self.params.lambda_\n",
                "        if self.params.split_coeff:\n",
                "            new_alphaf = self._pixelWiseMult(self.yf, kf)\n",
                "            new_alphaf_den = self._pixelWiseMult(kf, kf_lambda)\n",
                "        else:\n",
                "            new_alphaf = self.yf / (kf_lambda + 1e-10)\n",
                "\n",
                "        self.alphaf = (1 - interp) * self.alphaf + interp * new_alphaf\n",
                "        if self.params.split_coeff:\n",
                "            self.alphaf_den = (1 - interp) * self.alphaf_den + interp * new_alphaf_den\n",
                "\n",
                "        rx, ry, rw, rh = self.roi\n",
                "        obj_w, obj_h = rw / 2, rh / 2\n",
                "        obj_x, obj_y = rx + rw / 2, ry + rh / 2\n",
                "        \n",
                "        if self.resize_image:\n",
                "            obj_x *= 2\n",
                "            obj_y *= 2\n",
                "            obj_w *= 2\n",
                "            obj_h *= 2\n",
                "\n",
                "        img_h, img_w = image.shape[:2]\n",
                "        left = max(0.0, obj_x - obj_w / 2)\n",
                "        top = max(0.0, obj_y - obj_h / 2)\n",
                "        right = min(float(img_w), obj_x + obj_w / 2)\n",
                "        bottom = min(float(img_h), obj_y + obj_h / 2)\n",
                "        final_bbox = (left, top, right - left, bottom - top)\n",
                "        \n",
                "        self.frame += 1\n",
                "        return True, final_bbox\n",
                "\n",
                "    def setFeatureExtractor(self, callback, pca_func=False):\n",
                "        if pca_func:\n",
                "            self.extractor_pca.append(callback)\n",
                "            self.use_custom_extractor_pca = True\n",
                "        else:\n",
                "            self.extractor_npca.append(callback)\n",
                "            self.use_custom_extractor_npca = True\n",
                "\n",
                "    def _fft2(self, src):\n",
                "        return np.fft.fft2(src, axes=(0, 1))\n",
                "\n",
                "    def _ifft2(self, src):\n",
                "        return np.real(np.fft.ifft2(src, axes=(0, 1)))\n",
                "\n",
                "    def _pixelWiseMult(self, s1, s2, conjB=False):\n",
                "        return s1 * np.conj(s2) if conjB else s1 * s2\n",
                "\n",
                "    def _sumChannels(self, src):\n",
                "        return np.sum(src, axis=2)\n",
                "\n",
                "    def _createHanningWindow(self, s):\n",
                "        return cv2.createHanningWindow(s, cv2.CV_32F)\n",
                "\n",
                "    def _shift(self, mat, dx, dy):\n",
                "        return np.roll(mat, (dy, dx), axis=(0, 1))\n",
                "    \n",
                "    def _extractFeatures(self, image, roi):\n",
                "        features_npca = []\n",
                "        for d in self.descriptors_npca:\n",
                "            if d != 'CUSTOM':\n",
                "                f = self._getSubWindow(image, roi, d)\n",
                "                if f is not None:\n",
                "                    if f.ndim == 2:\n",
                "                        f = f[:, :, np.newaxis]\n",
                "                    features_npca.append(f)\n",
                "        features_pca = []\n",
                "        for d in self.descriptors_pca:\n",
                "            if d != 'CUSTOM':\n",
                "                f = self._getSubWindow(image, roi, d)\n",
                "                if f is not None:\n",
                "                    if f.ndim == 2:\n",
                "                        f = f[:, :, np.newaxis]\n",
                "                    features_pca.append(f)\n",
                "\n",
                "        for extractor in self.extractor_npca:\n",
                "            feat = extractor(image, roi)\n",
                "            if feat is not None:\n",
                "                if feat.ndim == 2:\n",
                "                    feat = feat[:, :, np.newaxis]\n",
                "                hann_win = np.dstack([self.hann] * feat.shape[2]) if feat.ndim == 3 and feat.shape[2] > 1 else self.hann\n",
                "                features_npca.append(feat * hann_win)\n",
                "        \n",
                "        for extractor in self.extractor_pca:\n",
                "            feat = extractor(image, roi)\n",
                "            if feat is not None:\n",
                "                if feat.ndim == 2:\n",
                "                    feat = feat[:, :, np.newaxis]\n",
                "                hann_win = np.dstack([self.hann] * feat.shape[2]) if feat.ndim == 3 and feat.shape[2] > 1 else self.hann\n",
                "                features_pca.append(feat * hann_win)\n",
                "            \n",
                "        return features_pca, features_npca\n",
                "\n",
                "    def _getSubWindow(self, img, roi, desc):\n",
                "        x, y, w, h = int(roi[0]), int(roi[1]), int(roi[2]), int(roi[3])\n",
                "        img_h, img_w = img.shape[:2]\n",
                "        vx, vy = max(0, x), max(0, y)\n",
                "        rb, bb = min(x + w, img_w), min(y + h, img_h)\n",
                "        vw, vh = rb - vx, bb - vy\n",
                "        if vw <= 0 or vh <= 0: return None\n",
                "        patch = img[vy:vy + vh, vx:vx + vw]\n",
                "        top, bot = vy - y, (y + h) - bb\n",
                "        left, rgt = vx - x, (x + w) - rb\n",
                "        patch = cv2.copyMakeBorder(patch, top, bot, left, rgt, cv2.BORDER_REPLICATE)\n",
                "        if patch.size == 0: return None\n",
                "        if desc == 'GRAY': return self._extractGray(patch)\n",
                "        elif desc == 'CN': return self._extractCN(patch)\n",
                "        return None\n",
                "\n",
                "    def _extractGray(self, p):\n",
                "        if p is None or p.size == 0: return None\n",
                "        f = cv2.cvtColor(p, cv2.COLOR_BGR2GRAY) if p.ndim > 2 else p\n",
                "        f = f.astype(np.float32) / 255.0 - 0.5\n",
                "        return f * self.hann\n",
                "\n",
                "    def _extractCN(self, p):\n",
                "        if p is None or p.size == 0: return None\n",
                "        if self._color_names_table is None: self._loadColorNamesTable()\n",
                "        px = p.reshape(-1, 3).astype(np.int32)\n",
                "        idx = (px[:, 2] // 8) + 32 * (px[:, 1] // 8) + 1024 * (px[:, 0] // 8)\n",
                "        f = self._color_names_table[idx].reshape(p.shape[0], p.shape[1], 10).astype(np.float32)\n",
                "        return f * self.hann_cn\n",
                "\n",
                "    def _loadColorNamesTable(self, path=\"colornames.npy\"):\n",
                "        try:\n",
                "            self._color_names_table = np.load(path)\n",
                "        except IOError:\n",
                "            raise IOError(f\"Color table '{path}' not found. Ensure the file exists or generate it from the original ColorNames array.\")\n",
                "\n",
                "    def _denseGaussKernel(self, sigma, x, y=None):\n",
                "        if y is None: y = x\n",
                "        xf = self._fft2(x)\n",
                "        yf = self._fft2(y)\n",
                "        nx_sq = np.sum(x**2)\n",
                "        ny_sq = np.sum(y**2)\n",
                "        xyf = self._pixelWiseMult(xf, yf, conjB=True)\n",
                "        xy_sum = self._sumChannels(xyf)\n",
                "        xy = self._ifft2(xy_sum)\n",
                "        if self.params.wrap_kernel:\n",
                "            shift_dy = int(x.shape[0] // 2)\n",
                "            shift_dx = int(x.shape[1] // 2)\n",
                "            xy = self._shift(xy, shift_dx, shift_dy)\n",
                "        d = (nx_sq + ny_sq - 2 * xy) / x.size\n",
                "        k = np.exp(-np.maximum(0, d) / (sigma**2))\n",
                "        return k\n",
                "\n",
                "    def _calcResponse(self, alphaf, kf):\n",
                "        if not self.params.split_coeff:\n",
                "            spec = self._pixelWiseMult(alphaf, kf)\n",
                "            return self._ifft2(spec)\n",
                "        else:\n",
                "            spec = self._pixelWiseMult(alphaf, kf)\n",
                "            response_spec = spec / (self.alphaf_den + 1e-10)\n",
                "            return self._ifft2(response_spec)\n",
                "\n",
                "    def _updateProjectionMatrix(self, src, pca_rate, compressed_sz):\n",
                "        num_pixels, num_channels = src.shape[0] * src.shape[1], src.shape[2]\n",
                "        data = src.reshape(num_pixels, num_channels)\n",
                "        mean = np.mean(data, axis=0)\n",
                "        data_nomean = data - mean\n",
                "        new_cov = (data_nomean.T @ data_nomean) / (num_pixels - 1)\n",
                "        if self.old_cov_mtx is None: self.old_cov_mtx = new_cov\n",
                "        self.old_cov_mtx = (1 - pca_rate) * self.old_cov_mtx + pca_rate * new_cov\n",
                "        u, s, _ = np.linalg.svd(self.old_cov_mtx)\n",
                "        self.proj_mtx = u[:, :compressed_sz]\n",
                "        proj_vars = np.diag(s[:compressed_sz])\n",
                "        stab_term = self.proj_mtx @ proj_vars @ self.proj_mtx.T\n",
                "        self.old_cov_mtx = (1 - pca_rate) * self.old_cov_mtx + pca_rate * stab_term\n",
                "\n",
                "    def _compress(self, src):\n",
                "        data = src.reshape(-1, src.shape[2])\n",
                "        compressed = data @ self.proj_mtx\n",
                "        return compressed.reshape(src.shape[0], src.shape[1], -1)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "bb2233de",
            "metadata": {},
            "source": [
                "## Custom Tracker"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "564de46b",
            "metadata": {},
            "outputs": [],
            "source": [
                "import cv2\n",
                "import numpy as np\n",
                "\n",
                "class CustomTracker:\n",
                "    def __init__(self, psr_threshold=3.5, filter_alpha=0.025, scale_lr=0.02, lambda_trust=0.001):\n",
                "        # Parameters not used in this optical flow version, but included for compatibility\n",
                "        self.PSR_THRESHOLD = psr_threshold\n",
                "        self.FILTER_ALPHA = filter_alpha\n",
                "        self.SCALE_LR = scale_lr\n",
                "        self.LAMBDA = lambda_trust\n",
                "        self.p0 = None\n",
                "        self.old_gray = None\n",
                "        self.bbox = None\n",
                "        self.lk_params = dict(winSize=(15, 15),\n",
                "                              maxLevel=2,\n",
                "                              criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
                "        self.feature_params = dict(maxCorners=200,\n",
                "                                   qualityLevel=0.3,\n",
                "                                   minDistance=7,\n",
                "                                   blockSize=7)\n",
                "        self.max_scale_change = 0.2  # Limit per-frame scale change\n",
                "        self.fb_error_threshold = 2.0  # Forward-backward error threshold from MedianFlow\n",
                "        self.kalman = None  # Kalman filter from SORT adaptation\n",
                "        self.min_points = 10  # Increased for stability\n",
                "        self.min_scale = 0.8  # Prevent excessive shrinkage (relative to previous frame)\n",
                "\n",
                "    def _init_kalman(self):\n",
                "        # 6D state: [x, y, vx, vy, s, vs] where s is scale, vs is scale velocity\n",
                "        kalman = cv2.KalmanFilter(6, 3)  # Measure x, y, s\n",
                "        kalman.transitionMatrix = np.eye(6, dtype=np.float32)\n",
                "        kalman.transitionMatrix[0, 2] = 1.0  # x += vx\n",
                "        kalman.transitionMatrix[1, 3] = 1.0  # y += vy\n",
                "        kalman.transitionMatrix[4, 5] = 1.0  # s += vs\n",
                "        kalman.measurementMatrix = np.zeros((3, 6), dtype=np.float32)\n",
                "        kalman.measurementMatrix[0, 0] = 1.0\n",
                "        kalman.measurementMatrix[1, 1] = 1.0\n",
                "        kalman.measurementMatrix[2, 4] = 1.0\n",
                "        kalman.processNoiseCov = np.eye(6, dtype=np.float32) * 0.05  # Increased for more flexibility\n",
                "        kalman.measurementNoiseCov = np.eye(3, dtype=np.float32) * 0.05  # Lowered to trust measurements more (less lag)\n",
                "        kalman.errorCovPost = np.eye(6, dtype=np.float32) * 0.1\n",
                "        return kalman\n",
                "\n",
                "    def init(self, frame, bbox):\n",
                "        x, y, w, h = map(int, bbox)\n",
                "        self.bbox = (x, y, w, h)\n",
                "        if len(frame.shape) == 2:  # Grayscale input\n",
                "            gray_frame = frame.copy()\n",
                "        else:\n",
                "            gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
                "        # Apply CLAHE for contrast enhancement\n",
                "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
                "        self.old_gray = clahe.apply(gray_frame)\n",
                "        roi_gray = self.old_gray[y:y+h, x:x+w]\n",
                "        p0 = cv2.goodFeaturesToTrack(roi_gray, mask=None, **self.feature_params)\n",
                "        if p0 is not None and len(p0) > 0:\n",
                "            p0[:,:,0] += x\n",
                "            p0[:,:,1] += y\n",
                "            self.p0 = p0\n",
                "        else:\n",
                "            self.p0 = np.empty((0, 1, 2), dtype=np.float32)\n",
                "            return False\n",
                "\n",
                "        # Initialize Kalman\n",
                "        self.kalman = self._init_kalman()\n",
                "        cx, cy = x + w / 2, y + h / 2\n",
                "        self.kalman.statePost = np.array([cx, cy, 0, 0, 1.0, 0], dtype=np.float32)  # Initial scale=1.0\n",
                "        return True\n",
                "\n",
                "    def update(self, frame):\n",
                "        if len(frame.shape) == 2:  # Grayscale\n",
                "            gray_frame = frame.copy()\n",
                "        else:\n",
                "            gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
                "        # Apply CLAHE\n",
                "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
                "        frame_gray = clahe.apply(gray_frame)\n",
                "        validated_new = np.array([])  # Initialize to empty array to avoid reference errors\n",
                "        if len(self.p0) < self.min_points:\n",
                "            # Dense flow fallback\n",
                "            flow = cv2.calcOpticalFlowFarneback(self.old_gray, frame_gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
                "            dx = np.median(flow[..., 0])\n",
                "            dy = np.median(flow[..., 1])\n",
                "            # Approximate scale from flow divergence (trace of Jacobian)\n",
                "            dx_dx = np.gradient(flow[..., 0])[1]\n",
                "            dy_dy = np.gradient(flow[..., 1])[0]\n",
                "            divergence = dx_dx + dy_dy\n",
                "            meas_scale = 1.0 + np.median(divergence) * 0.01  # Small adjustment based on divergence\n",
                "            meas_scale = np.clip(meas_scale, self.min_scale, 1 + self.max_scale_change)\n",
                "        else:\n",
                "            # Kalman predict\n",
                "            predicted = self.kalman.predict()\n",
                "            pred_cx, pred_cy, _, _, pred_scale, _ = predicted.flatten()\n",
                "            pred_w = self.bbox[2] * pred_scale\n",
                "            pred_h = self.bbox[3] * pred_scale\n",
                "            pred_x = pred_cx - pred_w / 2\n",
                "            pred_y = pred_cy - pred_h / 2\n",
                "\n",
                "            # Adjust LK params for small ROI\n",
                "            roi_area = self.bbox[2] * self.bbox[3]\n",
                "            if roi_area < 5000:\n",
                "                self.lk_params['maxLevel'] = 1\n",
                "                self.lk_params['winSize'] = (10, 10)\n",
                "                self.feature_params['qualityLevel'] = 0.1  # Lower for more points in low-res\n",
                "\n",
                "            # Optical flow forward\n",
                "            p1, st, err = cv2.calcOpticalFlowPyrLK(self.old_gray, frame_gray, self.p0, None, **self.lk_params)\n",
                "\n",
                "            if p1 is None or np.sum(st) < self.min_points:\n",
                "                self.p0 = np.empty((0, 1, 2), dtype=np.float32)\n",
                "                return False, self.bbox\n",
                "\n",
                "            st_flat = st.ravel() == 1\n",
                "            good_new = p1[st_flat].reshape(-1, 2)\n",
                "            good_old = self.p0[st_flat].reshape(-1, 2)\n",
                "\n",
                "            # Forward-backward validation\n",
                "            p0r = good_new.reshape(-1, 1, 2)\n",
                "            p_back, st_back, err_back = cv2.calcOpticalFlowPyrLK(frame_gray, self.old_gray, p0r, None, **self.lk_params)\n",
                "\n",
                "            if p_back is not None:\n",
                "                st_back_flat = st_back.ravel() == 1\n",
                "                p_back_resh = p_back[st_back_flat].reshape(-1, 2)\n",
                "                fb_errors = np.linalg.norm(good_old[st_back_flat] - p_back_resh, axis=1)\n",
                "                self.fb_error_threshold = max(1.0, 0.01 * np.mean([self.bbox[2], self.bbox[3]]))  # Adaptive\n",
                "                valid_submask = fb_errors < self.fb_error_threshold\n",
                "                valid_mask = np.zeros(len(good_old), dtype=bool)\n",
                "                valid_mask[st_back_flat] = valid_submask\n",
                "                validated_new = good_new[valid_mask]\n",
                "                validated_old = good_old[valid_mask]\n",
                "            else:\n",
                "                validated_new = good_new\n",
                "                validated_old = good_old\n",
                "\n",
                "            if len(validated_new) < self.min_points:\n",
                "                return False, self.bbox\n",
                "\n",
                "            # Additional deviation check\n",
                "            med_dx = np.median(validated_new[:, 0] - validated_old[:, 0])\n",
                "            med_dy = np.median(validated_new[:, 1] - validated_old[:, 1])\n",
                "            deviations = np.sqrt((validated_new[:, 0] - validated_old[:, 0] - med_dx)**2 + (validated_new[:, 1] - validated_old[:, 1] - med_dy)**2)\n",
                "            std_dev = np.std(deviations)\n",
                "            dev_mask = deviations < 2 * std_dev\n",
                "            validated_new = validated_new[dev_mask]\n",
                "            validated_old = validated_old[dev_mask]\n",
                "\n",
                "            # Median aggregation\n",
                "            dx = np.median(validated_new[:, 0] - validated_old[:, 0])\n",
                "            dy = np.median(validated_new[:, 1] - validated_old[:, 1])\n",
                "            if len(validated_old) >= 2:\n",
                "                old_dists = np.linalg.norm(validated_old[:, None] - validated_old[None, :], axis=2)\n",
                "                new_dists = np.linalg.norm(validated_new[:, None] - validated_new[None, :], axis=2)\n",
                "                ratios = new_dists[old_dists > 0] / old_dists[old_dists > 0]\n",
                "                meas_scale = np.median(ratios) if len(ratios) > 0 else 1.0\n",
                "                meas_scale = max(self.min_scale, np.clip(meas_scale, self.min_scale, 1 + self.max_scale_change))\n",
                "            else:\n",
                "                meas_scale = 1.0\n",
                "\n",
                "        # Measured values\n",
                "        x, y, w, h = self.bbox\n",
                "        meas_cx = x + w / 2 + dx\n",
                "        meas_cy = y + h / 2 + dy\n",
                "\n",
                "        # Kalman correct with blend for lag reduction\n",
                "        measurement = np.array([meas_cx, meas_cy, meas_scale], dtype=np.float32)\n",
                "        corrected = self.kalman.correct(measurement)\n",
                "        corr_cx, corr_cy, _, _, corr_scale, _ = corrected.flatten()\n",
                "        corr_cx = 0.7 * meas_cx + 0.3 * corr_cx  # Blend to reduce lag\n",
                "        corr_cy = 0.7 * meas_cy + 0.3 * corr_cy\n",
                "        corr_scale = 0.7 * meas_scale + 0.3 * corr_scale\n",
                "\n",
                "        # Update bbox with smoothed values\n",
                "        new_w = max(w * corr_scale, w * self.min_scale)  # Anti-shrinkage\n",
                "        new_h = max(h * corr_scale, h * self.min_scale)\n",
                "        new_x = corr_cx - new_w / 2\n",
                "        new_y = corr_cy - new_h / 2\n",
                "        self.bbox = (int(new_x), int(new_y), int(new_w), int(new_h))\n",
                "\n",
                "        # Recentering based on point centroid\n",
                "        if len(validated_new) > 0:\n",
                "            centroid_x, centroid_y = np.mean(validated_new, axis=0)\n",
                "            center_offset_x = centroid_x - (new_x + new_w / 2)\n",
                "            center_offset_y = centroid_y - (new_y + new_h / 2)\n",
                "            new_x += center_offset_x * 0.5  # Damped adjustment\n",
                "            new_y += center_offset_y * 0.5\n",
                "            self.bbox = (int(new_x), int(new_y), int(new_w), int(new_h))\n",
                "\n",
                "        # Clamp to frame bounds\n",
                "        frame_h, frame_w = frame_gray.shape\n",
                "        new_x = max(0, min(new_x, frame_w - new_w))\n",
                "        new_y = max(0, min(new_y, frame_h - new_h))\n",
                "        self.bbox = (int(new_x), int(new_y), int(new_w), int(new_h))\n",
                "\n",
                "        # Update points and gray frame\n",
                "        self.p0 = validated_new.reshape(-1, 1, 2)\n",
                "        self.old_gray = frame_gray.copy()\n",
                "\n",
                "        # Re-detect features with grid for symmetry\n",
                "        if len(self.p0) < 20:\n",
                "            x, y, w, h = self.bbox\n",
                "            roi_gray = self.old_gray[max(0, y):y+h, max(0, x):x+w]\n",
                "            grid_size = 4\n",
                "            step_y, step_x = max(1, h // grid_size), max(1, w // grid_size)  # Handle small ROIs\n",
                "            new_p0 = []\n",
                "            for gy in range(grid_size):\n",
                "                for gx in range(grid_size):\n",
                "                    sub_y_start = gy * step_y\n",
                "                    sub_y_end = (gy + 1) * step_y\n",
                "                    sub_x_start = gx * step_x\n",
                "                    sub_x_end = (gx + 1) * step_x\n",
                "                    sub_roi = roi_gray[sub_y_start:sub_y_end, sub_x_start:sub_x_end]\n",
                "                    sub_p0 = cv2.goodFeaturesToTrack(sub_roi, **self.feature_params)\n",
                "                    if sub_p0 is not None:\n",
                "                        sub_p0[:,:,0] += sub_x_start + max(0, x)\n",
                "                        sub_p0[:,:,1] += sub_y_start + max(0, y)\n",
                "                        new_p0.append(sub_p0)\n",
                "            if new_p0:\n",
                "                new_p0 = np.vstack(new_p0)\n",
                "                self.p0 = np.vstack((self.p0, new_p0)) if len(self.p0) > 0 else new_p0\n",
                "\n",
                "        return True, self.bbox"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "153643a7",
            "metadata": {},
            "source": [
                "## ObjectTracker Wrapper Class"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "feb41a98",
            "metadata": {},
            "outputs": [],
            "source": [
                "class ObjectTracker:\n",
                "    def __init__(self, model='./yolo11n.pt', tracker_type='CSRT', base_detect_interval=24, conf_threshold=0.5, \n",
                "                 max_lost_frames=30, lost_track_buffer=60,\n",
                "                 use_kalman=True, track_classes=None, \n",
                "                 appearance_weight=0.6, match_cost_threshold=0.85, \n",
                "                 reid_cost_threshold=0.3, occlusion_iou_threshold=0.2,\n",
                "                 iou_gating_threshold=0.1, **kwargs):\n",
                "        \n",
                "        # --- Check Cuda Presence ---\n",
                "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
                "        print(f'object tracker running on {self.device}')\n",
                "\n",
                "        # --- Core Parameters ---\n",
                "        self.detect_interval = base_detect_interval\n",
                "        self.conf_threshold = conf_threshold\n",
                "        self.max_lost_frames = max_lost_frames\n",
                "        self.track_classes = track_classes if track_classes is not None else []\n",
                "        self.model = YOLO(model).to(self.device)\n",
                "        print('--- Yolo loaded successfully ---')\n",
                "        \n",
                "        # --- Cost & Matching Parameters ---\n",
                "        self.appearance_weight = appearance_weight\n",
                "        self.match_cost_threshold = match_cost_threshold\n",
                "        self.reid_cost_threshold = reid_cost_threshold\n",
                "        self.occlusion_iou_threshold = occlusion_iou_threshold\n",
                "        self.iou_gating_threshold = iou_gating_threshold\n",
                "        \n",
                "        # --- State Management ---\n",
                "        self.frame_idx = 0\n",
                "        self.tracked_objects = []\n",
                "        self.lost_tracks = deque(maxlen=lost_track_buffer)\n",
                "        self.next_track_id = 0\n",
                "        \n",
                "        # --- Kalman Filter ---\n",
                "        self.use_kalman = use_kalman\n",
                "\n",
                "        # --- Re-ID Models & Warm-up ---\n",
                "        self.reid_models = self._load_reid_models()\n",
                "        for config in self.reid_models.values():\n",
                "            reid_model = config['model']\n",
                "            dummy_input = torch.randn(1, 3, 256, 128).to(self.device)\n",
                "            with torch.no_grad():\n",
                "                reid_model(dummy_input)\n",
                "        print(\"--- osnet loaded successfuly ---\")\n",
                "\n",
                "        self.tracker_params = {}\n",
                "        if tracker_type.upper() == 'CUSTOM':\n",
                "            self.tracker_params = {\n",
                "                'psr_threshold': kwargs.get('psr_threshold', 5.5),\n",
                "                'filter_alpha': kwargs.get('filter_alpha', 0.025),\n",
                "                'scale_lr': kwargs.get('scale_lr', 0.02),\n",
                "                'lambda_trust': kwargs.get('lambda_trust', 0.01)\n",
                "            }\n",
                "            print(f\"--- Custom tracker configured with params: {self.tracker_params} ---\")\n",
                "\n",
                "        # --- Tracker Constructors ---\n",
                "        self.tracker_constructors = {\n",
                "            'CSRT': cv2.legacy.TrackerCSRT_create, 'KCF': cv2.legacy.TrackerKCF_create,\n",
                "            'MOSSE': cv2.legacy.TrackerMOSSE_create, 'MEDIAN_FLOW': cv2.legacy.TrackerMedianFlow_create,\n",
                "            'CUSTOM': CustomTracker, \"CKCF\":TrackerKCF\n",
                "            \n",
                "        }\n",
                "        if tracker_type.upper() not in self.tracker_constructors:\n",
                "            raise ValueError(f\"Invalid tracker type: {tracker_type}. Choose from {list(self.tracker_constructors.keys())}\")\n",
                "        self.tracker_type = tracker_type.upper()\n",
                "        print(f\"--- Object Tracker Initialized ---\")\n",
                "\n",
                "    def _load_reid_models(self):\n",
                "        \"\"\"Loads pre-trained Re-ID models for different object classes.\"\"\"\n",
                "        models = {}\n",
                "        person_model = torchreid.models.build_model(name='osnet_x1_0', num_classes=4101, pretrained=False)\n",
                "        torchreid.utils.load_pretrained_weights(person_model, 'osnet_x1_0_msmt17_256x128.pth')\n",
                "        person_model.to(self.device).eval()\n",
                "        person_transform, _ = torchreid.data.transforms.build_transforms(height=256, width=128, is_train=False)\n",
                "        models['person'] = {'model': person_model, 'transform': person_transform}\n",
                "        models['car'] = {'model': person_model, 'transform': person_transform}\n",
                "        return models\n",
                "\n",
                "    def _extract_embedding(self, frame, bbox, track):\n",
                "        \"\"\"Extracts a feature embedding from a single bounding box.\"\"\"\n",
                "        reid_config = track.get('reid_config')\n",
                "        if not reid_config: return None\n",
                "        model, transform = reid_config['model'], reid_config['transform']\n",
                "        \n",
                "        x1, y1, x2, y2 = [int(c) for c in bbox]\n",
                "        roi = frame[y1:y2, x1:x2]\n",
                "        if roi.size == 0: return None\n",
                "\n",
                "        roi_rgb = cv2.cvtColor(roi, cv2.COLOR_BGR2RGB)\n",
                "        image_tensor = transform(Image.fromarray(roi_rgb)).unsqueeze(0).to(self.device)\n",
                "        \n",
                "        with torch.no_grad():\n",
                "            embedding = model(image_tensor)\n",
                "        \n",
                "        return torch.nn.functional.normalize(embedding, p=2, dim=1).cpu().numpy().flatten()\n",
                "\n",
                "    def _extract_batch_embeddings(self, frame, detections):\n",
                "        \"\"\"Extracts embeddings for a batch of detections, grouped by class.\"\"\"\n",
                "        grouped_dets = {}\n",
                "        for i, det in enumerate(detections):\n",
                "            cls_name = det['class_name']\n",
                "            if cls_name in self.reid_models:\n",
                "                grouped_dets.setdefault(cls_name, []).append((i, det))\n",
                "\n",
                "        for cls_name, dets_with_indices in grouped_dets.items():\n",
                "            reid_config = self.reid_models[cls_name]\n",
                "            model, transform = reid_config['model'], reid_config['transform']\n",
                "            \n",
                "            rois_with_indices = []\n",
                "            for original_idx, det in dets_with_indices:\n",
                "                x1, y1, x2, y2 = det['x1'], det['y1'], det['x2'], det['y2']\n",
                "                roi = frame[y1:y2, x1:x2]\n",
                "                if roi.size > 0:\n",
                "                    roi_rgb = cv2.cvtColor(roi, cv2.COLOR_BGR2RGB)\n",
                "                    rois_with_indices.append({'roi': Image.fromarray(roi_rgb), 'idx': original_idx})\n",
                "            \n",
                "            if not rois_with_indices: continue\n",
                "\n",
                "            batch_tensor = torch.stack([transform(item['roi']) for item in rois_with_indices]).to(self.device)\n",
                "            with torch.no_grad():\n",
                "                batch_embeddings = model(batch_tensor)\n",
                "            \n",
                "            normalized_embeddings = torch.nn.functional.normalize(batch_embeddings, p=2, dim=1).cpu().numpy()\n",
                "\n",
                "            for item, emb in zip(rois_with_indices, normalized_embeddings):\n",
                "                detections[item['idx']]['embedding'] = emb\n",
                "\n",
                "    def process_frame(self, frame):\n",
                "        \"\"\"Main processing function for each frame.\"\"\"\n",
                "        self._apply_boundary_conditions(frame.shape)\n",
                "\n",
                "        newly_lost = [t for t in self.tracked_objects if t['lost_frames'] >= self.max_lost_frames]\n",
                "        for t in newly_lost:\n",
                "            t['state'] = 'LOST'\n",
                "            self.lost_tracks.append(t)\n",
                "        self.tracked_objects = [t for t in self.tracked_objects if t['lost_frames'] < self.max_lost_frames]\n",
                "\n",
                "        if self.use_kalman: self._predict_phase()\n",
                "        self._update_phase(frame)\n",
                "        \n",
                "        if self.frame_idx % self.detect_interval == 0:\n",
                "            self._match_and_update_phase(frame)\n",
                "\n",
                "        annotated_frame = self._drawing_phase(frame)\n",
                "        self.frame_idx += 1\n",
                "        return annotated_frame\n",
                "    \n",
                "    def _apply_boundary_conditions(self, frame_shape):\n",
                "        if not self.tracked_objects: return\n",
                "        \n",
                "        bboxes = np.array([t['bbox'] for t in self.tracked_objects])\n",
                "        visibility = self._get_box_visibility(bboxes, frame_shape)\n",
                "        \n",
                "        for i, track in enumerate(self.tracked_objects):\n",
                "            if visibility[i] < 0.5:\n",
                "                track['lost_frames'] = self.max_lost_frames\n",
                "\n",
                "    def _predict_phase(self):\n",
                "        for obj in self.tracked_objects:\n",
                "            obj['kf'].predict()\n",
                "            predicted_state = obj['kf'].statePost\n",
                "            cx, cy, w, h = predicted_state[:4]\n",
                "            obj['bbox'] = (int(cx - w/2), int(cy - h/2), int(cx + w/2), int(cy + h/2))\n",
                "\n",
                "    def _update_phase(self, frame):\n",
                "        for obj in self.tracked_objects:\n",
                "            if obj['state'] == 'OCCLUDED': continue\n",
                "            success, bbox = obj['tracker'].update(frame)\n",
                "            if success:\n",
                "                x1, y1, w, h = [int(v) for v in bbox]\n",
                "                obj['bbox'] = (x1, y1, x1 + w, y1 + h)\n",
                "                obj['lost_frames'] = 0 # Reset lost counter on successful short-term track\n",
                "                if self.use_kalman:\n",
                "                    measurement = np.array([x1 + w/2, y1 + h/2, w, h], dtype=np.float32)\n",
                "                    obj['kf'].correct(measurement)\n",
                "            else:\n",
                "                obj['lost_frames'] += 1\n",
                "\n",
                "    def _match_and_update_phase(self, frame):\n",
                "        detections = self.detect(frame)\n",
                "        if not detections: return\n",
                "        self._extract_batch_embeddings(frame, detections)\n",
                "        \n",
                "        # --- Stage 1: Match Active Tracks with Detections ---\n",
                "        if self.tracked_objects:\n",
                "            cost_matrix = self._build_cost_matrix(self.tracked_objects, detections)\n",
                "            track_indices, det_indices = linear_sum_assignment(cost_matrix)\n",
                "            \n",
                "            matched_track_indices = set()\n",
                "            matched_det_indices = set()\n",
                "            for t_idx, d_idx in zip(track_indices, det_indices):\n",
                "                if cost_matrix[t_idx, d_idx] < self.match_cost_threshold:\n",
                "                    self._update_matched_track(frame, self.tracked_objects[t_idx], detections[d_idx])\n",
                "                    matched_track_indices.add(t_idx)\n",
                "                    matched_det_indices.add(d_idx)\n",
                "            \n",
                "            unmatched_track_indices = set(range(len(self.tracked_objects))) - matched_track_indices\n",
                "            for t_idx in unmatched_track_indices:\n",
                "                self._handle_unmatched_track(t_idx, matched_track_indices)\n",
                "        else:\n",
                "            matched_det_indices = set()\n",
                "        \n",
                "        # --- Stage 2: Re-identify Lost Tracks with Unmatched Detections ---\n",
                "        unmatched_dets = [d for i, d in enumerate(detections) if i not in matched_det_indices]\n",
                "        if unmatched_dets and self.lost_tracks:\n",
                "            reid_cost_matrix = self._build_cost_matrix(list(self.lost_tracks), unmatched_dets, only_appearance=True)\n",
                "            lost_indices, reid_det_indices = linear_sum_assignment(reid_cost_matrix)\n",
                "\n",
                "            revived_lost_indices = set()\n",
                "            for lt_idx, d_idx in zip(lost_indices, reid_det_indices):\n",
                "                if reid_cost_matrix[lt_idx, d_idx] < self.reid_cost_threshold:\n",
                "                    revived_track = self.lost_tracks[lt_idx]\n",
                "                    detection = unmatched_dets[d_idx]\n",
                "                    \n",
                "                    self._update_matched_track(frame, revived_track, detection)\n",
                "                    self.tracked_objects.append(revived_track)\n",
                "                    revived_lost_indices.add(lt_idx)\n",
                "            \n",
                "            self.lost_tracks = deque([t for i, t in enumerate(self.lost_tracks) if i not in revived_lost_indices], maxlen=self.lost_tracks.maxlen)\n",
                "    \n",
                "    def _build_cost_matrix(self, tracks, detections, only_appearance=False):\n",
                "        \"\"\"Builds the cost matrix using vectorized GPU operations.\"\"\"\n",
                "        num_tracks = len(tracks)\n",
                "        num_dets = len(detections)\n",
                "        if num_tracks == 0 or num_dets == 0:\n",
                "            return np.empty((num_tracks, num_dets))\n",
                "\n",
                "        # --- Prepare data as tensors on the GPU ---\n",
                "        track_embeddings = torch.tensor(\n",
                "            np.array([t['embedding_gallery'][-1] for t in tracks if t['embedding_gallery']]),\n",
                "            device=self.device, dtype=torch.float32\n",
                "        )\n",
                "        det_embeddings = torch.tensor(\n",
                "            np.array([d['embedding'] for d in detections if 'embedding' in d]),\n",
                "            device=self.device, dtype=torch.float32\n",
                "        )\n",
                "\n",
                "        # --- Vectorized Appearance Cost (Cosine Distance on GPU) ---\n",
                "        # 1 - cosine_similarity = cosine distance\n",
                "        app_cost = 1 - (track_embeddings @ det_embeddings.T)\n",
                "        \n",
                "        if only_appearance:\n",
                "            return app_cost.cpu().numpy()\n",
                "\n",
                "        # --- Vectorized IoU Cost (on GPU) ---\n",
                "        track_bboxes = torch.tensor([t['bbox'] for t in tracks], device=self.device, dtype=torch.float32)\n",
                "        det_bboxes = torch.tensor(\n",
                "            [[d['x1'], d['y1'], d['x2'], d['y2']] for d in detections], \n",
                "            device=self.device, dtype=torch.float32\n",
                "        )\n",
                "        iou_matrix = self._calculate_iou(track_bboxes, det_bboxes)\n",
                "        iou_cost = 1 - iou_matrix\n",
                "\n",
                "        # --- Vectorized Class Mismatch Mask (on GPU) ---\n",
                "        track_classes = np.array([t['class_name'] for t in tracks])\n",
                "        det_classes = np.array([d['class_name'] for d in detections])\n",
                "        mismatch_mask = torch.tensor(track_classes[:, None] != det_classes, device=self.device)\n",
                "        \n",
                "        # --- Combine Costs ---\n",
                "        cost_matrix = (self.appearance_weight * app_cost) + ((1 - self.appearance_weight) * iou_cost)\n",
                "        cost_matrix[mismatch_mask] = 1e6  # Invalidate non-matching classes\n",
                "        cost_matrix[iou_matrix < self.iou_gating_threshold] = 1e6 # Apply IoU gating\n",
                "\n",
                "        return cost_matrix.cpu().numpy()\n",
                "\n",
                "    def _update_matched_track(self, frame, track, det):\n",
                "        x1, y1, x2, y2 = det['x1'], det['y1'], det['x2'], det['y2']\n",
                "        w, h = x2 - x1, y2 - y1\n",
                "        \n",
                "        track['bbox'] = (x1, y1, x2, y2)\n",
                "        track['lost_frames'] = 0\n",
                "        track['state'] = 'CONFIRMED'\n",
                "            \n",
                "        new_cv_tracker = self.tracker_constructors[self.tracker_type](**self.tracker_params)\n",
                "        new_cv_tracker.init(frame, (x1, y1, w, h))\n",
                "        track['tracker'] = new_cv_tracker\n",
                "\n",
                "        if 'embedding' in det and det['embedding'] is not None:\n",
                "            track['embedding_gallery'].append(det['embedding'])\n",
                "            \n",
                "        if self.use_kalman:\n",
                "            measurement = np.array([x1 + w/2, y1 + h/2, w, h], dtype=np.float32)\n",
                "            track['kf'].correct(measurement)\n",
                "            track['kf'].statePost[4:] = 0; track['kf'].statePre[4:] = 0\n",
                "\n",
                "    def _handle_unmatched_track(self, t_idx, matched_track_indices):\n",
                "        track = self.tracked_objects[int(t_idx)]\n",
                "        # Check for occlusion against currently tracked (matched) objects\n",
                "        if matched_track_indices:\n",
                "            matched_bboxes = np.array([self.tracked_objects[int(m_idx)]['bbox'] for m_idx in matched_track_indices])\n",
                "            ious = self._calculate_iou_numpy(np.array([track['bbox']]), matched_bboxes)\n",
                "            if np.max(ious) > self.occlusion_iou_threshold:\n",
                "                track['state'] = 'OCCLUDED'\n",
                "                return\n",
                "\n",
                "        track['lost_frames'] += 1\n",
                "        if track['state'] == 'OCCLUDED': track['state'] = 'CONFIRMED'\n",
                "\n",
                "    def add_manual_track(self, frame, bbox, class_name):\n",
                "        if class_name not in self.reid_models:\n",
                "            print(f\"Warning: No Re-ID model for class '{class_name}'.\")\n",
                "            return\n",
                "\n",
                "        x1, y1, x2, y2 = [int(c) for c in bbox]\n",
                "        w = x2 - x1\n",
                "        h = y2 - y1\n",
                "        if w <= 0 or h <= 0:\n",
                "            print(\"Warning: Invalid bounding box dimensions.\")\n",
                "            return\n",
                "\n",
                "        new_track = {\n",
                "            'id': self.next_track_id, 'class_name': class_name,\n",
                "            'bbox': (x1, y1, x2, y2), 'lost_frames': 0, 'state': 'CONFIRMED', \n",
                "            'embedding_gallery': deque(maxlen=20),\n",
                "            'reid_config': self.reid_models[class_name]\n",
                "        }\n",
                "\n",
                "        tracker = self.tracker_constructors[self.tracker_type](**self.tracker_params)\n",
                "        tracker.init(frame, (x1, y1, w, h))\n",
                "        new_track['tracker'] = tracker\n",
                "\n",
                "        embedding = self._extract_embedding(frame, new_track['bbox'], new_track)\n",
                "        if embedding is not None: new_track['embedding_gallery'].append(embedding)\n",
                "\n",
                "        if self.use_kalman:\n",
                "            new_track['kf'] = self._create_kalman_filter()\n",
                "            cx, cy = x1 + w/2, y1 + h/2\n",
                "            new_track['kf'].statePost = np.array([cx, cy, w, h, 0, 0, 0, 0], dtype=np.float32)\n",
                "        \n",
                "        self.tracked_objects.append(new_track)\n",
                "        self.next_track_id += 1\n",
                "                                \n",
                "    def _drawing_phase(self, frame):\n",
                "        frame_copy = frame.copy()\n",
                "        if not self.tracked_objects: return frame_copy\n",
                "\n",
                "        bboxes = np.array([obj['bbox'] for obj in self.tracked_objects])\n",
                "        visibilities = self._get_box_visibility(bboxes, frame.shape)\n",
                "\n",
                "        for i, obj in enumerate(self.tracked_objects):\n",
                "            if obj['state'] == 'OCCLUDED' or visibilities[i] < 0.7: continue\n",
                "            \n",
                "            color = (0, 255, 0)\n",
                "            x1, y1, x2, y2 = [int(c) for c in obj['bbox']]\n",
                "            label = f\"{obj['class_name']} {obj['id']}\"\n",
                "            cv2.rectangle(frame_copy, (x1, y1), (x2, y2), color, 2)\n",
                "            cv2.putText(frame_copy, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
                "        return frame_copy\n",
                "\n",
                "    # --- Utility Methods ---\n",
                "    def detect(self, frame):\n",
                "        results = self.model(frame, verbose=False)[0]\n",
                "        detections = []\n",
                "        for box in results.boxes:\n",
                "            conf = box.conf[0].item()\n",
                "            if conf > self.conf_threshold:\n",
                "                class_name = self.model.names[int(box.cls[0].item())]\n",
                "                if self.track_classes and class_name not in self.track_classes: continue\n",
                "                coords = box.xyxy[0].tolist()\n",
                "                detections.append({\n",
                "                    'class_name': class_name, 'x1': int(coords[0]), 'y1': int(coords[1]),\n",
                "                    'x2': int(coords[2]), 'y2': int(coords[3]), 'conf': conf\n",
                "                })\n",
                "        return detections\n",
                "    \n",
                "    def _get_box_visibility(self, bboxes, frame_shape):\n",
                "        frame_h, frame_w = frame_shape[:2]\n",
                "        x1, y1, x2, y2 = bboxes[:, 0], bboxes[:, 1], bboxes[:, 2], bboxes[:, 3]\n",
                "        \n",
                "        total_area = (x2 - x1) * (y2 - y1)\n",
                "        total_area[total_area <= 0] = 1e-6\n",
                "\n",
                "        visible_x1, visible_y1 = np.maximum(x1, 0), np.maximum(y1, 0)\n",
                "        visible_x2, visible_y2 = np.minimum(x2, frame_w), np.minimum(y2, frame_h)\n",
                "        \n",
                "        visible_w = np.maximum(0, visible_x2 - visible_x1)\n",
                "        visible_h = np.maximum(0, visible_y2 - visible_y1)\n",
                "        visible_area = visible_w * visible_h\n",
                "        return visible_area / total_area\n",
                "\n",
                "    def _calculate_iou(self, bboxes1, bboxes2):\n",
                "        \"\"\"Calculates IoU for two sets of bounding boxes using PyTorch tensors.\"\"\"\n",
                "        # Broadcasting to get intersection coordinates\n",
                "        xA = torch.maximum(bboxes1[:, 0].unsqueeze(1), bboxes2[:, 0])\n",
                "        yA = torch.maximum(bboxes1[:, 1].unsqueeze(1), bboxes2[:, 1])\n",
                "        xB = torch.minimum(bboxes1[:, 2].unsqueeze(1), bboxes2[:, 2])\n",
                "        yB = torch.minimum(bboxes1[:, 3].unsqueeze(1), bboxes2[:, 3])\n",
                "\n",
                "        interArea = torch.clamp(xB - xA, min=0) * torch.clamp(yB - yA, min=0)\n",
                "\n",
                "        boxAArea = (bboxes1[:, 2] - bboxes1[:, 0]) * (bboxes1[:, 3] - bboxes1[:, 1])\n",
                "        boxBArea = (bboxes2[:, 2] - bboxes2[:, 0]) * (bboxes2[:, 3] - bboxes2[:, 1])\n",
                "\n",
                "        iou = interArea / (boxAArea.unsqueeze(1) + boxBArea - interArea + 1e-6)\n",
                "        return iou\n",
                "\n",
                "    def _calculate_iou_numpy(self, bboxes1, bboxes2):\n",
                "        \"\"\"A NumPy version for CPU-bound occlusion checks.\"\"\"\n",
                "        xA = np.maximum(bboxes1[:, 0][:, np.newaxis], bboxes2[:, 0])\n",
                "        yA = np.maximum(bboxes1[:, 1][:, np.newaxis], bboxes2[:, 1])\n",
                "        xB = np.minimum(bboxes1[:, 2][:, np.newaxis], bboxes2[:, 2])\n",
                "        yB = np.minimum(bboxes1[:, 3][:, np.newaxis], bboxes2[:, 3])\n",
                "        interArea = np.maximum(0, xB - xA) * np.maximum(0, yB - yA)\n",
                "        boxAArea = (bboxes1[:, 2] - bboxes1[:, 0]) * (bboxes1[:, 3] - bboxes1[:, 1])\n",
                "        boxBArea = (bboxes2[:, 2] - bboxes2[:, 0]) * (bboxes2[:, 3] - bboxes2[:, 1])\n",
                "        return interArea / (boxAArea[:, np.newaxis] + boxBArea - interArea + 1e-6)\n",
                "    \n",
                "    def _create_kalman_filter(self):\n",
                "        kf = cv2.KalmanFilter(8, 4)\n",
                "        kf.transitionMatrix = np.array([[1,0,0,0,1,0,0,0],[0,1,0,0,0,1,0,0],[0,0,1,0,0,0,1,0],[0,0,0,1,0,0,0,1],\n",
                "                                         [0,0,0,0,1,0,0,0],[0,0,0,0,0,1,0,0],[0,0,0,0,0,0,1,0],[0,0,0,0,0,0,0,1]], np.float32)\n",
                "        kf.measurementMatrix = np.array([[1,0,0,0,0,0,0,0],[0,1,0,0,0,0,0,0],[0,0,1,0,0,0,0,0],[0,0,0,1,0,0,0,0]], np.float32)\n",
                "        kf.processNoiseCov = np.eye(8, dtype=np.float32) * 0.03\n",
                "        kf.processNoiseCov[4:, 4:] *= 10\n",
                "        kf.measurementNoiseCov = np.eye(4, dtype=np.float32) * 0.1\n",
                "        return kf"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "4eb94019",
            "metadata": {},
            "source": [
                "## VideoPlayer class"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c0e49fee",
            "metadata": {},
            "outputs": [],
            "source": [
                "class VideoPlayer:\n",
                "    def __init__(self, source, target_fps=24, size_multiplier=1.0, window_title=\"Video Playback\"):\n",
                "        self.window_title = window_title\n",
                "        self.source = source\n",
                "        self.target_fps = target_fps\n",
                "\n",
                "        if os.path.isdir(self.source):\n",
                "            self.source_type = 'images'\n",
                "            image_extensions = ('.jpg', '.jpeg', '.png', '.bmp', '.tif', '.tiff')\n",
                "            self.image_files = sorted([os.path.join(self.source, f) for f in os.listdir(self.source) if f.lower().endswith(image_extensions)])\n",
                "            if not self.image_files: raise ValueError(\"Source directory contains no supported image files.\")\n",
                "            first_frame = cv2.imread(self.image_files[0])\n",
                "            if first_frame is None: raise IOError(f\"Could not read the first image: {self.image_files[0]}\")\n",
                "            self.frame_height, self.frame_width = first_frame.shape[:2]\n",
                "            self.cap = None\n",
                "            self.original_fps = 30\n",
                "        else:\n",
                "            self.source_type = 'video'\n",
                "            self.cap = cv2.VideoCapture(self.source)\n",
                "            if not self.cap.isOpened(): raise IOError(f\"Could not open video file: {self.source}\")\n",
                "            self.frame_width = int(self.cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
                "            self.frame_height = int(self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
                "            self.original_fps = self.cap.get(cv2.CAP_PROP_FPS)\n",
                "\n",
                "        if self.target_fps == 0:\n",
                "            self.target_fps = self.original_fps\n",
                "            print(f\"Target FPS set to 0. Using original video FPS: {self.target_fps:.2f}\")\n",
                "\n",
                "        # --- New: Adaptive UI Scaling Factor ---\n",
                "        self.ui_scale_factor = max(0.5, min(self.frame_height, 2200.0) / 1080.0) # Base scale on 1080p, with a minimum\n",
                "\n",
                "        self.total_processing_time = 0.0\n",
                "        self.processed_frame_count = 0\n",
                "        self.state = 'INITIALIZING'\n",
                "        self.selectable_detections, self.user_selections = [], []\n",
                "        self.is_drawing_roi, self.show_help = False, True\n",
                "        self.roi_start_point, self.roi_end_point, self.new_manual_box = None, None, None\n",
                "        \n",
                "        self.YOLO_CLASSES = {\n",
                "            0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', \n",
                "            5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light',\n",
                "            10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench',\n",
                "            14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow',\n",
                "            20: 'other'\n",
                "        }\n",
                "        \n",
                "        cv2.namedWindow(self.window_title, cv2.WINDOW_NORMAL)\n",
                "        cv2.resizeWindow(self.window_title, int(self.frame_width * size_multiplier), int(self.frame_height * size_multiplier))\n",
                "        \n",
                "        cv2.setMouseCallback(self.window_title, self._mouse_callback)\n",
                "        print(\"--- Video Player Initialized for Interactive Tracking ---\")\n",
                "\n",
                "    def _mouse_callback(self, event, x, y, flags, param):\n",
                "        if self.state != 'PAUSED_FOR_SELECTION': return\n",
                "\n",
                "        if event == cv2.EVENT_LBUTTONDOWN:\n",
                "            self.is_drawing_roi = True\n",
                "            self.roi_start_point, self.roi_end_point = (x, y), (x, y)\n",
                "        elif event == cv2.EVENT_MOUSEMOVE:\n",
                "            if self.is_drawing_roi: self.roi_end_point = (x, y)\n",
                "        elif event == cv2.EVENT_LBUTTONUP:\n",
                "            if self.is_drawing_roi:\n",
                "                self.is_drawing_roi = False\n",
                "                if self.roi_end_point and self.roi_start_point and abs(self.roi_start_point[0] - self.roi_end_point[0]) > 5:\n",
                "                    x1, y1, x2, y2 = self.roi_start_point[0], self.roi_start_point[1], self.roi_end_point[0], self.roi_end_point[1]\n",
                "                    self.new_manual_box = (min(x1, x2), min(y1, y2), max(x1, x2), max(y1, y2))\n",
                "                self.roi_start_point, self.roi_end_point = None, None\n",
                "        elif event == cv2.EVENT_RBUTTONDOWN:\n",
                "            removed_selection = False\n",
                "            for i, sel in reversed(list(enumerate(self.user_selections))):\n",
                "                bbox = sel.get('bbox') or (sel['x1'], sel['y1'], sel['x2'], sel['y2'])\n",
                "                if bbox[0] < x < bbox[2] and bbox[1] < y < bbox[3]:\n",
                "                    removed_item = self.user_selections.pop(i)\n",
                "                    if 'x1' in removed_item: self.selectable_detections.append(removed_item)\n",
                "                    removed_selection = True\n",
                "                    break\n",
                "            if not removed_selection:\n",
                "                for i, det in reversed(list(enumerate(self.selectable_detections))):\n",
                "                    if det['x1'] < x < det['x2'] and det['y1'] < y < det['y2']:\n",
                "                        self.user_selections.append(self.selectable_detections.pop(i))\n",
                "                        break\n",
                "\n",
                "    def _draw_pause_menu(self, frame):\n",
                "        s = self.ui_scale_factor\n",
                "        # Scaled values for fonts and layout\n",
                "        bg_height = int(240 * s)\n",
                "        title_scale, head_scale, text_scale = 1.8 * s, 1.0 * s, 0.9 * s\n",
                "        thick_main, thick_sub = max(1, int(3 * s)), max(1, int(2 * s))\n",
                "\n",
                "        overlay = frame.copy()\n",
                "        cv2.rectangle(overlay, (0, 0), (frame.shape[1], bg_height), (0, 0, 0), -1)\n",
                "        frame = cv2.addWeighted(overlay, 0.7, frame, 0.3, 0)\n",
                "        \n",
                "        cv2.putText(frame, \"PAUSED - SELECTION MODE\", (int(25*s), int(60*s)), cv2.FONT_HERSHEY_TRIPLEX, title_scale, (0, 255, 255), thick_main)\n",
                "        cv2.putText(frame, \"Mouse Controls:\", (int(25*s), int(115*s)), cv2.FONT_HERSHEY_SIMPLEX, head_scale, (255, 255, 255), thick_main)\n",
                "        cv2.putText(frame, \"- Left-Click & Drag: Draw a new box to track\", (int(35*s), int(145*s)), cv2.FONT_HERSHEY_SIMPLEX, text_scale, (255, 255, 255), thick_sub)\n",
                "        cv2.putText(frame, \"- Right-Click: Select (Red) / Deselect (Green)\", (int(35*s), int(170*s)), cv2.FONT_HERSHEY_SIMPLEX, text_scale, (255, 255, 255), thick_sub)\n",
                "        cv2.putText(frame, \"Keyboard: C: Confirm | H: Toggle Help | Space: Pause | Q: Quit\", (int(25*s), int(210*s)), cv2.FONT_HERSHEY_SIMPLEX, text_scale, (255, 255, 255), thick_sub)\n",
                "        return frame\n",
                "    \n",
                "    def _get_numeric_input(self, frame):\n",
                "        s = self.ui_scale_factor\n",
                "        # Scaled values for fonts and layout\n",
                "        title_scale, text_scale = 1.8 * s, 1.2 * s\n",
                "        thick_main, thick_sub = max(1, int(4 * s)), max(1, int(3 * s))\n",
                "        y_offset_start, y_offset_inc = int(120*s), int(45*s)\n",
                "\n",
                "        num_input = \"\"\n",
                "        while True:\n",
                "            frame_copy, overlay = frame.copy(), frame.copy()\n",
                "            cv2.rectangle(overlay, (0, 0), (frame_copy.shape[1], frame_copy.shape[0]), (0, 0, 0), -1)\n",
                "            frame_copy = cv2.addWeighted(overlay, 0.85, frame_copy, 0.15, 0)\n",
                "            \n",
                "            current_selection_id = -1\n",
                "            try:\n",
                "                if num_input: current_selection_id = int(num_input)\n",
                "            except ValueError: pass\n",
                "\n",
                "            cv2.putText(frame_copy, \"Enter Class ID & Press Enter:\", (int(50*s), int(65*s)), cv2.FONT_HERSHEY_TRIPLEX, title_scale, (0, 255, 255), thick_main)\n",
                "            y_offset = y_offset_start\n",
                "            for i, name in self.YOLO_CLASSES.items():\n",
                "                if y_offset < frame.shape[0] - 30:\n",
                "                    color = (0, 255, 0) if i == current_selection_id else (255, 255, 255)\n",
                "                    thickness = thick_main if i == current_selection_id else thick_sub\n",
                "                    cv2.putText(frame_copy, f\"{i}: {name}\", (int(50*s), y_offset), cv2.FONT_HERSHEY_SIMPLEX, text_scale, color, thickness)\n",
                "                    y_offset += y_offset_inc\n",
                "            \n",
                "            cv2.imshow(self.window_title, frame_copy)\n",
                "            key = cv2.waitKey(0)\n",
                "            if key == 13:\n",
                "                try:\n",
                "                    if num_input and int(num_input) in self.YOLO_CLASSES: return int(num_input)\n",
                "                    else: print(f\"Error: Invalid ID. Please try again.\"); num_input = \"\"\n",
                "                except ValueError: print(\"Error: Invalid input.\"); num_input = \"\"\n",
                "            elif key == 8: num_input = num_input[:-1]\n",
                "            elif ord('0') <= key <= ord('9'): num_input += chr(key)\n",
                "            elif key == 27: return None\n",
                "\n",
                "    def play(self, tracker):\n",
                "        frame_idx = -1 # Start at -1 to handle loop logic correctly\n",
                "        temp_frame = None\n",
                "\n",
                "        while True:\n",
                "            loop_start_time = time.perf_counter()\n",
                "\n",
                "            # --- Unified Frame Loading ---\n",
                "            ret, frame = False, None\n",
                "            if self.state in ['INITIALIZING', 'PLAYING']:\n",
                "                frame_idx += 1\n",
                "                if self.source_type == 'video':\n",
                "                    ret, frame = self.cap.read()\n",
                "                elif self.source_type == 'images':\n",
                "                    if frame_idx < len(self.image_files):\n",
                "                        frame = cv2.imread(self.image_files[frame_idx])\n",
                "                        ret = frame is not None\n",
                "                if ret: temp_frame = frame.copy()\n",
                "                else: break\n",
                "            else: # Paused state\n",
                "                frame = temp_frame.copy()\n",
                "\n",
                "            # --- State Machine ---\n",
                "            display_frame = frame.copy()\n",
                "            if self.state == 'INITIALIZING' and frame_idx >= 1:\n",
                "                self.state = 'PAUSED_FOR_SELECTION'\n",
                "                self.selectable_detections = tracker.detect(display_frame)\n",
                "            elif self.state == 'PLAYING':\n",
                "                display_frame = tracker.process_frame(display_frame)\n",
                "            elif self.state == 'PAUSED_FOR_SELECTION':\n",
                "                if self.show_help: display_frame = self._draw_pause_menu(display_frame)\n",
                "                for det in self.selectable_detections: cv2.rectangle(display_frame, (det['x1'], det['y1']), (det['x2'], det['y2']), (0, 0, 255), 2)\n",
                "                for sel in self.user_selections:\n",
                "                    bbox = sel.get('bbox') or (sel['x1'], sel['y1'], sel['x2'], sel['y2'])\n",
                "                    cv2.rectangle(display_frame, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (0, 255, 0), 3)\n",
                "                if self.is_drawing_roi and self.roi_start_point and self.roi_end_point:\n",
                "                    cv2.rectangle(display_frame, self.roi_start_point, self.roi_end_point, (255, 255, 0), 2)\n",
                "                if self.new_manual_box:\n",
                "                    class_id = self._get_numeric_input(display_frame)\n",
                "                    if class_id is not None:\n",
                "                        self.user_selections.append({'bbox': self.new_manual_box, 'class_name': self.YOLO_CLASSES[class_id]})\n",
                "                    self.new_manual_box = None\n",
                "            \n",
                "            # --- Live FPS and Final Display ---\n",
                "            processing_time = time.perf_counter() - loop_start_time\n",
                "            live_fps = 1.0 / processing_time if processing_time > 0 else float('inf')\n",
                "            if self.state != 'PAUSED_FOR_SELECTION':\n",
                "                self.total_processing_time += processing_time\n",
                "                self.processed_frame_count += 1\n",
                "            \n",
                "            s = self.ui_scale_factor\n",
                "            cv2.putText(display_frame, f\"FPS: {live_fps:.1f}\", (int(20*s), int(40*s)), cv2.FONT_HERSHEY_SIMPLEX, 1.2*s, (0, 255, 0), max(1, int(2*s)))\n",
                "            cv2.imshow(self.window_title, display_frame)\n",
                "\n",
                "            wait_ms = 1\n",
                "            if self.target_fps != -1 and self.state == 'PLAYING':\n",
                "                target_duration = 1.0 / self.target_fps\n",
                "                if (delay_needed := target_duration - processing_time) > 0: wait_ms = int(delay_needed * 1000)\n",
                "            elif self.state == 'PAUSED_FOR_SELECTION': wait_ms = 20\n",
                "            \n",
                "            key = cv2.waitKey(wait_ms) & 0xFF\n",
                "            if key == ord('q'): break\n",
                "            elif key == ord('h'): self.show_help = not self.show_help\n",
                "            elif key == 32 and self.state == 'PLAYING':\n",
                "                self.state = 'PAUSED_FOR_SELECTION'\n",
                "                self.selectable_detections = tracker.detect(frame)\n",
                "                self.user_selections = list(tracker.tracked_objects)\n",
                "            elif key == ord('c') and self.state == 'PAUSED_FOR_SELECTION':\n",
                "                tracker.tracked_objects, tracker.next_track_id = [], 0\n",
                "                for sel in self.user_selections:\n",
                "                    bbox = sel.get('bbox') or (sel['x1'], sel['y1'], sel['x2'], sel['y2'])\n",
                "                    tracker.add_manual_track(temp_frame, bbox, sel['class_name'])\n",
                "                self.selectable_detections, self.user_selections, self.state = [], [], 'PLAYING'\n",
                "\n",
                "        if self.processed_frame_count > 0:\n",
                "            avg_fps = self.processed_frame_count / self.total_processing_time\n",
                "            print(f\"\\n--- Playback Finished ---\\nAverage Processing FPS: {avg_fps:.2f}\\n-------------------------\")\n",
                "        \n",
                "        self.release()\n",
                "\n",
                "    def release(self):\n",
                "        print(\"Releasing resources...\")\n",
                "        if self.cap and self.cap.isOpened(): self.cap.release()\n",
                "        cv2.destroyAllWindows()\n",
                "        for _ in range(5): cv2.waitKey(1)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ff4ec198",
            "metadata": {},
            "source": [
                "## Realtime Playback"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3adef366",
            "metadata": {},
            "outputs": [],
            "source": [
                "# VIDEO_PATH = './assets/OTB100/human2/img/'\n",
                "VIDEO_PATH = './assets/footage/person2.mp4'\n",
                "MODEL_PATH = './yolo11n.pt'\n",
                "TARGET_FPS = 0 # 0: standard video fps / -1: max fps\n",
                "WINDOW_SIZE = .75\n",
                "\n",
                "try:\n",
                "    tracker = ObjectTracker(\n",
                "        tracker_type='ckcf',\n",
                "        track_classes=['person', 'car'],\n",
                "        use_kalman=False,\n",
                "        base_detect_interval=80\n",
                "    )\n",
                "\n",
                "    player = VideoPlayer(\n",
                "        source=VIDEO_PATH,\n",
                "        target_fps=TARGET_FPS,\n",
                "        size_multiplier=WINDOW_SIZE,\n",
                "        window_title=\"Realtime Player\"\n",
                "    )\n",
                "    \n",
                "    player.play(tracker)\n",
                "except IOError as e:\n",
                "    print(e)\n",
                "except Exception as e:\n",
                "    print(f\"Error: {e}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "signalenv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
