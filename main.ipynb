{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c52d8f99",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "  <a href=\"http://www.sharif.edu/\">\n",
    "    <img src=\"https://cdn.freebiesupply.com/logos/large/2x/sharif-logo-png-transparent.png\" alt=\"SUT Logo\" width=\"140\">\n",
    "  </a>\n",
    "  \n",
    "  # Sharif University of Technology\n",
    "  ### Electrical Engineering Department\n",
    "\n",
    "  ## Signals and Systems\n",
    "  #### *Final Project - Spring 2025*\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "<div align=\"center\">\n",
    "  <h1>\n",
    "    <b>Object Tracker</b>\n",
    "  </h1>\n",
    "  <p>\n",
    "    An object tracking system using YOLO for detection and various algorithms (KCF, CSRT, MOSSE) for tracking.\n",
    "  </p>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "| Professor                  |\n",
    "| :-------------------------: |\n",
    "| Dr. Mohammad Mehdi Mojahedian |\n",
    "\n",
    "<br>\n",
    "\n",
    "| Contributors              |\n",
    "| :-----------------------: |\n",
    "| **Amirreza Mousavi** |\n",
    "| **Mahdi Falahi** |\n",
    "| **Zahra Miladipour** |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6017895",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e8b3de",
   "metadata": {},
   "source": [
    "# 0: Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3b8cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import time\n",
    "import torch\n",
    "from scipy.optimize import linear_sum_assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427bd61d",
   "metadata": {},
   "source": [
    "# 1: Object Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8186abf2",
   "metadata": {},
   "source": [
    "## Preparing Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5a1c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"ObjectTracker using device: {device}\")\n",
    "model = YOLO('./yolo11n.pt').to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153643a7",
   "metadata": {},
   "source": [
    "## ObjectTracker Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb41a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObjectTracker:\n",
    "    def __init__(self, model, tracker_type='KCF', detect_interval=48, conf_threshold=0.5):\n",
    "\n",
    "        self.model = model\n",
    "        self.detect_interval = detect_interval\n",
    "        self.conf_threshold = conf_threshold\n",
    "        self.frame_idx = 0\n",
    "        self.tracked_objects = []\n",
    "\n",
    "        self.tracker_constructors = {\n",
    "            'CSRT': cv2.legacy.TrackerCSRT_create,\n",
    "            'KCF': cv2.legacy.TrackerKCF_create,\n",
    "            'MOSSE': cv2.legacy.TrackerMOSSE_create,\n",
    "            'MEDIAN_FLOW': cv2.legacy.TrackerMedianFlow_create\n",
    "        }\n",
    "        if tracker_type not in self.tracker_constructors:\n",
    "            raise ValueError(f\"Invalid tracker type: {tracker_type}. Choose from {list(self.tracker_constructors.keys())}\")\n",
    "        self.tracker_type = tracker_type\n",
    "        print(f\"Using tracker: {self.tracker_type}\")\n",
    "\n",
    "    def process_frame(self, frame, iou_threshold=0.7, max_lost_frames=5, max_objects=1):\n",
    "        height, width = frame.shape[:2]\n",
    "\n",
    "        # Lazily initialize a tracker ID counter\n",
    "        if not hasattr(self, 'next_track_id'):\n",
    "            self.next_track_id = 0\n",
    "            # print(frame.shape)\n",
    "\n",
    "        # --- PHASE 1: PREDICT with Kalman Filter ---\n",
    "        # Predict the new state for each track based on its motion model\n",
    "        for obj in self.tracked_objects:\n",
    "            obj['kf'].predict()\n",
    "            # Update bbox to the KF's prediction. This is our guess before we get new data.\n",
    "            predicted_state = obj['kf'].statePost\n",
    "            cx, cy, w, h = predicted_state[0], predicted_state[1], predicted_state[2], predicted_state[3]\n",
    "            obj['bbox'] = (int(cx - w/2), int(cy - h/2), int(cx + w/2), int(cy + h/2))\n",
    "\n",
    "        for obj in self.tracked_objects:\n",
    "            x1, y1, x2, y2 = obj['bbox']\n",
    "            # Check if fully out of frame\n",
    "            if x2 < 0 or y2 < 0 or x1 > width or y1 > height:\n",
    "                obj['lost_frames'] = max_lost_frames  # Force removal in cleanup\n",
    "            # Or, minimal overlap: compute intersection area with frame\n",
    "            elif max(0, min(x2, width) - max(x1, 0)) * max(0, min(y2, height) - max(y1, 0)) < 0.1 * (x2 - x1) * (y2 - y1):\n",
    "                obj['lost_frames'] += 1  # Gradual loss for partial exits\n",
    "            # Clip bbox to frame (optional, for drawing)\n",
    "            obj['bbox'] = (max(0, x1), max(0, y1), min(width, x2), min(height, y2))\n",
    "\n",
    "        # --- PHASE 2: UPDATE with Lightweight Tracker ---\n",
    "        lost_track_detected = False\n",
    "        for obj in self.tracked_objects:\n",
    "            success, bbox = obj['tracker'].update(frame)\n",
    "            if success:\n",
    "                # Measurement is successful. Correct the KF with the tracker's output.\n",
    "                x1, y1, w, h = [int(v) for v in bbox]\n",
    "                cx, cy = x1 + w/2, y1 + h/2\n",
    "                measurement = np.array([cx, cy, w, h], dtype=np.float32)\n",
    "                obj['kf'].correct(measurement)\n",
    "                obj['lost_frames'] = 0 # Reset lost counter\n",
    "            else:\n",
    "                # Tracker failed. Increment the lost counter.\n",
    "                obj['lost_frames'] += 1\n",
    "                lost_track_detected = True\n",
    "\n",
    "        # --- PHASE 3: CLEANUP & DETECT ---\n",
    "        # Remove tracks that have been lost for too long\n",
    "        survived_tracks = [t for t in self.tracked_objects if t['lost_frames'] < max_lost_frames]\n",
    "        \n",
    "        # Trigger detector if it's the first frame or a track was lost (even temporarily) or periodic\n",
    "        if self.frame_idx == 0 or lost_track_detected or (self.frame_idx % self.detect_interval == 0):\n",
    "            detections = self.detect(frame)\n",
    "            \n",
    "            # Associate detections with survived tracks\n",
    "            if detections:\n",
    "                tracked_bboxes = [t['bbox'] for t in survived_tracks]\n",
    "                detected_bboxes = [[d['x1'], d['y1'], d['x2'], d['y2']] for d in detections]\n",
    "                \n",
    "                iou_matrix = self._calculate_iou(tracked_bboxes, detected_bboxes)\n",
    "                matched_pairs, _, unmatched_detections = self._apply_matching(iou_matrix, iou_threshold)\n",
    "\n",
    "                # Update matched tracks with detector's more accurate data\n",
    "                for t_idx, d_idx in matched_pairs:\n",
    "                    track = survived_tracks[t_idx]\n",
    "                    det = detections[d_idx]\n",
    "                    x1, y1, x2, y2 = det['x1'], det['y1'], det['x2'], det['y2']\n",
    "                    w, h = x2 - x1, y2 - y1\n",
    "                    cx, cy = x1 + w/2, y1 + h/2\n",
    "                    \n",
    "                    # Correct the KF with the accurate detector measurement\n",
    "                    measurement = np.array([cx, cy, w, h], dtype=np.float32)\n",
    "                    track['kf'].correct(measurement)\n",
    "                    \n",
    "                    # Re-initialize the lightweight tracker to prevent drift\n",
    "                    track['tracker'].init(frame, (x1, y1, w, h))\n",
    "                    track['lost_frames'] = 0 # Reset lost counter as we found it\n",
    "                \n",
    "                # Create new tracks for unmatched detections\n",
    "                for d_idx in unmatched_detections:\n",
    "                    det = detections[d_idx]\n",
    "                    x1, y1, x2, y2 = det['x1'], det['y1'], det['x2'], det['y2']\n",
    "                    w, h = x2-x1, y2-y1\n",
    "                    \n",
    "                    new_kf = self._create_kalman_filter()\n",
    "                    new_kf.statePost = np.array([x1+w/2, y1+h/2, w, h, 0, 0, 0, 0], dtype=np.float32)\n",
    "                    \n",
    "                    tracker = self.tracker_constructors[self.tracker_type]()\n",
    "                    tracker.init(frame, (x1, y1, w, h))\n",
    "                    \n",
    "                    survived_tracks.append({\n",
    "                        'id': self.next_track_id, 'kf': new_kf, 'tracker': tracker,\n",
    "                        'class_name': det['class_name'], 'conf': det['conf'],\n",
    "                        'color': np.random.uniform(0, 255, 3).tolist(), 'bbox': (x1, y1, x2, y2),\n",
    "                        'lost_frames': 0\n",
    "                    })\n",
    "                    self.next_track_id += 1\n",
    "        \n",
    "        self.tracked_objects = survived_tracks[:min(max_objects, len(survived_tracks))]\n",
    "\n",
    "        # --- PHASE 4: DRAWING ---\n",
    "        # Final bboxes are taken from the corrected KF state for smoothness\n",
    "        objects_to_draw = []\n",
    "        for obj in self.tracked_objects:\n",
    "            # Get the smoothed bbox from the Kalman Filter's state\n",
    "            state = obj['kf'].statePost\n",
    "            cx, cy, w, h = state[0], state[1], state[2], state[3]\n",
    "            x1, y1, x2, y2 = int(cx-w/2), int(cy-h/2), int(cx+w/2), int(cy+h/2)\n",
    "            \n",
    "            objects_to_draw.append({\n",
    "                'x1': x1, 'y1': y1, 'x2': x2, 'y2': y2,\n",
    "                'class_name': f\"ID-{obj['id']} {obj['class_name']}\",\n",
    "                'conf': obj.get('conf', 1.0), 'color': obj['color']\n",
    "            })\n",
    "        \n",
    "        annotated_frame = self.draw_boxes(frame, objects_to_draw)\n",
    "\n",
    "        self.frame_idx += 1\n",
    "        return annotated_frame\n",
    "    \n",
    "    def _calculate_iou(self, bboxes1, bboxes2):\n",
    "        \"\"\"\n",
    "        Calculates the Intersection over Union (IoU) matrix between two sets of boxes.\n",
    "        - bboxes1: A NumPy array of shape (N, 4) for the first set of boxes.\n",
    "        - bboxes2: A NumPy array of shape (M, 4) for the second set of boxes.\n",
    "        Returns a NumPy array of shape (N, M) with the IoU scores.\n",
    "        \"\"\"\n",
    "        # Ensure we have NumPy arrays\n",
    "        bboxes1 = np.array(bboxes1)\n",
    "        bboxes2 = np.array(bboxes2)\n",
    "\n",
    "        # Return empty if either input is empty\n",
    "        if bboxes1.size == 0 or bboxes2.size == 0:\n",
    "            return np.empty((len(bboxes1), len(bboxes2)))\n",
    "\n",
    "        # Determine the coordinates of the intersection rectangles\n",
    "        xA = np.maximum(bboxes1[:, 0][:, np.newaxis], bboxes2[:, 0])\n",
    "        yA = np.maximum(bboxes1[:, 1][:, np.newaxis], bboxes2[:, 1])\n",
    "        xB = np.minimum(bboxes1[:, 2][:, np.newaxis], bboxes2[:, 2])\n",
    "        yB = np.minimum(bboxes1[:, 3][:, np.newaxis], bboxes2[:, 3])\n",
    "\n",
    "        # Compute the area of intersection\n",
    "        interArea = np.maximum(0, xB - xA) * np.maximum(0, yB - yA)\n",
    "\n",
    "        # Compute the area of both sets of bounding boxes\n",
    "        boxAArea = (bboxes1[:, 2] - bboxes1[:, 0]) * (bboxes1[:, 3] - bboxes1[:, 1])\n",
    "        boxBArea = (bboxes2[:, 2] - bboxes2[:, 0]) * (bboxes2[:, 3] - bboxes2[:, 1])\n",
    "\n",
    "        # Compute the IoU, adding a small epsilon to avoid division by zero\n",
    "        iou = interArea / (boxAArea[:, np.newaxis] + boxBArea - interArea + 1e-6)\n",
    "        \n",
    "        return iou\n",
    "    \n",
    "    def _create_kalman_filter(self):\n",
    "        \"\"\"\n",
    "        Creates a new Kalman Filter configured for tracking bounding boxes.\n",
    "        The state is [cx, cy, w, h, vx, vy, vw, vh] - 8 variables.\n",
    "        The measurement is [cx, cy, w, h] - 4 variables.\n",
    "        \"\"\"\n",
    "        kf = cv2.KalmanFilter(8, 4)\n",
    "        # State transition matrix (F) - constant velocity model\n",
    "        kf.transitionMatrix = np.array([\n",
    "            [1, 0, 0, 0, 1, 0, 0, 0],\n",
    "            [0, 1, 0, 0, 0, 1, 0, 0],\n",
    "            [0, 0, 1, 0, 0, 0, 1, 0],\n",
    "            [0, 0, 0, 1, 0, 0, 0, 1],\n",
    "            [0, 0, 0, 0, 1, 0, 0, 0],\n",
    "            [0, 0, 0, 0, 0, 1, 0, 0],\n",
    "            [0, 0, 0, 0, 0, 0, 1, 0],\n",
    "            [0, 0, 0, 0, 0, 0, 0, 1]], np.float32)\n",
    "        # Measurement matrix (H) - we only measure position and size\n",
    "        kf.measurementMatrix = np.array([\n",
    "            [1, 0, 0, 0, 0, 0, 0, 0],\n",
    "            [0, 1, 0, 0, 0, 0, 0, 0],\n",
    "            [0, 0, 1, 0, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 1, 0, 0, 0, 0]], np.float32)\n",
    "        \n",
    "        # Process noise covariance (Q) - accounts for uncertainty in the model\n",
    "        # Tuned for more uncertainty in velocity\n",
    "        kf.processNoiseCov = np.eye(8, dtype=np.float32) * 0.03\n",
    "        kf.processNoiseCov[4:, 4:] *= 10\n",
    "        \n",
    "        # Measurement noise covariance (R) - accounts for uncertainty in the measurement\n",
    "        kf.measurementNoiseCov = np.eye(4, dtype=np.float32) * 0.1\n",
    "        \n",
    "        return kf\n",
    "\n",
    "    def _apply_matching(self, iou_matrix, iou_threshold):\n",
    "        \"\"\"\n",
    "        Performs optimal matching between tracks and detections using the\n",
    "        Hungarian algorithm.\n",
    "        \"\"\"\n",
    "        # Use Hungarian algorithm for optimal assignment. We want to maximize\n",
    "        # IoU, so we use (1 - IoU) as the cost for the assignment problem.\n",
    "        cost_matrix = 1 - iou_matrix\n",
    "        track_indices, detection_indices = linear_sum_assignment(cost_matrix)\n",
    "        \n",
    "        matched_pairs = []\n",
    "        unmatched_track_indices = set(range(iou_matrix.shape[0]))\n",
    "        unmatched_detection_indices = set(range(iou_matrix.shape[1]))\n",
    "        \n",
    "        # Filter out matches that are below the IoU threshold\n",
    "        for t_idx, d_idx in zip(track_indices, detection_indices):\n",
    "            if iou_matrix[t_idx, d_idx] >= iou_threshold:\n",
    "                matched_pairs.append((t_idx, d_idx))\n",
    "                unmatched_track_indices.discard(t_idx)\n",
    "                unmatched_detection_indices.discard(d_idx)\n",
    "                \n",
    "        return matched_pairs, list(unmatched_track_indices), list(unmatched_detection_indices)\n",
    "\n",
    "    def detect(self, frame):\n",
    "        results = self.model(frame, verbose=False)[0]\n",
    "        detections = []\n",
    "        for box in results.boxes:\n",
    "            conf = box.conf[0].item()\n",
    "            if conf > self.conf_threshold:\n",
    "                class_name = self.model.names[int(box.cls[0].item())]\n",
    "                coords = box.xyxy[0].tolist()\n",
    "                detections.append({\n",
    "                    'class_name': class_name,\n",
    "                    'x1': int(coords[0]), 'y1': int(coords[1]),\n",
    "                    'x2': int(coords[2]), 'y2': int(coords[3]),\n",
    "                    'conf': conf\n",
    "                })\n",
    "        return detections\n",
    "\n",
    "    def draw_boxes(self, frame, objects, default_color='random'):\n",
    "        frame_copy = frame.copy()\n",
    "\n",
    "        for obj in objects:\n",
    "            x1, y1, x2, y2 = obj['x1'], obj['y1'], obj['x2'], obj['y2']\n",
    "            label = f'{obj['class_name']} {obj['conf']:.2f}'\n",
    "            if 'color' in obj:\n",
    "                box_color = obj['color']\n",
    "            else:\n",
    "                box_color = np.random.uniform(0, 255, 3).tolist() if default_color == 'random' else default_color\n",
    "\n",
    "            box_w = x2 - x1\n",
    "            font_scale = max(0.5, box_w / 150)\n",
    "            font_thickness = max(1, int(box_w / 100))\n",
    "\n",
    "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "            (tw, th), bl = cv2.getTextSize(label, font, font_scale, font_thickness)\n",
    "\n",
    "\n",
    "            # Adaptive text color\n",
    "            brightness_vect = np.array([0.114, 0.587, 0.299])\n",
    "            brightness = np.dot(box_color, brightness_vect)\n",
    "            text_color = (0,0,0) if brightness > 128 else (255,255,255)\n",
    "\n",
    "            cv2.rectangle(frame_copy, (x1, y1 - th - 8), (x1 + tw, y1), box_color, -1)\n",
    "            cv2.rectangle(frame_copy, (x1, y1), (x2, y2), box_color, 3)\n",
    "            cv2.putText(frame_copy, label, (x1, y1 - 4), font, font_scale, text_color, font_thickness, cv2.LINE_AA)\n",
    "            \n",
    "        return frame_copy\n",
    "    \n",
    "    def plot_image(self, frame, size_mult=1.0, frame_title=False, axis=False):\n",
    "        h, w = frame.shape[:2]\n",
    "        base_figsize = (w / 100, h / 100)\n",
    "        fig_w, fig_h = base_figsize[0]*size_mult, base_figsize[1]*size_mult\n",
    "        plt.figure(figsize=(fig_w, fig_h))\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        plt.imshow(frame_rgb)\n",
    "        plt.axis(axis)\n",
    "        if frame_title != False:\n",
    "            plt.title(frame_title)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f61c42",
   "metadata": {},
   "source": [
    "## Sample Detection Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09da1649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_img = cv2.imread('./assets/images/0001.jpg')\n",
    "\n",
    "# test_tracker = ObjectTracker(model)\n",
    "# test_img_detections = test_tracker.detect(test_img)\n",
    "# test_tracker.plot_image(test_tracker.draw_boxes(test_img, test_img_detections))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77598323",
   "metadata": {},
   "source": [
    "# 2: Tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb94019",
   "metadata": {},
   "source": [
    "## VideoPlayer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e49fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoPlayer:\n",
    "    def __init__(self, source, size_multiplier=1.0, playback_speed=1.0, window_title=\"Video Playback\"):\n",
    "        self.cap = cv2.VideoCapture(source)\n",
    "        self.window_title = window_title\n",
    "        \n",
    "        # Get video properties from the underlying stream\n",
    "        self.fps = self.cap.get(cv2.CAP_PROP_FPS)\n",
    "        self.frame_width = int(self.cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        self.frame_height = int(self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        self.total_frames = self.cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "        \n",
    "        # Calculate target delay based on original FPS and desired playback speed\n",
    "        self.target_delay_ms = (1000 / self.fps) / playback_speed if self.fps > 0 else 33\n",
    "\n",
    "        # Create and resize the display window\n",
    "        cv2.namedWindow(self.window_title, cv2.WINDOW_NORMAL)\n",
    "        cv2.resizeWindow(\n",
    "            self.window_title,\n",
    "            int(self.frame_width * size_multiplier),\n",
    "            int(self.frame_height * size_multiplier)\n",
    "        )\n",
    "        print(\"--- Video Player Initialized ---\")\n",
    "        print(f\"  Resolution: {self.frame_width}x{self.frame_height}\")\n",
    "        print(f\"  Original FPS: {self.fps:.2f}\")\n",
    "        print(f\"  Total Frames: {self.total_frames}\")\n",
    "        print(f\"  Playback Speed: {playback_speed}x\")\n",
    "        print(f\"  Target Delay: {self.target_delay_ms:.2f} ms\")\n",
    "        print(\"--------------------------------\")\n",
    "\n",
    "    def play(self, tracker):\n",
    "        print(\"Starting playback... Press 'q' to quit.\")\n",
    "        last_time = time.time()\n",
    "        \n",
    "        while True:\n",
    "            start_time = time.perf_counter()\n",
    "            \n",
    "            ret, frame = self.cap.read()\n",
    "            \n",
    "            if not ret:\n",
    "                print(\"Video stream ended.\")\n",
    "                break\n",
    "\n",
    "            # Process the object tracking\n",
    "            processed_frame = tracker.process_frame(frame)\n",
    "\n",
    "            # Adding FPS Annotation\n",
    "            fps = 1 / (time.time() - last_time)\n",
    "            last_time = time.time()\n",
    "            cv2.putText(processed_frame, f\"FPS: {fps:.2f}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "            cv2.imshow(self.window_title, processed_frame)\n",
    "\n",
    "            # Calculate the actual delay needed to maintain the target playback speed\n",
    "            processing_time_ms = (time.perf_counter() - start_time) * 1000\n",
    "            real_delay_ms = int(self.target_delay_ms - processing_time_ms)\n",
    "            \n",
    "            # Exit if 'q' is pressed\n",
    "            wait_key = cv2.waitKey(max(1, real_delay_ms))\n",
    "            if wait_key & 0xFF == ord('q'):\n",
    "                print(\"Playback stopped by user.\")\n",
    "                break\n",
    "            \n",
    "        self.release()\n",
    "\n",
    "    def release(self):\n",
    "        \"\"\"Stops the reader thread and closes all OpenCV windows.\"\"\"\n",
    "        print(\"Releasing resources...\")\n",
    "        self.cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        # Add a small delay to ensure windows close properly on all systems\n",
    "        for _ in range(5):\n",
    "            cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4ec198",
   "metadata": {},
   "source": [
    "## Test Playback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adef366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can change these parameters\n",
    "VIDEO_PATH = './assets/footage/person4.mp4' # Make sure this path is correct\n",
    "MODEL_PATH = './yolo11n.pt'\n",
    "PLAYBACK_SPEED = 1.5 # Play at 1.5x speed\n",
    "WINDOW_SIZE = 0.5   # Display window at 75% of original size\n",
    "\n",
    "try:\n",
    "    tracker = ObjectTracker(model, 'CSRT')\n",
    "    player = VideoPlayer(\n",
    "        source=VIDEO_PATH,\n",
    "        playback_speed=PLAYBACK_SPEED,\n",
    "        size_multiplier=WINDOW_SIZE,\n",
    "        window_title=\"High-Performance Player\"\n",
    "    )\n",
    "    player.play(tracker)\n",
    "except IOError as e:\n",
    "    print(e)\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "signalenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
