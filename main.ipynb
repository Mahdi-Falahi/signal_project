{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "c52d8f99",
            "metadata": {},
            "source": [
                "<div align=\"center\">\n",
                "  <a href=\"http://www.sharif.edu/\">\n",
                "    <img src=\"https://cdn.freebiesupply.com/logos/large/2x/sharif-logo-png-transparent.png\" alt=\"SUT Logo\" width=\"140\">\n",
                "  </a>\n",
                "  \n",
                "  # Sharif University of Technology\n",
                "  ### Electrical Engineering Department\n",
                "\n",
                "  ## Signals and Systems\n",
                "  #### *Final Project - Spring 2025*\n",
                "</div>\n",
                "\n",
                "---\n",
                "\n",
                "<div align=\"center\">\n",
                "  <h1>\n",
                "    <b>Object Tracker</b>\n",
                "  </h1>\n",
                "  <p>\n",
                "    An object tracking system using YOLO for detection and various algorithms (KCF, CSRT, MOSSE) for tracking.\n",
                "  </p>\n",
                "</div>\n",
                "\n",
                "<br>\n",
                "\n",
                "| Professor                  |\n",
                "| :-------------------------: |\n",
                "| Dr. Mohammad Mehdi Mojahedian |\n",
                "\n",
                "<br>\n",
                "\n",
                "| Contributors              |\n",
                "| :-----------------------: |\n",
                "| **Amirreza Mousavi** |\n",
                "| **Mahdi Falahi** |\n",
                "| **Zahra Miladipour** |"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f6017895",
            "metadata": {},
            "source": [
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "70e8b3de",
            "metadata": {},
            "source": [
                "## Imports"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0d3b8cc9",
            "metadata": {},
            "outputs": [],
            "source": [
                "import cv2\n",
                "import numpy as np\n",
                "from ultralytics import YOLO\n",
                "import time\n",
                "import torch\n",
                "from scipy.optimize import linear_sum_assignment\n",
                "import os\n",
                "import torchreid\n",
                "from PIL import Image\n",
                "from sklearn.metrics.pairwise import cosine_similarity\n",
                "from collections import deque"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "bb2233de",
            "metadata": {},
            "source": [
                "## Custom Tracker"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "564de46b",
            "metadata": {},
            "outputs": [],
            "source": [
                "import cv2\n",
                "import numpy as np\n",
                "\n",
                "class CustomTracker:\n",
                "    def __init__(self, psr_threshold=3.5, filter_alpha=0.025, scale_lr=0.02, lambda_trust=0.001):\n",
                "        # Parameters not used in this optical flow version, but included for compatibility\n",
                "        self.PSR_THRESHOLD = psr_threshold\n",
                "        self.FILTER_ALPHA = filter_alpha\n",
                "        self.SCALE_LR = scale_lr\n",
                "        self.LAMBDA = lambda_trust\n",
                "        self.p0 = None\n",
                "        self.old_gray = None\n",
                "        self.bbox = None\n",
                "        self.lk_params = dict(winSize=(15, 15),\n",
                "                              maxLevel=2,\n",
                "                              criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
                "        self.feature_params = dict(maxCorners=200,\n",
                "                                   qualityLevel=0.3,\n",
                "                                   minDistance=7,\n",
                "                                   blockSize=7)\n",
                "        self.max_scale_change = 0.2  # Limit per-frame scale change\n",
                "        self.fb_error_threshold = 2.0  # Forward-backward error threshold from MedianFlow\n",
                "        self.kalman = None  # Kalman filter from SORT adaptation\n",
                "\n",
                "    def _init_kalman(self):\n",
                "        # 6D state: [x, y, vx, vy, s, vs] where s is scale, vs is scale velocity\n",
                "        kalman = cv2.KalmanFilter(6, 3)  # Measure x, y, s\n",
                "        kalman.transitionMatrix = np.eye(6, dtype=np.float32)\n",
                "        kalman.transitionMatrix[0, 2] = 1.0  # x += vx\n",
                "        kalman.transitionMatrix[1, 3] = 1.0  # y += vy\n",
                "        kalman.transitionMatrix[4, 5] = 1.0  # s += vs\n",
                "        kalman.measurementMatrix = np.zeros((3, 6), dtype=np.float32)\n",
                "        kalman.measurementMatrix[0, 0] = 1.0\n",
                "        kalman.measurementMatrix[1, 1] = 1.0\n",
                "        kalman.measurementMatrix[2, 4] = 1.0\n",
                "        kalman.processNoiseCov = np.eye(6, dtype=np.float32) * 0.03\n",
                "        kalman.measurementNoiseCov = np.eye(3, dtype=np.float32) * 0.1\n",
                "        kalman.errorCovPost = np.eye(6, dtype=np.float32) * 0.1\n",
                "        return kalman\n",
                "\n",
                "    def init(self, frame, bbox):\n",
                "        x, y, w, h = map(int, bbox)\n",
                "        self.bbox = (x, y, w, h)\n",
                "        self.old_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
                "        roi_gray = self.old_gray[y:y+h, x:x+w]\n",
                "        p0 = cv2.goodFeaturesToTrack(roi_gray, mask=None, **self.feature_params)\n",
                "        if p0 is not None and len(p0) > 0:\n",
                "            p0[:,:,0] += x\n",
                "            p0[:,:,1] += y\n",
                "            self.p0 = p0\n",
                "        else:\n",
                "            self.p0 = np.empty((0, 1, 2), dtype=np.float32)\n",
                "            return False\n",
                "\n",
                "        # Initialize Kalman\n",
                "        self.kalman = self._init_kalman()\n",
                "        cx, cy = x + w / 2, y + h / 2\n",
                "        self.kalman.statePost = np.array([cx, cy, 0, 0, 1.0, 0], dtype=np.float32)  # Initial scale=1.0\n",
                "        return True\n",
                "\n",
                "    def update(self, frame):\n",
                "        frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
                "        if len(self.p0) < 3:\n",
                "            return False, self.bbox\n",
                "\n",
                "        # Kalman predict\n",
                "        predicted = self.kalman.predict()\n",
                "        pred_cx, pred_cy, _, _, pred_scale, _ = predicted.flatten()\n",
                "        pred_w = self.bbox[2] * pred_scale\n",
                "        pred_h = self.bbox[3] * pred_scale\n",
                "        pred_x = pred_cx - pred_w / 2\n",
                "        pred_y = pred_cy - pred_h / 2\n",
                "\n",
                "        # Optical flow forward\n",
                "        p1, st, err = cv2.calcOpticalFlowPyrLK(self.old_gray, frame_gray, self.p0, None, **self.lk_params)\n",
                "\n",
                "        if p1 is None or np.sum(st) < 3:\n",
                "            self.p0 = np.empty((0, 1, 2), dtype=np.float32)\n",
                "            return False, self.bbox\n",
                "\n",
                "        st_flat = st.ravel() == 1\n",
                "        good_new = p1[st_flat].reshape(-1, 2)\n",
                "        good_old = self.p0[st_flat].reshape(-1, 2)\n",
                "\n",
                "        # Forward-backward validation from MedianFlow\n",
                "        p0r = good_new.reshape(-1, 1, 2)\n",
                "        p_back, st_back, err_back = cv2.calcOpticalFlowPyrLK(frame_gray, self.old_gray, p0r, None, **self.lk_params)\n",
                "\n",
                "        if p_back is None:\n",
                "            return False, self.bbox\n",
                "\n",
                "        st_back_flat = st_back.ravel() == 1\n",
                "        p_back_resh = p_back[st_back_flat].reshape(-1, 2)\n",
                "        fb_errors = np.linalg.norm(good_old[st_back_flat] - p_back_resh, axis=1)\n",
                "\n",
                "        valid_submask = fb_errors < self.fb_error_threshold\n",
                "        valid_mask = np.zeros(len(good_old), dtype=bool)\n",
                "        valid_mask[st_back_flat] = valid_submask\n",
                "\n",
                "        validated_new = good_new[valid_mask]\n",
                "        validated_old = good_old[valid_mask]\n",
                "\n",
                "        if len(validated_new) < 3:\n",
                "            return False, self.bbox\n",
                "\n",
                "        # Median aggregation from MedianFlow for translation and scale\n",
                "        dx = np.median(validated_new[:, 0] - validated_old[:, 0])\n",
                "        dy = np.median(validated_new[:, 1] - validated_old[:, 1])\n",
                "        if len(validated_old) >= 2:\n",
                "            old_dists = np.linalg.norm(validated_old[:, None] - validated_old[None, :], axis=2)\n",
                "            new_dists = np.linalg.norm(validated_new[:, None] - validated_new[None, :], axis=2)\n",
                "            ratios = new_dists[old_dists > 0] / old_dists[old_dists > 0]\n",
                "            meas_scale = np.median(ratios) if len(ratios) > 0 else 1.0\n",
                "            meas_scale = np.clip(meas_scale, 1 - self.max_scale_change, 1 + self.max_scale_change)\n",
                "        else:\n",
                "            meas_scale = 1.0\n",
                "\n",
                "        # Measured values\n",
                "        x, y, w, h = self.bbox\n",
                "        meas_cx = x + w / 2 + dx\n",
                "        meas_cy = y + h / 2 + dy\n",
                "\n",
                "        # Kalman correct\n",
                "        measurement = np.array([meas_cx, meas_cy, meas_scale], dtype=np.float32)\n",
                "        corrected = self.kalman.correct(measurement)\n",
                "        corr_cx, corr_cy, _, _, corr_scale, _ = corrected.flatten()\n",
                "\n",
                "        # Update bbox with smoothed values\n",
                "        new_w = w * corr_scale\n",
                "        new_h = h * corr_scale\n",
                "        new_x = corr_cx - new_w / 2\n",
                "        new_y = corr_cy - new_h / 2\n",
                "        self.bbox = (int(new_x), int(new_y), int(new_w), int(new_h))\n",
                "\n",
                "        # Update points and gray frame\n",
                "        self.p0 = validated_new.reshape(-1, 1, 2)\n",
                "        self.old_gray = frame_gray.copy()\n",
                "\n",
                "        # Re-detect features if too few\n",
                "        if len(self.p0) < 20:\n",
                "            x, y, w, h = self.bbox\n",
                "            roi_gray = self.old_gray[max(0, y):y+h, max(0, x):x+w]\n",
                "            new_p0 = cv2.goodFeaturesToTrack(roi_gray, mask=None, **self.feature_params)\n",
                "            if new_p0 is not None and len(new_p0) > 0:\n",
                "                new_p0[:,:,0] += max(0, x)\n",
                "                new_p0[:,:,1] += max(0, y)\n",
                "                self.p0 = np.vstack((self.p0, new_p0)) if len(self.p0) > 0 else new_p0\n",
                "\n",
                "        return True, self.bbox"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "153643a7",
            "metadata": {},
            "source": [
                "## ObjectTracker Wrapper Class"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "feb41a98",
            "metadata": {},
            "outputs": [],
            "source": [
                "class ObjectTracker:\n",
                "    def __init__(self, model='./yolo11n.pt', tracker_type='CSRT', base_detect_interval=24, conf_threshold=0.5, \n",
                "                 max_lost_frames=30, lost_track_buffer=60,\n",
                "                 use_kalman=True, track_classes=None, \n",
                "                 appearance_weight=0.6, match_cost_threshold=0.85, \n",
                "                 reid_cost_threshold=0.3, occlusion_iou_threshold=0.2,\n",
                "                 iou_gating_threshold=0.1, **kwargs):\n",
                "        \n",
                "        # --- Check Cuda Presence ---\n",
                "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
                "        print(f'object tracker running on {self.device}')\n",
                "\n",
                "        # --- Core Parameters ---\n",
                "        self.detect_interval = base_detect_interval\n",
                "        self.conf_threshold = conf_threshold\n",
                "        self.max_lost_frames = max_lost_frames\n",
                "        self.track_classes = track_classes if track_classes is not None else []\n",
                "        self.model = YOLO(model).to(self.device)\n",
                "        print('--- Yolo loaded successfully ---')\n",
                "        \n",
                "        # --- Cost & Matching Parameters ---\n",
                "        self.appearance_weight = appearance_weight\n",
                "        self.match_cost_threshold = match_cost_threshold\n",
                "        self.reid_cost_threshold = reid_cost_threshold\n",
                "        self.occlusion_iou_threshold = occlusion_iou_threshold\n",
                "        self.iou_gating_threshold = iou_gating_threshold\n",
                "        \n",
                "        # --- State Management ---\n",
                "        self.frame_idx = 0\n",
                "        self.tracked_objects = []\n",
                "        self.lost_tracks = deque(maxlen=lost_track_buffer)\n",
                "        self.next_track_id = 0\n",
                "        \n",
                "        # --- Kalman Filter ---\n",
                "        self.use_kalman = use_kalman\n",
                "\n",
                "        # --- Re-ID Models & Warm-up ---\n",
                "        self.reid_models = self._load_reid_models()\n",
                "        for config in self.reid_models.values():\n",
                "            reid_model = config['model']\n",
                "            dummy_input = torch.randn(1, 3, 256, 128).to(self.device)\n",
                "            with torch.no_grad():\n",
                "                reid_model(dummy_input)\n",
                "        print(\"--- osnet loaded successfuly ---\")\n",
                "\n",
                "        self.tracker_params = {}\n",
                "        if tracker_type.upper() == 'CUSTOM':\n",
                "            self.tracker_params = {\n",
                "                'psr_threshold': kwargs.get('psr_threshold', 5.5),\n",
                "                'filter_alpha': kwargs.get('filter_alpha', 0.025),\n",
                "                'scale_lr': kwargs.get('scale_lr', 0.02),\n",
                "                'lambda_trust': kwargs.get('lambda_trust', 0.01)\n",
                "            }\n",
                "            print(f\"--- Custom tracker configured with params: {self.tracker_params} ---\")\n",
                "\n",
                "        # --- Tracker Constructors ---\n",
                "        self.tracker_constructors = {\n",
                "            'CSRT': cv2.legacy.TrackerCSRT_create, 'KCF': cv2.legacy.TrackerKCF_create,\n",
                "            'MOSSE': cv2.legacy.TrackerMOSSE_create, 'MEDIAN_FLOW': cv2.legacy.TrackerMedianFlow_create,\n",
                "            'CUSTOM': CustomTracker\n",
                "            \n",
                "        }\n",
                "        if tracker_type.upper() not in self.tracker_constructors:\n",
                "            raise ValueError(f\"Invalid tracker type: {tracker_type}. Choose from {list(self.tracker_constructors.keys())}\")\n",
                "        self.tracker_type = tracker_type.upper()\n",
                "        print(f\"--- Object Tracker Initialized ---\")\n",
                "\n",
                "    def _load_reid_models(self):\n",
                "        \"\"\"Loads pre-trained Re-ID models for different object classes.\"\"\"\n",
                "        models = {}\n",
                "        person_model = torchreid.models.build_model(name='osnet_x1_0', num_classes=4101, pretrained=False)\n",
                "        torchreid.utils.load_pretrained_weights(person_model, 'osnet_x1_0_msmt17_256x128.pth')\n",
                "        person_model.to(self.device).eval()\n",
                "        person_transform, _ = torchreid.data.transforms.build_transforms(height=256, width=128, is_train=False)\n",
                "        models['person'] = {'model': person_model, 'transform': person_transform}\n",
                "        models['car'] = {'model': person_model, 'transform': person_transform}\n",
                "        return models\n",
                "\n",
                "    def _extract_embedding(self, frame, bbox, track):\n",
                "        \"\"\"Extracts a feature embedding from a single bounding box.\"\"\"\n",
                "        reid_config = track.get('reid_config')\n",
                "        if not reid_config: return None\n",
                "        model, transform = reid_config['model'], reid_config['transform']\n",
                "        \n",
                "        x1, y1, x2, y2 = [int(c) for c in bbox]\n",
                "        roi = frame[y1:y2, x1:x2]\n",
                "        if roi.size == 0: return None\n",
                "\n",
                "        roi_rgb = cv2.cvtColor(roi, cv2.COLOR_BGR2RGB)\n",
                "        image_tensor = transform(Image.fromarray(roi_rgb)).unsqueeze(0).to(self.device)\n",
                "        \n",
                "        with torch.no_grad():\n",
                "            embedding = model(image_tensor)\n",
                "        \n",
                "        return torch.nn.functional.normalize(embedding, p=2, dim=1).cpu().numpy().flatten()\n",
                "\n",
                "    def _extract_batch_embeddings(self, frame, detections):\n",
                "        \"\"\"Extracts embeddings for a batch of detections, grouped by class.\"\"\"\n",
                "        grouped_dets = {}\n",
                "        for i, det in enumerate(detections):\n",
                "            cls_name = det['class_name']\n",
                "            if cls_name in self.reid_models:\n",
                "                grouped_dets.setdefault(cls_name, []).append((i, det))\n",
                "\n",
                "        for cls_name, dets_with_indices in grouped_dets.items():\n",
                "            reid_config = self.reid_models[cls_name]\n",
                "            model, transform = reid_config['model'], reid_config['transform']\n",
                "            \n",
                "            rois_with_indices = []\n",
                "            for original_idx, det in dets_with_indices:\n",
                "                x1, y1, x2, y2 = det['x1'], det['y1'], det['x2'], det['y2']\n",
                "                roi = frame[y1:y2, x1:x2]\n",
                "                if roi.size > 0:\n",
                "                    roi_rgb = cv2.cvtColor(roi, cv2.COLOR_BGR2RGB)\n",
                "                    rois_with_indices.append({'roi': Image.fromarray(roi_rgb), 'idx': original_idx})\n",
                "            \n",
                "            if not rois_with_indices: continue\n",
                "\n",
                "            batch_tensor = torch.stack([transform(item['roi']) for item in rois_with_indices]).to(self.device)\n",
                "            with torch.no_grad():\n",
                "                batch_embeddings = model(batch_tensor)\n",
                "            \n",
                "            normalized_embeddings = torch.nn.functional.normalize(batch_embeddings, p=2, dim=1).cpu().numpy()\n",
                "\n",
                "            for item, emb in zip(rois_with_indices, normalized_embeddings):\n",
                "                detections[item['idx']]['embedding'] = emb\n",
                "\n",
                "    def process_frame(self, frame):\n",
                "        \"\"\"Main processing function for each frame.\"\"\"\n",
                "        self._apply_boundary_conditions(frame.shape)\n",
                "\n",
                "        newly_lost = [t for t in self.tracked_objects if t['lost_frames'] >= self.max_lost_frames]\n",
                "        for t in newly_lost:\n",
                "            t['state'] = 'LOST'\n",
                "            self.lost_tracks.append(t)\n",
                "        self.tracked_objects = [t for t in self.tracked_objects if t['lost_frames'] < self.max_lost_frames]\n",
                "\n",
                "        if self.use_kalman: self._predict_phase()\n",
                "        self._update_phase(frame)\n",
                "        \n",
                "        if self.frame_idx % self.detect_interval == 0:\n",
                "            self._match_and_update_phase(frame)\n",
                "\n",
                "        annotated_frame = self._drawing_phase(frame)\n",
                "        self.frame_idx += 1\n",
                "        return annotated_frame\n",
                "    \n",
                "    def _apply_boundary_conditions(self, frame_shape):\n",
                "        if not self.tracked_objects: return\n",
                "        \n",
                "        bboxes = np.array([t['bbox'] for t in self.tracked_objects])\n",
                "        visibility = self._get_box_visibility(bboxes, frame_shape)\n",
                "        \n",
                "        for i, track in enumerate(self.tracked_objects):\n",
                "            if visibility[i] < 0.5:\n",
                "                track['lost_frames'] = self.max_lost_frames\n",
                "\n",
                "    def _predict_phase(self):\n",
                "        for obj in self.tracked_objects:\n",
                "            obj['kf'].predict()\n",
                "            predicted_state = obj['kf'].statePost\n",
                "            cx, cy, w, h = predicted_state[:4]\n",
                "            obj['bbox'] = (int(cx - w/2), int(cy - h/2), int(cx + w/2), int(cy + h/2))\n",
                "\n",
                "    def _update_phase(self, frame):\n",
                "        for obj in self.tracked_objects:\n",
                "            if obj['state'] == 'OCCLUDED': continue\n",
                "            success, bbox = obj['tracker'].update(frame)\n",
                "            if success:\n",
                "                x1, y1, w, h = [int(v) for v in bbox]\n",
                "                obj['bbox'] = (x1, y1, x1 + w, y1 + h)\n",
                "                obj['lost_frames'] = 0 # Reset lost counter on successful short-term track\n",
                "                if self.use_kalman:\n",
                "                    measurement = np.array([x1 + w/2, y1 + h/2, w, h], dtype=np.float32)\n",
                "                    obj['kf'].correct(measurement)\n",
                "            else:\n",
                "                obj['lost_frames'] += 1\n",
                "\n",
                "    def _match_and_update_phase(self, frame):\n",
                "        detections = self.detect(frame)\n",
                "        if not detections: return\n",
                "        self._extract_batch_embeddings(frame, detections)\n",
                "        \n",
                "        # --- Stage 1: Match Active Tracks with Detections ---\n",
                "        if self.tracked_objects:\n",
                "            cost_matrix = self._build_cost_matrix(self.tracked_objects, detections)\n",
                "            track_indices, det_indices = linear_sum_assignment(cost_matrix)\n",
                "            \n",
                "            matched_track_indices = set()\n",
                "            matched_det_indices = set()\n",
                "            for t_idx, d_idx in zip(track_indices, det_indices):\n",
                "                if cost_matrix[t_idx, d_idx] < self.match_cost_threshold:\n",
                "                    self._update_matched_track(frame, self.tracked_objects[t_idx], detections[d_idx])\n",
                "                    matched_track_indices.add(t_idx)\n",
                "                    matched_det_indices.add(d_idx)\n",
                "            \n",
                "            unmatched_track_indices = set(range(len(self.tracked_objects))) - matched_track_indices\n",
                "            for t_idx in unmatched_track_indices:\n",
                "                self._handle_unmatched_track(t_idx, matched_track_indices)\n",
                "        else:\n",
                "            matched_det_indices = set()\n",
                "        \n",
                "        # --- Stage 2: Re-identify Lost Tracks with Unmatched Detections ---\n",
                "        unmatched_dets = [d for i, d in enumerate(detections) if i not in matched_det_indices]\n",
                "        if unmatched_dets and self.lost_tracks:\n",
                "            reid_cost_matrix = self._build_cost_matrix(list(self.lost_tracks), unmatched_dets, only_appearance=True)\n",
                "            lost_indices, reid_det_indices = linear_sum_assignment(reid_cost_matrix)\n",
                "\n",
                "            revived_lost_indices = set()\n",
                "            for lt_idx, d_idx in zip(lost_indices, reid_det_indices):\n",
                "                if reid_cost_matrix[lt_idx, d_idx] < self.reid_cost_threshold:\n",
                "                    revived_track = self.lost_tracks[lt_idx]\n",
                "                    detection = unmatched_dets[d_idx]\n",
                "                    \n",
                "                    self._update_matched_track(frame, revived_track, detection)\n",
                "                    self.tracked_objects.append(revived_track)\n",
                "                    revived_lost_indices.add(lt_idx)\n",
                "            \n",
                "            self.lost_tracks = deque([t for i, t in enumerate(self.lost_tracks) if i not in revived_lost_indices], maxlen=self.lost_tracks.maxlen)\n",
                "    \n",
                "    def _build_cost_matrix(self, tracks, detections, only_appearance=False):\n",
                "        \"\"\"Builds the cost matrix using vectorized GPU operations.\"\"\"\n",
                "        num_tracks = len(tracks)\n",
                "        num_dets = len(detections)\n",
                "        if num_tracks == 0 or num_dets == 0:\n",
                "            return np.empty((num_tracks, num_dets))\n",
                "\n",
                "        # --- Prepare data as tensors on the GPU ---\n",
                "        track_embeddings = torch.tensor(\n",
                "            np.array([t['embedding_gallery'][-1] for t in tracks if t['embedding_gallery']]),\n",
                "            device=self.device, dtype=torch.float32\n",
                "        )\n",
                "        det_embeddings = torch.tensor(\n",
                "            np.array([d['embedding'] for d in detections if 'embedding' in d]),\n",
                "            device=self.device, dtype=torch.float32\n",
                "        )\n",
                "\n",
                "        # --- Vectorized Appearance Cost (Cosine Distance on GPU) ---\n",
                "        # 1 - cosine_similarity = cosine distance\n",
                "        app_cost = 1 - (track_embeddings @ det_embeddings.T)\n",
                "        \n",
                "        if only_appearance:\n",
                "            return app_cost.cpu().numpy()\n",
                "\n",
                "        # --- Vectorized IoU Cost (on GPU) ---\n",
                "        track_bboxes = torch.tensor([t['bbox'] for t in tracks], device=self.device, dtype=torch.float32)\n",
                "        det_bboxes = torch.tensor(\n",
                "            [[d['x1'], d['y1'], d['x2'], d['y2']] for d in detections], \n",
                "            device=self.device, dtype=torch.float32\n",
                "        )\n",
                "        iou_matrix = self._calculate_iou(track_bboxes, det_bboxes)\n",
                "        iou_cost = 1 - iou_matrix\n",
                "\n",
                "        # --- Vectorized Class Mismatch Mask (on GPU) ---\n",
                "        track_classes = np.array([t['class_name'] for t in tracks])\n",
                "        det_classes = np.array([d['class_name'] for d in detections])\n",
                "        mismatch_mask = torch.tensor(track_classes[:, None] != det_classes, device=self.device)\n",
                "        \n",
                "        # --- Combine Costs ---\n",
                "        cost_matrix = (self.appearance_weight * app_cost) + ((1 - self.appearance_weight) * iou_cost)\n",
                "        cost_matrix[mismatch_mask] = 1e6  # Invalidate non-matching classes\n",
                "        cost_matrix[iou_matrix < self.iou_gating_threshold] = 1e6 # Apply IoU gating\n",
                "\n",
                "        return cost_matrix.cpu().numpy()\n",
                "\n",
                "    def _update_matched_track(self, frame, track, det):\n",
                "        x1, y1, x2, y2 = det['x1'], det['y1'], det['x2'], det['y2']\n",
                "        w, h = x2 - x1, y2 - y1\n",
                "        \n",
                "        track['bbox'] = (x1, y1, x2, y2)\n",
                "        track['lost_frames'] = 0\n",
                "        track['state'] = 'CONFIRMED'\n",
                "            \n",
                "        new_cv_tracker = self.tracker_constructors[self.tracker_type](**self.tracker_params)\n",
                "        new_cv_tracker.init(frame, (x1, y1, w, h))\n",
                "        track['tracker'] = new_cv_tracker\n",
                "\n",
                "        if 'embedding' in det and det['embedding'] is not None:\n",
                "            track['embedding_gallery'].append(det['embedding'])\n",
                "            \n",
                "        if self.use_kalman:\n",
                "            measurement = np.array([x1 + w/2, y1 + h/2, w, h], dtype=np.float32)\n",
                "            track['kf'].correct(measurement)\n",
                "            track['kf'].statePost[4:] = 0; track['kf'].statePre[4:] = 0\n",
                "\n",
                "    def _handle_unmatched_track(self, t_idx, matched_track_indices):\n",
                "        track = self.tracked_objects[int(t_idx)]\n",
                "        # Check for occlusion against currently tracked (matched) objects\n",
                "        if matched_track_indices:\n",
                "            matched_bboxes = np.array([self.tracked_objects[int(m_idx)]['bbox'] for m_idx in matched_track_indices])\n",
                "            ious = self._calculate_iou_numpy(np.array([track['bbox']]), matched_bboxes)\n",
                "            if np.max(ious) > self.occlusion_iou_threshold:\n",
                "                track['state'] = 'OCCLUDED'\n",
                "                return\n",
                "\n",
                "        track['lost_frames'] += 1\n",
                "        if track['state'] == 'OCCLUDED': track['state'] = 'CONFIRMED'\n",
                "\n",
                "    def add_manual_track(self, frame, bbox, class_name):\n",
                "        if class_name not in self.reid_models:\n",
                "            print(f\"Warning: No Re-ID model for class '{class_name}'.\")\n",
                "            return\n",
                "\n",
                "        x1, y1, x2, y2 = [int(c) for c in bbox]\n",
                "        w = x2 - x1\n",
                "        h = y2 - y1\n",
                "        if w <= 0 or h <= 0:\n",
                "            print(\"Warning: Invalid bounding box dimensions.\")\n",
                "            return\n",
                "\n",
                "        new_track = {\n",
                "            'id': self.next_track_id, 'class_name': class_name,\n",
                "            'bbox': (x1, y1, x2, y2), 'lost_frames': 0, 'state': 'CONFIRMED', \n",
                "            'embedding_gallery': deque(maxlen=20),\n",
                "            'reid_config': self.reid_models[class_name]\n",
                "        }\n",
                "\n",
                "        tracker = self.tracker_constructors[self.tracker_type](**self.tracker_params)\n",
                "        tracker.init(frame, (x1, y1, w, h))\n",
                "        new_track['tracker'] = tracker\n",
                "\n",
                "        embedding = self._extract_embedding(frame, new_track['bbox'], new_track)\n",
                "        if embedding is not None: new_track['embedding_gallery'].append(embedding)\n",
                "\n",
                "        if self.use_kalman:\n",
                "            new_track['kf'] = self._create_kalman_filter()\n",
                "            cx, cy = x1 + w/2, y1 + h/2\n",
                "            new_track['kf'].statePost = np.array([cx, cy, w, h, 0, 0, 0, 0], dtype=np.float32)\n",
                "        \n",
                "        self.tracked_objects.append(new_track)\n",
                "        self.next_track_id += 1\n",
                "                                \n",
                "    def _drawing_phase(self, frame):\n",
                "        frame_copy = frame.copy()\n",
                "        if not self.tracked_objects: return frame_copy\n",
                "\n",
                "        bboxes = np.array([obj['bbox'] for obj in self.tracked_objects])\n",
                "        visibilities = self._get_box_visibility(bboxes, frame.shape)\n",
                "\n",
                "        for i, obj in enumerate(self.tracked_objects):\n",
                "            if obj['state'] == 'OCCLUDED' or visibilities[i] < 0.7: continue\n",
                "            \n",
                "            color = (0, 255, 0)\n",
                "            x1, y1, x2, y2 = [int(c) for c in obj['bbox']]\n",
                "            label = f\"{obj['class_name']} {obj['id']}\"\n",
                "            cv2.rectangle(frame_copy, (x1, y1), (x2, y2), color, 2)\n",
                "            cv2.putText(frame_copy, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
                "        return frame_copy\n",
                "\n",
                "    # --- Utility Methods ---\n",
                "    def detect(self, frame):\n",
                "        results = self.model(frame, verbose=False)[0]\n",
                "        detections = []\n",
                "        for box in results.boxes:\n",
                "            conf = box.conf[0].item()\n",
                "            if conf > self.conf_threshold:\n",
                "                class_name = self.model.names[int(box.cls[0].item())]\n",
                "                if self.track_classes and class_name not in self.track_classes: continue\n",
                "                coords = box.xyxy[0].tolist()\n",
                "                detections.append({\n",
                "                    'class_name': class_name, 'x1': int(coords[0]), 'y1': int(coords[1]),\n",
                "                    'x2': int(coords[2]), 'y2': int(coords[3]), 'conf': conf\n",
                "                })\n",
                "        return detections\n",
                "    \n",
                "    def _get_box_visibility(self, bboxes, frame_shape):\n",
                "        frame_h, frame_w = frame_shape[:2]\n",
                "        x1, y1, x2, y2 = bboxes[:, 0], bboxes[:, 1], bboxes[:, 2], bboxes[:, 3]\n",
                "        \n",
                "        total_area = (x2 - x1) * (y2 - y1)\n",
                "        total_area[total_area <= 0] = 1e-6\n",
                "\n",
                "        visible_x1, visible_y1 = np.maximum(x1, 0), np.maximum(y1, 0)\n",
                "        visible_x2, visible_y2 = np.minimum(x2, frame_w), np.minimum(y2, frame_h)\n",
                "        \n",
                "        visible_w = np.maximum(0, visible_x2 - visible_x1)\n",
                "        visible_h = np.maximum(0, visible_y2 - visible_y1)\n",
                "        visible_area = visible_w * visible_h\n",
                "        return visible_area / total_area\n",
                "\n",
                "    def _calculate_iou(self, bboxes1, bboxes2):\n",
                "        \"\"\"Calculates IoU for two sets of bounding boxes using PyTorch tensors.\"\"\"\n",
                "        # Broadcasting to get intersection coordinates\n",
                "        xA = torch.maximum(bboxes1[:, 0].unsqueeze(1), bboxes2[:, 0])\n",
                "        yA = torch.maximum(bboxes1[:, 1].unsqueeze(1), bboxes2[:, 1])\n",
                "        xB = torch.minimum(bboxes1[:, 2].unsqueeze(1), bboxes2[:, 2])\n",
                "        yB = torch.minimum(bboxes1[:, 3].unsqueeze(1), bboxes2[:, 3])\n",
                "\n",
                "        interArea = torch.clamp(xB - xA, min=0) * torch.clamp(yB - yA, min=0)\n",
                "\n",
                "        boxAArea = (bboxes1[:, 2] - bboxes1[:, 0]) * (bboxes1[:, 3] - bboxes1[:, 1])\n",
                "        boxBArea = (bboxes2[:, 2] - bboxes2[:, 0]) * (bboxes2[:, 3] - bboxes2[:, 1])\n",
                "\n",
                "        iou = interArea / (boxAArea.unsqueeze(1) + boxBArea - interArea + 1e-6)\n",
                "        return iou\n",
                "\n",
                "    def _calculate_iou_numpy(self, bboxes1, bboxes2):\n",
                "        \"\"\"A NumPy version for CPU-bound occlusion checks.\"\"\"\n",
                "        xA = np.maximum(bboxes1[:, 0][:, np.newaxis], bboxes2[:, 0])\n",
                "        yA = np.maximum(bboxes1[:, 1][:, np.newaxis], bboxes2[:, 1])\n",
                "        xB = np.minimum(bboxes1[:, 2][:, np.newaxis], bboxes2[:, 2])\n",
                "        yB = np.minimum(bboxes1[:, 3][:, np.newaxis], bboxes2[:, 3])\n",
                "        interArea = np.maximum(0, xB - xA) * np.maximum(0, yB - yA)\n",
                "        boxAArea = (bboxes1[:, 2] - bboxes1[:, 0]) * (bboxes1[:, 3] - bboxes1[:, 1])\n",
                "        boxBArea = (bboxes2[:, 2] - bboxes2[:, 0]) * (bboxes2[:, 3] - bboxes2[:, 1])\n",
                "        return interArea / (boxAArea[:, np.newaxis] + boxBArea - interArea + 1e-6)\n",
                "    \n",
                "    def _create_kalman_filter(self):\n",
                "        kf = cv2.KalmanFilter(8, 4)\n",
                "        kf.transitionMatrix = np.array([[1,0,0,0,1,0,0,0],[0,1,0,0,0,1,0,0],[0,0,1,0,0,0,1,0],[0,0,0,1,0,0,0,1],\n",
                "                                         [0,0,0,0,1,0,0,0],[0,0,0,0,0,1,0,0],[0,0,0,0,0,0,1,0],[0,0,0,0,0,0,0,1]], np.float32)\n",
                "        kf.measurementMatrix = np.array([[1,0,0,0,0,0,0,0],[0,1,0,0,0,0,0,0],[0,0,1,0,0,0,0,0],[0,0,0,1,0,0,0,0]], np.float32)\n",
                "        kf.processNoiseCov = np.eye(8, dtype=np.float32) * 0.03\n",
                "        kf.processNoiseCov[4:, 4:] *= 10\n",
                "        kf.measurementNoiseCov = np.eye(4, dtype=np.float32) * 0.1\n",
                "        return kf"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "4eb94019",
            "metadata": {},
            "source": [
                "## VideoPlayer class"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c0e49fee",
            "metadata": {},
            "outputs": [],
            "source": [
                "class VideoPlayer:\n",
                "    def __init__(self, source, target_fps=24, size_multiplier=1.0, window_title=\"Video Playback\"):\n",
                "        self.window_title = window_title\n",
                "        self.source = source\n",
                "        self.target_fps = target_fps\n",
                "\n",
                "        if os.path.isdir(self.source):\n",
                "            self.source_type = 'images'\n",
                "            image_extensions = ('.jpg', '.jpeg', '.png', '.bmp', '.tif', '.tiff')\n",
                "            self.image_files = sorted([os.path.join(self.source, f) for f in os.listdir(self.source) if f.lower().endswith(image_extensions)])\n",
                "            if not self.image_files: raise ValueError(\"Source directory contains no supported image files.\")\n",
                "            first_frame = cv2.imread(self.image_files[0])\n",
                "            if first_frame is None: raise IOError(f\"Could not read the first image: {self.image_files[0]}\")\n",
                "            self.frame_height, self.frame_width = first_frame.shape[:2]\n",
                "            self.cap = None\n",
                "            self.original_fps = 30\n",
                "        else:\n",
                "            self.source_type = 'video'\n",
                "            self.cap = cv2.VideoCapture(self.source)\n",
                "            if not self.cap.isOpened(): raise IOError(f\"Could not open video file: {self.source}\")\n",
                "            self.frame_width = int(self.cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
                "            self.frame_height = int(self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
                "            self.original_fps = self.cap.get(cv2.CAP_PROP_FPS)\n",
                "\n",
                "        if self.target_fps == 0:\n",
                "            self.target_fps = self.original_fps\n",
                "            print(f\"Target FPS set to 0. Using original video FPS: {self.target_fps:.2f}\")\n",
                "\n",
                "        # --- New: Adaptive UI Scaling Factor ---\n",
                "        self.ui_scale_factor = max(0.5, min(self.frame_height, 2200.0) / 1080.0) # Base scale on 1080p, with a minimum\n",
                "\n",
                "        self.total_processing_time = 0.0\n",
                "        self.processed_frame_count = 0\n",
                "        self.state = 'INITIALIZING'\n",
                "        self.selectable_detections, self.user_selections = [], []\n",
                "        self.is_drawing_roi, self.show_help = False, True\n",
                "        self.roi_start_point, self.roi_end_point, self.new_manual_box = None, None, None\n",
                "        \n",
                "        self.YOLO_CLASSES = {\n",
                "            0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', \n",
                "            5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light',\n",
                "            10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench',\n",
                "            14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow',\n",
                "            20: 'other'\n",
                "        }\n",
                "        \n",
                "        cv2.namedWindow(self.window_title, cv2.WINDOW_NORMAL)\n",
                "        cv2.resizeWindow(self.window_title, int(self.frame_width * size_multiplier), int(self.frame_height * size_multiplier))\n",
                "        \n",
                "        cv2.setMouseCallback(self.window_title, self._mouse_callback)\n",
                "        print(\"--- Video Player Initialized for Interactive Tracking ---\")\n",
                "\n",
                "    def _mouse_callback(self, event, x, y, flags, param):\n",
                "        if self.state != 'PAUSED_FOR_SELECTION': return\n",
                "\n",
                "        if event == cv2.EVENT_LBUTTONDOWN:\n",
                "            self.is_drawing_roi = True\n",
                "            self.roi_start_point, self.roi_end_point = (x, y), (x, y)\n",
                "        elif event == cv2.EVENT_MOUSEMOVE:\n",
                "            if self.is_drawing_roi: self.roi_end_point = (x, y)\n",
                "        elif event == cv2.EVENT_LBUTTONUP:\n",
                "            if self.is_drawing_roi:\n",
                "                self.is_drawing_roi = False\n",
                "                if self.roi_end_point and self.roi_start_point and abs(self.roi_start_point[0] - self.roi_end_point[0]) > 5:\n",
                "                    x1, y1, x2, y2 = self.roi_start_point[0], self.roi_start_point[1], self.roi_end_point[0], self.roi_end_point[1]\n",
                "                    self.new_manual_box = (min(x1, x2), min(y1, y2), max(x1, x2), max(y1, y2))\n",
                "                self.roi_start_point, self.roi_end_point = None, None\n",
                "        elif event == cv2.EVENT_RBUTTONDOWN:\n",
                "            removed_selection = False\n",
                "            for i, sel in reversed(list(enumerate(self.user_selections))):\n",
                "                bbox = sel.get('bbox') or (sel['x1'], sel['y1'], sel['x2'], sel['y2'])\n",
                "                if bbox[0] < x < bbox[2] and bbox[1] < y < bbox[3]:\n",
                "                    removed_item = self.user_selections.pop(i)\n",
                "                    if 'x1' in removed_item: self.selectable_detections.append(removed_item)\n",
                "                    removed_selection = True\n",
                "                    break\n",
                "            if not removed_selection:\n",
                "                for i, det in reversed(list(enumerate(self.selectable_detections))):\n",
                "                    if det['x1'] < x < det['x2'] and det['y1'] < y < det['y2']:\n",
                "                        self.user_selections.append(self.selectable_detections.pop(i))\n",
                "                        break\n",
                "\n",
                "    def _draw_pause_menu(self, frame):\n",
                "        s = self.ui_scale_factor\n",
                "        # Scaled values for fonts and layout\n",
                "        bg_height = int(240 * s)\n",
                "        title_scale, head_scale, text_scale = 1.8 * s, 1.0 * s, 0.9 * s\n",
                "        thick_main, thick_sub = max(1, int(3 * s)), max(1, int(2 * s))\n",
                "\n",
                "        overlay = frame.copy()\n",
                "        cv2.rectangle(overlay, (0, 0), (frame.shape[1], bg_height), (0, 0, 0), -1)\n",
                "        frame = cv2.addWeighted(overlay, 0.7, frame, 0.3, 0)\n",
                "        \n",
                "        cv2.putText(frame, \"PAUSED - SELECTION MODE\", (int(25*s), int(60*s)), cv2.FONT_HERSHEY_TRIPLEX, title_scale, (0, 255, 255), thick_main)\n",
                "        cv2.putText(frame, \"Mouse Controls:\", (int(25*s), int(115*s)), cv2.FONT_HERSHEY_SIMPLEX, head_scale, (255, 255, 255), thick_main)\n",
                "        cv2.putText(frame, \"- Left-Click & Drag: Draw a new box to track\", (int(35*s), int(145*s)), cv2.FONT_HERSHEY_SIMPLEX, text_scale, (255, 255, 255), thick_sub)\n",
                "        cv2.putText(frame, \"- Right-Click: Select (Red) / Deselect (Green)\", (int(35*s), int(170*s)), cv2.FONT_HERSHEY_SIMPLEX, text_scale, (255, 255, 255), thick_sub)\n",
                "        cv2.putText(frame, \"Keyboard: C: Confirm | H: Toggle Help | Space: Pause | Q: Quit\", (int(25*s), int(210*s)), cv2.FONT_HERSHEY_SIMPLEX, text_scale, (255, 255, 255), thick_sub)\n",
                "        return frame\n",
                "    \n",
                "    def _get_numeric_input(self, frame):\n",
                "        s = self.ui_scale_factor\n",
                "        # Scaled values for fonts and layout\n",
                "        title_scale, text_scale = 1.8 * s, 1.2 * s\n",
                "        thick_main, thick_sub = max(1, int(4 * s)), max(1, int(3 * s))\n",
                "        y_offset_start, y_offset_inc = int(120*s), int(45*s)\n",
                "\n",
                "        num_input = \"\"\n",
                "        while True:\n",
                "            frame_copy, overlay = frame.copy(), frame.copy()\n",
                "            cv2.rectangle(overlay, (0, 0), (frame_copy.shape[1], frame_copy.shape[0]), (0, 0, 0), -1)\n",
                "            frame_copy = cv2.addWeighted(overlay, 0.85, frame_copy, 0.15, 0)\n",
                "            \n",
                "            current_selection_id = -1\n",
                "            try:\n",
                "                if num_input: current_selection_id = int(num_input)\n",
                "            except ValueError: pass\n",
                "\n",
                "            cv2.putText(frame_copy, \"Enter Class ID & Press Enter:\", (int(50*s), int(65*s)), cv2.FONT_HERSHEY_TRIPLEX, title_scale, (0, 255, 255), thick_main)\n",
                "            y_offset = y_offset_start\n",
                "            for i, name in self.YOLO_CLASSES.items():\n",
                "                if y_offset < frame.shape[0] - 30:\n",
                "                    color = (0, 255, 0) if i == current_selection_id else (255, 255, 255)\n",
                "                    thickness = thick_main if i == current_selection_id else thick_sub\n",
                "                    cv2.putText(frame_copy, f\"{i}: {name}\", (int(50*s), y_offset), cv2.FONT_HERSHEY_SIMPLEX, text_scale, color, thickness)\n",
                "                    y_offset += y_offset_inc\n",
                "            \n",
                "            cv2.imshow(self.window_title, frame_copy)\n",
                "            key = cv2.waitKey(0)\n",
                "            if key == 13:\n",
                "                try:\n",
                "                    if num_input and int(num_input) in self.YOLO_CLASSES: return int(num_input)\n",
                "                    else: print(f\"Error: Invalid ID. Please try again.\"); num_input = \"\"\n",
                "                except ValueError: print(\"Error: Invalid input.\"); num_input = \"\"\n",
                "            elif key == 8: num_input = num_input[:-1]\n",
                "            elif ord('0') <= key <= ord('9'): num_input += chr(key)\n",
                "            elif key == 27: return None\n",
                "\n",
                "    def play(self, tracker):\n",
                "        frame_idx = -1 # Start at -1 to handle loop logic correctly\n",
                "        temp_frame = None\n",
                "\n",
                "        while True:\n",
                "            loop_start_time = time.perf_counter()\n",
                "\n",
                "            # --- Unified Frame Loading ---\n",
                "            ret, frame = False, None\n",
                "            if self.state in ['INITIALIZING', 'PLAYING']:\n",
                "                frame_idx += 1\n",
                "                if self.source_type == 'video':\n",
                "                    ret, frame = self.cap.read()\n",
                "                elif self.source_type == 'images':\n",
                "                    if frame_idx < len(self.image_files):\n",
                "                        frame = cv2.imread(self.image_files[frame_idx])\n",
                "                        ret = frame is not None\n",
                "                if ret: temp_frame = frame.copy()\n",
                "                else: break\n",
                "            else: # Paused state\n",
                "                frame = temp_frame.copy()\n",
                "\n",
                "            # --- State Machine ---\n",
                "            display_frame = frame.copy()\n",
                "            if self.state == 'INITIALIZING' and frame_idx >= 1:\n",
                "                self.state = 'PAUSED_FOR_SELECTION'\n",
                "                self.selectable_detections = tracker.detect(display_frame)\n",
                "            elif self.state == 'PLAYING':\n",
                "                display_frame = tracker.process_frame(display_frame)\n",
                "            elif self.state == 'PAUSED_FOR_SELECTION':\n",
                "                if self.show_help: display_frame = self._draw_pause_menu(display_frame)\n",
                "                for det in self.selectable_detections: cv2.rectangle(display_frame, (det['x1'], det['y1']), (det['x2'], det['y2']), (0, 0, 255), 2)\n",
                "                for sel in self.user_selections:\n",
                "                    bbox = sel.get('bbox') or (sel['x1'], sel['y1'], sel['x2'], sel['y2'])\n",
                "                    cv2.rectangle(display_frame, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (0, 255, 0), 3)\n",
                "                if self.is_drawing_roi and self.roi_start_point and self.roi_end_point:\n",
                "                    cv2.rectangle(display_frame, self.roi_start_point, self.roi_end_point, (255, 255, 0), 2)\n",
                "                if self.new_manual_box:\n",
                "                    class_id = self._get_numeric_input(display_frame)\n",
                "                    if class_id is not None:\n",
                "                        self.user_selections.append({'bbox': self.new_manual_box, 'class_name': self.YOLO_CLASSES[class_id]})\n",
                "                    self.new_manual_box = None\n",
                "            \n",
                "            # --- Live FPS and Final Display ---\n",
                "            processing_time = time.perf_counter() - loop_start_time\n",
                "            live_fps = 1.0 / processing_time if processing_time > 0 else float('inf')\n",
                "            if self.state != 'PAUSED_FOR_SELECTION':\n",
                "                self.total_processing_time += processing_time\n",
                "                self.processed_frame_count += 1\n",
                "            \n",
                "            s = self.ui_scale_factor\n",
                "            cv2.putText(display_frame, f\"FPS: {live_fps:.1f}\", (int(20*s), int(40*s)), cv2.FONT_HERSHEY_SIMPLEX, 1.2*s, (0, 255, 0), max(1, int(2*s)))\n",
                "            cv2.imshow(self.window_title, display_frame)\n",
                "\n",
                "            wait_ms = 1\n",
                "            if self.target_fps != -1 and self.state == 'PLAYING':\n",
                "                target_duration = 1.0 / self.target_fps\n",
                "                if (delay_needed := target_duration - processing_time) > 0: wait_ms = int(delay_needed * 1000)\n",
                "            elif self.state == 'PAUSED_FOR_SELECTION': wait_ms = 20\n",
                "            \n",
                "            key = cv2.waitKey(wait_ms) & 0xFF\n",
                "            if key == ord('q'): break\n",
                "            elif key == ord('h'): self.show_help = not self.show_help\n",
                "            elif key == 32 and self.state == 'PLAYING':\n",
                "                self.state = 'PAUSED_FOR_SELECTION'\n",
                "                self.selectable_detections = tracker.detect(frame)\n",
                "                self.user_selections = list(tracker.tracked_objects)\n",
                "            elif key == ord('c') and self.state == 'PAUSED_FOR_SELECTION':\n",
                "                tracker.tracked_objects, tracker.next_track_id = [], 0\n",
                "                for sel in self.user_selections:\n",
                "                    bbox = sel.get('bbox') or (sel['x1'], sel['y1'], sel['x2'], sel['y2'])\n",
                "                    tracker.add_manual_track(temp_frame, bbox, sel['class_name'])\n",
                "                self.selectable_detections, self.user_selections, self.state = [], [], 'PLAYING'\n",
                "\n",
                "        if self.processed_frame_count > 0:\n",
                "            avg_fps = self.processed_frame_count / self.total_processing_time\n",
                "            print(f\"\\n--- Playback Finished ---\\nAverage Processing FPS: {avg_fps:.2f}\\n-------------------------\")\n",
                "        \n",
                "        self.release()\n",
                "\n",
                "    def release(self):\n",
                "        print(\"Releasing resources...\")\n",
                "        if self.cap and self.cap.isOpened(): self.cap.release()\n",
                "        cv2.destroyAllWindows()\n",
                "        for _ in range(5): cv2.waitKey(1)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ff4ec198",
            "metadata": {},
            "source": [
                "## Realtime Playback"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3adef366",
            "metadata": {},
            "outputs": [],
            "source": [
                "# VIDEO_PATH = './assets/OTB100/Skating1/img/'\n",
                "VIDEO_PATH = './assets/footage/person2.mp4'\n",
                "MODEL_PATH = './yolo11n.pt'\n",
                "TARGET_FPS = 0 # 0: standard video fps / -1: max fps\n",
                "WINDOW_SIZE = 1.0\n",
                "\n",
                "try:\n",
                "    tracker = ObjectTracker(\n",
                "        tracker_type='custom',\n",
                "        track_classes=['person', 'car'],\n",
                "        use_kalman=True,\n",
                "        base_detect_interval=12\n",
                "    )\n",
                "\n",
                "    player = VideoPlayer(\n",
                "        source=VIDEO_PATH,\n",
                "        target_fps=TARGET_FPS,\n",
                "        size_multiplier=WINDOW_SIZE,\n",
                "        window_title=\"Realtime Player\"\n",
                "    )\n",
                "    \n",
                "    player.play(tracker)\n",
                "except IOError as e:\n",
                "    print(e)\n",
                "except Exception as e:\n",
                "    print(f\"Error: {e}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "signalenv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
