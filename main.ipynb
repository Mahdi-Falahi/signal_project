{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c52d8f99",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "  <a href=\"http://www.sharif.edu/\">\n",
    "    <img src=\"https://cdn.freebiesupply.com/logos/large/2x/sharif-logo-png-transparent.png\" alt=\"SUT Logo\" width=\"140\">\n",
    "  </a>\n",
    "  \n",
    "  # Sharif University of Technology\n",
    "  ### Electrical Engineering Department\n",
    "\n",
    "  ## Signals and Systems\n",
    "  #### *Final Project - Spring 2025*\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "<div align=\"center\">\n",
    "  <h1>\n",
    "    <b>Object Tracker</b>\n",
    "  </h1>\n",
    "  <p>\n",
    "    An object tracking system using YOLO for detection and various algorithms (KCF, CSRT, MOSSE) for tracking.\n",
    "  </p>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "| Professor                  |\n",
    "| :-------------------------: |\n",
    "| Dr. Mohammad Mehdi Mojahedian |\n",
    "\n",
    "<br>\n",
    "\n",
    "| Contributors              |\n",
    "| :-----------------------: |\n",
    "| **Amirreza Mousavi** |\n",
    "| **Mahdi Falahi** |\n",
    "| **Zahra Miladipour** |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6017895",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e8b3de",
   "metadata": {},
   "source": [
    "# 0: Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3b8cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import time\n",
    "import torch\n",
    "from queue import Empty, Queue\n",
    "from threading import Thread"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427bd61d",
   "metadata": {},
   "source": [
    "# 1: Object Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8186abf2",
   "metadata": {},
   "source": [
    "## Preparing Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5a1c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"ObjectTracker using device: {device}\")\n",
    "model = YOLO('./yolo11n.pt').to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153643a7",
   "metadata": {},
   "source": [
    "## ObjectTracker Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb41a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObjectTracker:\n",
    "    def __init__(self, model, tracker_type='KCF', detect_interval=80, conf_threshold=0.5):\n",
    "\n",
    "        self.model = model\n",
    "        self.detect_interval = detect_interval\n",
    "        self.conf_threshold = conf_threshold\n",
    "        self.frame_idx = 0\n",
    "        self.tracked_objects = []\n",
    "\n",
    "        self.tracker_constructors = {\n",
    "            'CSRT': cv2.legacy.TrackerCSRT_create,\n",
    "            'KCF': cv2.legacy.TrackerKCF_create,\n",
    "            'MOSSE': cv2.legacy.TrackerMOSSE_create\n",
    "        }\n",
    "        if tracker_type not in self.tracker_constructors:\n",
    "            raise ValueError(f\"Invalid tracker type: {tracker_type}. Choose from {list(self.tracker_constructors.keys())}\")\n",
    "        self.tracker_type = tracker_type\n",
    "        print(f\"Using tracker: {self.tracker_type}\")\n",
    "\n",
    "    def process_frame(self, frame):\n",
    "        # --- DETECTION PHASE ---\n",
    "        if self.frame_idx % self.detect_interval == 0:\n",
    "            detections = self.detect(frame)\n",
    "            self.tracked_objects = [] # Reset trackers\n",
    "            \n",
    "            for obj_data in detections:\n",
    "                # Create a new tracker for each detected object\n",
    "                tracker = self.tracker_constructors[self.tracker_type]()\n",
    "                bbox = (obj_data['x1'], obj_data['y1'], obj_data['x2'] - obj_data['x1'], obj_data['y2'] - obj_data['y1'])\n",
    "                tracker.init(frame, bbox)\n",
    "                \n",
    "                # Store the tracker along with its metadata\n",
    "                obj_data['tracker'] = tracker\n",
    "                self.tracked_objects.append(obj_data)\n",
    "            \n",
    "            annotated_frame = self.draw_boxes(frame, self.tracked_objects)\n",
    "\n",
    "        # --- TRACKING PHASE ---\n",
    "        else:\n",
    "            for obj in self.tracked_objects:\n",
    "                tracker = obj['tracker']\n",
    "                success, bbox = tracker.update(frame)\n",
    "                if success:\n",
    "                    # Update object coordinates based on the tracker's prediction\n",
    "                    obj['x1'], obj['y1'] = int(bbox[0]), int(bbox[1])\n",
    "                    obj['x2'], obj['y2'] = int(bbox[0] + bbox[2]), int(bbox[1] + bbox[3])\n",
    "                # Optional: You could add logic here to remove failed trackers\n",
    "            \n",
    "            annotated_frame = self.draw_boxes(frame, self.tracked_objects)\n",
    "        \n",
    "        self.frame_idx += 1\n",
    "        return annotated_frame\n",
    "\n",
    "    def detect(self, frame):\n",
    "        results = self.model(frame, verbose=False)[0]\n",
    "        detections = []\n",
    "        for box in results.boxes:\n",
    "            conf = box.conf[0].item()\n",
    "            if conf > self.conf_threshold:\n",
    "                class_name = self.model.names[int(box.cls[0].item())]\n",
    "                coords = box.xyxy[0].tolist()\n",
    "                detections.append({\n",
    "                    'class_name': class_name,\n",
    "                    'x1': int(coords[0]), 'y1': int(coords[1]),\n",
    "                    'x2': int(coords[2]), 'y2': int(coords[3]),\n",
    "                    'conf': conf\n",
    "                })\n",
    "        return detections\n",
    "\n",
    "    def draw_boxes(self, frame, objects, boxes_color='random'):\n",
    "        frame_copy = frame.copy()\n",
    "\n",
    "        for obj in objects:\n",
    "            x1, y1, x2, y2 = obj['x1'], obj['y1'], obj['x2'], obj['y2']\n",
    "            label = f'{obj['class_name']} {obj['conf']:.2f}'\n",
    "            box_color = np.random.uniform(0, 255, 3).tolist() if boxes_color == 'random' else boxes_color\n",
    "\n",
    "            box_w = x2 - x1\n",
    "            font_scale = max(0.5, box_w / 200)\n",
    "            font_thickness = max(1, int(box_w / 150))\n",
    "\n",
    "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "            (tw, th), bl = cv2.getTextSize(label, font, font_scale, font_thickness)\n",
    "\n",
    "\n",
    "            # Adaptive text color\n",
    "            brightness_vect = np.array([0.114, 0.587, 0.299])\n",
    "            brightness = np.dot(box_color, brightness_vect)\n",
    "            text_color = (0,0,0) if brightness > 128 else (255,255,255)\n",
    "\n",
    "            cv2.rectangle(frame_copy, (x1, y1 - th - 8), (x1 + tw, y1), box_color, -1)\n",
    "            cv2.rectangle(frame_copy, (x1, y1), (x2, y2), box_color, 3)\n",
    "            cv2.putText(frame_copy, label, (x1, y1 - 4), font, font_scale, text_color, font_thickness, cv2.LINE_AA)\n",
    "            \n",
    "        return frame_copy\n",
    "    \n",
    "    def plot_image(self, frame, size_mult=1.0, frame_title=False, axis=False):\n",
    "        h, w = frame.shape[:2]\n",
    "        base_figsize = (w / 100, h / 100)\n",
    "        fig_w, fig_h = base_figsize[0]*size_mult, base_figsize[1]*size_mult\n",
    "        plt.figure(figsize=(fig_w, fig_h))\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        plt.imshow(frame_rgb)\n",
    "        plt.axis(axis)\n",
    "        if frame_title != False:\n",
    "            plt.title(frame_title)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f61c42",
   "metadata": {},
   "source": [
    "## Sample Detection Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09da1649",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img = cv2.imread('./assets/images/0001.jpg')\n",
    "\n",
    "test_tracker = ObjectTracker(model)\n",
    "test_img_detections = test_tracker.detect(test_img)\n",
    "test_tracker.plot_image(test_tracker.draw_boxes(test_img, test_img_detections))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77598323",
   "metadata": {},
   "source": [
    "# 2: Tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb94019",
   "metadata": {},
   "source": [
    "## Video Playback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e49fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoReader:\n",
    "    def __init__(self, source_path):\n",
    "        self.stream = cv2.VideoCapture(source_path)\n",
    "        if not self.stream.isOpened():\n",
    "            raise IOError(f\"Could not open video source: {source_path}\")\n",
    "        self.q = Queue(maxsize=2)\n",
    "        self.stopped = False\n",
    "        self.thread = Thread(target=self.update, args=(), daemon=True)\n",
    "\n",
    "    def start(self):\n",
    "        self.thread.start()\n",
    "\n",
    "    def update(self):\n",
    "        while not self.stopped:\n",
    "            if not self.q.full():\n",
    "                ret, frame = self.stream.read()\n",
    "                if not ret:\n",
    "                    self.stop()\n",
    "                    return\n",
    "                frame_pos = self.stream.get(cv2.CAP_PROP_POS_FRAMES)\n",
    "                self.q.put((frame, frame_pos))\n",
    "\n",
    "    def read(self):\n",
    "        try:\n",
    "            return self.q.get(timeout=1)\n",
    "        except Empty:\n",
    "            return None, None\n",
    "\n",
    "    def stop(self):\n",
    "        self.stopped = True\n",
    "        if self.thread.is_alive():\n",
    "            self.thread.join(timeout=0.5)\n",
    "        self.stream.release()\n",
    "\n",
    "\n",
    "class VideoPlayer:\n",
    "    def __init__(self, source, size_multiplier=1.0, playback_speed=1.0, window_title=\"Video Playback\"):\n",
    "        self.reader = VideoReader(source)\n",
    "        self.window_title = window_title\n",
    "        \n",
    "        # Get video properties from the underlying stream\n",
    "        self.fps = self.reader.stream.get(cv2.CAP_PROP_FPS)\n",
    "        self.frame_width = int(self.reader.stream.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        self.frame_height = int(self.reader.stream.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        self.total_frames = self.reader.stream.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "        \n",
    "        # Calculate target delay based on original FPS and desired playback speed\n",
    "        self.target_delay_ms = (1000 / self.fps) / playback_speed if self.fps > 0 else 33\n",
    "\n",
    "        # Create and resize the display window\n",
    "        cv2.namedWindow(self.window_title, cv2.WINDOW_NORMAL)\n",
    "        cv2.resizeWindow(\n",
    "            self.window_title,\n",
    "            int(self.frame_width * size_multiplier),\n",
    "            int(self.frame_height * size_multiplier)\n",
    "        )\n",
    "        print(\"--- Video Player Initialized ---\")\n",
    "        print(f\"  Resolution: {self.frame_width}x{self.frame_height}\")\n",
    "        print(f\"  Original FPS: {self.fps:.2f}\")\n",
    "        print(f\"  Total Frames: {self.total_frames}\")\n",
    "        print(f\"  Playback Speed: {playback_speed}x\")\n",
    "        print(f\"  Target Delay: {self.target_delay_ms:.2f} ms\")\n",
    "        print(\"--------------------------------\")\n",
    "\n",
    "    def play(self, tracker):\n",
    "        print(\"Starting playback... Press 'q' to quit.\")\n",
    "        last_time = time.time()\n",
    "        self.reader.start()\n",
    "        \n",
    "        while True:\n",
    "            start_time = time.perf_counter()\n",
    "            \n",
    "            # Unpack the synchronized frame and its position\n",
    "            frame, current_frame_pos = self.reader.read()\n",
    "            \n",
    "            # If the reader returns None, the video has ended or the thread stopped.\n",
    "            if frame is None:\n",
    "                print(\"Video stream ended.\")\n",
    "                break\n",
    "\n",
    "            # Process the object tracking\n",
    "            # processed_frame = tracker.process_frame(frame)\n",
    "            processed_frame = frame\n",
    "\n",
    "            # Adding FPS Annotation\n",
    "            fps = 1 / (time.time() - last_time)\n",
    "            last_time = time.time()\n",
    "            cv2.putText(processed_frame, f\"FPS: {fps:.2f}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "            cv2.imshow(self.window_title, processed_frame)\n",
    "\n",
    "            # Calculate the actual delay needed to maintain the target playback speed\n",
    "            processing_time_ms = (time.perf_counter() - start_time) * 1000\n",
    "            real_delay_ms = int(self.target_delay_ms - processing_time_ms)\n",
    "            \n",
    "            # Exit if 'q' is pressed\n",
    "            wait_key = cv2.waitKey(max(1, real_delay_ms))\n",
    "            if wait_key & 0xFF == ord('q'):\n",
    "                print(\"Playback stopped by user.\")\n",
    "                break\n",
    "            \n",
    "            # Check if this was the last frame\n",
    "            if current_frame_pos >= self.total_frames:\n",
    "                print('Video Playback finished.')\n",
    "                break\n",
    "        \n",
    "        self.release()\n",
    "\n",
    "    def release(self):\n",
    "        \"\"\"Stops the reader thread and closes all OpenCV windows.\"\"\"\n",
    "        print(\"Releasing resources...\")\n",
    "        self.reader.stop()\n",
    "        cv2.destroyAllWindows()\n",
    "        # Add a small delay to ensure windows close properly on all systems\n",
    "        for _ in range(5):\n",
    "            cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4ec198",
   "metadata": {},
   "source": [
    "## Test Playback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adef366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can change these parameters\n",
    "VIDEO_PATH = './assets/footage/car1.mp4' # Make sure this path is correct\n",
    "MODEL_PATH = './yolo11n.pt'\n",
    "PLAYBACK_SPEED = 1.5 # Play at 1.5x speed\n",
    "WINDOW_SIZE = 1   # Display window at 75% of original size\n",
    "\n",
    "try:\n",
    "    tracker = ObjectTracker(model, 'CSRT')\n",
    "    player = VideoPlayer(\n",
    "        source=VIDEO_PATH,\n",
    "        playback_speed=PLAYBACK_SPEED,\n",
    "        size_multiplier=WINDOW_SIZE,\n",
    "        window_title=\"High-Performance Player\"\n",
    "    )\n",
    "    player.play(tracker)\n",
    "except IOError as e:\n",
    "    print(e)\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "signalenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
