{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "c52d8f99",
            "metadata": {},
            "source": [
                "<div align=\"center\">\n",
                "  <a href=\"http://www.sharif.edu/\">\n",
                "    <img src=\"https://cdn.freebiesupply.com/logos/large/2x/sharif-logo-png-transparent.png\" alt=\"SUT Logo\" width=\"140\">\n",
                "  </a>\n",
                "  \n",
                "  # Sharif University of Technology\n",
                "  ### Electrical Engineering Department\n",
                "\n",
                "  ## Signals and Systems\n",
                "  #### *Final Project - Spring 2025*\n",
                "</div>\n",
                "\n",
                "---\n",
                "\n",
                "<div align=\"center\">\n",
                "  <h1>\n",
                "    <b>Object Tracker</b>\n",
                "  </h1>\n",
                "  <p>\n",
                "    An object tracking system using YOLO for detection and various algorithms (KCF, CSRT, MOSSE) for tracking.\n",
                "  </p>\n",
                "</div>\n",
                "\n",
                "<br>\n",
                "\n",
                "| Professor                  |\n",
                "| :-------------------------: |\n",
                "| Dr. Mohammad Mehdi Mojahedian |\n",
                "\n",
                "<br>\n",
                "\n",
                "| Contributors              |\n",
                "| :-----------------------: |\n",
                "| **Amirreza Mousavi** |\n",
                "| **Mahdi Falahi** |\n",
                "| **Zahra Miladipour** |"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f6017895",
            "metadata": {},
            "source": [
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "70e8b3de",
            "metadata": {},
            "source": [
                "# 0: Imports"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 28,
            "id": "0d3b8cc9",
            "metadata": {},
            "outputs": [],
            "source": [
                "import cv2\n",
                "import matplotlib.pyplot as plt\n",
                "import numpy as np\n",
                "from ultralytics import YOLO\n",
                "import time\n",
                "import torch\n",
                "from scipy.optimize import linear_sum_assignment\n",
                "import os"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "427bd61d",
            "metadata": {},
            "source": [
                "# 1: Object Detection"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "8186abf2",
            "metadata": {},
            "source": [
                "## Preparing Models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "id": "be5a1c83",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ObjectTracker using device: cuda\n"
                    ]
                }
            ],
            "source": [
                "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
                "print(f\"ObjectTracker using device: {device}\")\n",
                "model = YOLO('./yolo11n.pt').to(device)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "153643a7",
            "metadata": {},
            "source": [
                "## ObjectTracker Class"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "id": "feb41a98",
            "metadata": {},
            "outputs": [],
            "source": [
                "import cv2\n",
                "import numpy as np\n",
                "import time\n",
                "from scipy.optimize import linear_sum_assignment\n",
                "from collections import deque\n",
                "\n",
                "class ObjectTracker:\n",
                "    def __init__(self, model, tracker_type='KCF', detect_interval=24, conf_threshold=0.5, \n",
                "                 max_lost_frames=30, max_objects=10, \n",
                "                 use_kalman=True, track_classes=None, \n",
                "                 appearance_weight=0.6, occlusion_iou_threshold=0.2):\n",
                "        \n",
                "        self.model = model\n",
                "        self.detect_interval = detect_interval\n",
                "        self.conf_threshold = conf_threshold\n",
                "        self.max_lost_frames = max_lost_frames\n",
                "        self.max_objects = max_objects\n",
                "        self.use_kalman = use_kalman\n",
                "        self.track_classes = track_classes if track_classes is not None else []\n",
                "        \n",
                "        self.appearance_weight = appearance_weight\n",
                "        self.occlusion_iou_threshold = occlusion_iou_threshold\n",
                "        \n",
                "        self.frame_idx = 0\n",
                "        self.tracked_objects = []\n",
                "        self.next_track_id = 0\n",
                "\n",
                "        self.tracker_constructors = {\n",
                "            'CSRT': cv2.legacy.TrackerCSRT_create,\n",
                "            'KCF': cv2.legacy.TrackerKCF_create,\n",
                "            'MOSSE': cv2.legacy.TrackerMOSSE_create,\n",
                "            'MEDIAN_FLOW': cv2.legacy.TrackerMedianFlow_create\n",
                "        }\n",
                "        if tracker_type not in self.tracker_constructors:\n",
                "            raise ValueError(f\"Invalid tracker type: {tracker_type}. Choose from {list(self.tracker_constructors.keys())}\")\n",
                "        self.tracker_type = tracker_type\n",
                "        print(f\"--- Object Tracker Initialized (Manual Mode) ---\")\n",
                "\n",
                "    def _extract_histogram(self, frame, bbox):\n",
                "        h, w = frame.shape[:2]\n",
                "        x1, y1, x2, y2 = [int(c) for c in bbox]\n",
                "        \n",
                "        clamped_x1 = max(0, x1); clamped_y1 = max(0, y1)\n",
                "        clamped_x2 = min(w, x2); clamped_y2 = min(h, y2)\n",
                "        \n",
                "        if clamped_x1 >= clamped_x2 or clamped_y1 >= clamped_y2: return None\n",
                "            \n",
                "        roi = frame[clamped_y1:clamped_y2, clamped_x1:clamped_x2]\n",
                "        if roi.size == 0: return None\n",
                "            \n",
                "        hsv_roi = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)\n",
                "        hist = cv2.calcHist([hsv_roi], [0, 1, 2], None, [8, 4, 4], [0, 180, 0, 256, 0, 256])\n",
                "        cv2.normalize(hist, hist)\n",
                "        return hist.flatten()\n",
                "\n",
                "    def process_frame(self, frame):\n",
                "        self.tracked_objects = [t for t in self.tracked_objects if t['state'] == 'OCCLUDED' or t['lost_frames'] < self.max_lost_frames]\n",
                "        height, width = frame.shape[:2]\n",
                "\n",
                "        if self.use_kalman:\n",
                "            self._predict_phase()\n",
                "\n",
                "        self._apply_boundary_conditions(height, width)\n",
                "        self._update_phase(frame)\n",
                "        self._match_and_update_phase(frame)\n",
                "        annotated_frame = self._drawing_phase(frame)\n",
                "\n",
                "        self.frame_idx += 1\n",
                "        return annotated_frame\n",
                "\n",
                "    def _apply_boundary_conditions(self, frame_h, frame_w):\n",
                "        for track in self.tracked_objects:\n",
                "            x1, y1, x2, y2 = track['bbox']\n",
                "            bbox_w, bbox_h = x2 - x1, y2 - y1\n",
                "            \n",
                "            if bbox_w > frame_w or bbox_h > frame_h:\n",
                "                track['lost_frames'] = self.max_lost_frames\n",
                "                continue\n",
                "            \n",
                "            visible_x1 = max(x1, 0); visible_y1 = max(y1, 0)\n",
                "            visible_x2 = min(x2, frame_w); visible_y2 = min(y2, frame_h)\n",
                "            visible_area = (visible_x2 - visible_x1) * (visible_y2 - visible_y1)\n",
                "            total_area = bbox_w * bbox_h\n",
                "            \n",
                "            if total_area > 0 and (visible_area / total_area) < 0.5:\n",
                "                track['lost_frames'] = self.max_lost_frames\n",
                "\n",
                "    def _predict_phase(self):\n",
                "        for obj in self.tracked_objects:\n",
                "            obj['kf'].predict()\n",
                "            predicted_state = obj['kf'].statePost\n",
                "            cx, cy, w, h = predicted_state[0], predicted_state[1], predicted_state[2], predicted_state[3]\n",
                "            obj['bbox'] = (int(cx - w/2), int(cy - h/2), int(cx + w/2), int(cy + h/2))\n",
                "\n",
                "    def _update_phase(self, frame):\n",
                "        for obj in self.tracked_objects:\n",
                "            if obj['state'] == 'OCCLUDED': continue\n",
                "            success, bbox = obj['tracker'].update(frame)\n",
                "            if success:\n",
                "                x1, y1, w, h = [int(v) for v in bbox]\n",
                "                obj['bbox'] = (x1, y1, x1 + w, y1 + h)\n",
                "                if self.use_kalman:\n",
                "                    cx, cy = x1 + w/2, y1 + h/2\n",
                "                    measurement = np.array([cx, cy, w, h], dtype=np.float32)\n",
                "                    obj['kf'].correct(measurement)\n",
                "            else:\n",
                "                obj['lost_frames'] += 1\n",
                "\n",
                "    def _match_and_update_phase(self, frame):\n",
                "        if self.frame_idx % self.detect_interval != 0 or not self.tracked_objects:\n",
                "            return\n",
                "\n",
                "        detections = self.detect(frame)\n",
                "        if not detections: return\n",
                "\n",
                "        num_tracks = len(self.tracked_objects)\n",
                "        num_dets = len(detections)\n",
                "        cost_matrix = np.full((num_tracks, num_dets), 1e6)\n",
                "\n",
                "        for t_idx, track in enumerate(self.tracked_objects):\n",
                "            for d_idx, det in enumerate(detections):\n",
                "                if track['class_name'] != det['class_name']: continue\n",
                "                \n",
                "                iou = self._calculate_iou([track['bbox']], [[det['x1'], det['y1'], det['x2'], det['y2']]])[0,0]\n",
                "                iou_cost = 1 - iou\n",
                "\n",
                "                det_hist = self._extract_histogram(frame, (det['x1'], det['y1'], det['x2'], det['y2']))\n",
                "                if det_hist is None or not track['histogram_gallery']:\n",
                "                    appearance_cost = 0.5\n",
                "                else:\n",
                "                    correlations = [cv2.compareHist(det_hist, track_hist, cv2.HISTCMP_CORREL) for track_hist in track['histogram_gallery']]\n",
                "                    appearance_cost = 1 - max(correlations)\n",
                "\n",
                "                cost = (self.appearance_weight * appearance_cost) + ((1 - self.appearance_weight) * iou_cost)\n",
                "                cost_matrix[t_idx, d_idx] = cost\n",
                "\n",
                "        track_indices, det_indices = linear_sum_assignment(cost_matrix)\n",
                "        \n",
                "        matched_track_indices = set()\n",
                "        max_cost_threshold = 0.85\n",
                "        for t_idx, d_idx in zip(track_indices, det_indices):\n",
                "            if cost_matrix[t_idx, d_idx] < max_cost_threshold:\n",
                "                self._update_matched_track(frame, self.tracked_objects[t_idx], detections[d_idx])\n",
                "                matched_track_indices.add(t_idx)\n",
                "\n",
                "        for t_idx in range(num_tracks):\n",
                "            if t_idx not in matched_track_indices:\n",
                "                self._handle_unmatched_track(t_idx, matched_track_indices)\n",
                "\n",
                "    def _update_matched_track(self, frame, track, det):\n",
                "        x1, y1, x2, y2 = det['x1'], det['y1'], det['x2'], det['y2']\n",
                "        w, h = x2 - x1, y2 - y1\n",
                "        \n",
                "        track['bbox'] = (x1, y1, x2, y2)\n",
                "        track['lost_frames'] = 0\n",
                "        track['tracker'].init(frame, (x1, y1, w, h))\n",
                "        track['state'] = 'CONFIRMED'\n",
                "            \n",
                "        new_hist = self._extract_histogram(frame, track['bbox'])\n",
                "        if new_hist is not None:\n",
                "            track['histogram_gallery'].append(new_hist)\n",
                "            \n",
                "        if self.use_kalman:\n",
                "            cx, cy = x1 + w/2, y1 + h/2\n",
                "            measurement = np.array([cx, cy, w, h], dtype=np.float32)\n",
                "            track['kf'].correct(measurement)\n",
                "\n",
                "    def _handle_unmatched_track(self, t_idx, matched_track_indices):\n",
                "        track = self.tracked_objects[t_idx]\n",
                "        is_occluded = False\n",
                "        for m_idx in matched_track_indices:\n",
                "            iou = self._calculate_iou([track['bbox']], [self.tracked_objects[m_idx]['bbox']])[0,0]\n",
                "            if iou > self.occlusion_iou_threshold:\n",
                "                is_occluded = True\n",
                "                break\n",
                "        \n",
                "        if is_occluded:\n",
                "            track['state'] = 'OCCLUDED'\n",
                "        else:\n",
                "            track['lost_frames'] += 1\n",
                "            if track['state'] == 'OCCLUDED':\n",
                "                track['state'] = 'CONFIRMED'\n",
                "\n",
                "    def add_manual_track(self, frame, bbox, class_name):\n",
                "        x1, y1, x2, y2 = [int(c) for c in bbox]\n",
                "        w, h = x2 - x1, y2 - y1\n",
                "        if w <= 0 or h <= 0: return\n",
                "\n",
                "        new_track = {\n",
                "            'id': self.next_track_id, 'class_name': class_name, 'conf': 1.0,\n",
                "            'color': np.random.uniform(0, 255, 3).tolist(), 'bbox': (x1, y1, x2, y2),\n",
                "            'lost_frames': 0, 'state': 'CONFIRMED', \n",
                "            'histogram_gallery': deque(maxlen=10)\n",
                "        }\n",
                "\n",
                "        tracker = self.tracker_constructors[self.tracker_type]()\n",
                "        tracker.init(frame, (x1, y1, w, h))\n",
                "        new_track['tracker'] = tracker\n",
                "\n",
                "        hist = self._extract_histogram(frame, new_track['bbox'])\n",
                "        if hist is not None: new_track['histogram_gallery'].append(hist)\n",
                "\n",
                "        if self.use_kalman:\n",
                "            new_kf = self._create_kalman_filter()\n",
                "            new_kf.statePost = np.array([x1 + w/2, y1 + h/2, w, h, 0, 0, 0, 0], dtype=np.float32)\n",
                "            new_track['kf'] = new_kf\n",
                "        \n",
                "        self.tracked_objects.append(new_track)\n",
                "        self.next_track_id += 1\n",
                "                        \n",
                "    def _drawing_phase(self, frame):\n",
                "        frame_copy = frame.copy()\n",
                "        for obj in self.tracked_objects:\n",
                "            color = (255, 165, 0) if obj['state'] == 'OCCLUDED' else obj['color']\n",
                "            x1, y1, x2, y2 = [int(c) for c in obj['bbox']]\n",
                "            label = f\"ID-{obj['id']}\"\n",
                "            cv2.rectangle(frame_copy, (x1, y1), (x2, y2), color, 2)\n",
                "            cv2.putText(frame_copy, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
                "        return frame_copy\n",
                "\n",
                "    def detect(self, frame):\n",
                "        results = self.model(frame, verbose=False)[0]\n",
                "        detections = []\n",
                "        for box in results.boxes:\n",
                "            conf = box.conf[0].item()\n",
                "            if conf > self.conf_threshold:\n",
                "                class_name = self.model.names[int(box.cls[0].item())]\n",
                "                if self.track_classes and class_name not in self.track_classes:\n",
                "                    continue\n",
                "                coords = box.xyxy[0].tolist()\n",
                "                detections.append({\n",
                "                    'class_name': class_name, 'x1': int(coords[0]), 'y1': int(coords[1]),\n",
                "                    'x2': int(coords[2]), 'y2': int(coords[3]), 'conf': conf\n",
                "                })\n",
                "        return detections\n",
                "    \n",
                "    def _calculate_iou(self, bboxes1, bboxes2):\n",
                "        bboxes1 = np.array(bboxes1)\n",
                "        bboxes2 = np.array(bboxes2)\n",
                "        if bboxes1.size == 0 or bboxes2.size == 0: return np.empty((len(bboxes1), len(bboxes2)))\n",
                "        xA = np.maximum(bboxes1[:, 0][:, np.newaxis], bboxes2[:, 0])\n",
                "        yA = np.maximum(bboxes1[:, 1][:, np.newaxis], bboxes2[:, 1])\n",
                "        xB = np.minimum(bboxes1[:, 2][:, np.newaxis], bboxes2[:, 2])\n",
                "        yB = np.minimum(bboxes1[:, 3][:, np.newaxis], bboxes2[:, 3])\n",
                "        interArea = np.maximum(0, xB - xA) * np.maximum(0, yB - yA)\n",
                "        boxAArea = (bboxes1[:, 2] - bboxes1[:, 0]) * (bboxes1[:, 3] - bboxes1[:, 1])\n",
                "        boxBArea = (bboxes2[:, 2] - bboxes2[:, 0]) * (bboxes2[:, 3] - bboxes2[:, 1])\n",
                "        iou = interArea / (boxAArea[:, np.newaxis] + boxBArea - interArea + 1e-6)\n",
                "        return iou\n",
                "    \n",
                "    def _create_kalman_filter(self):\n",
                "        kf = cv2.KalmanFilter(8, 4)\n",
                "        kf.transitionMatrix = np.array([[1,0,0,0,1,0,0,0],[0,1,0,0,0,1,0,0],[0,0,1,0,0,0,1,0],[0,0,0,1,0,0,0,1],\n",
                "                                     [0,0,0,0,1,0,0,0],[0,0,0,0,0,1,0,0],[0,0,0,0,0,0,1,0],[0,0,0,0,0,0,0,1]], np.float32)\n",
                "        kf.measurementMatrix = np.array([[1,0,0,0,0,0,0,0],[0,1,0,0,0,0,0,0],[0,0,1,0,0,0,0,0],[0,0,0,1,0,0,0,0]], np.float32)\n",
                "        kf.processNoiseCov = np.eye(8, dtype=np.float32) * 0.03\n",
                "        kf.processNoiseCov[4:, 4:] *= 10\n",
                "        kf.measurementNoiseCov = np.eye(4, dtype=np.float32) * 0.1\n",
                "        return kf"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c2f61c42",
            "metadata": {},
            "source": [
                "## Sample Detection Test"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "id": "09da1649",
            "metadata": {},
            "outputs": [],
            "source": [
                "# test_img = cv2.imread('./assets/images/0001.jpg')\n",
                "\n",
                "# test_tracker = ObjectTracker(model)\n",
                "# test_img_detections = test_tracker.detect(test_img)\n",
                "# test_tracker.plot_image(test_tracker.draw_boxes(test_img, test_img_detections))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "77598323",
            "metadata": {},
            "source": [
                "# 2: Tracking"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "4eb94019",
            "metadata": {},
            "source": [
                "## VideoPlayer class"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "c0e49fee",
            "metadata": {},
            "outputs": [],
            "source": [
                "class VideoPlayer:\n",
                "    def __init__(self, source, target_fps=30, size_multiplier=1.0, window_title=\"Video Playback\"):\n",
                "        self.window_title = window_title\n",
                "        self.source = source\n",
                "        self.target_fps = target_fps\n",
                "\n",
                "        if os.path.isdir(self.source):\n",
                "            self.source_type = 'images'\n",
                "            image_extensions = ('.jpg', '.jpeg', '.png', '.bmp', '.tif', '.tiff')\n",
                "            self.image_files = sorted([os.path.join(self.source, f) for f in os.listdir(self.source) if f.lower().endswith(image_extensions)])\n",
                "            if not self.image_files: raise ValueError(\"Source directory contains no supported image files.\")\n",
                "            first_frame = cv2.imread(self.image_files[0])\n",
                "            if first_frame is None: raise IOError(f\"Could not read the first image: {self.image_files[0]}\")\n",
                "            self.frame_height, self.frame_width = first_frame.shape[:2]\n",
                "            self.cap = None\n",
                "            self.original_fps = 30\n",
                "        else:\n",
                "            self.source_type = 'video'\n",
                "            self.cap = cv2.VideoCapture(self.source)\n",
                "            if not self.cap.isOpened(): raise IOError(f\"Could not open video file: {self.source}\")\n",
                "            self.frame_width = int(self.cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
                "            self.frame_height = int(self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
                "            self.original_fps = self.cap.get(cv2.CAP_PROP_FPS)\n",
                "\n",
                "        if self.target_fps == 0:\n",
                "            self.target_fps = self.original_fps\n",
                "            print(f\"Target FPS set to 0. Using original video FPS: {self.target_fps:.2f}\")\n",
                "\n",
                "        # --- New: Adaptive UI Scaling Factor ---\n",
                "        self.ui_scale_factor = max(0.5, min(self.frame_height, 2200.0) / 1080.0) # Base scale on 1080p, with a minimum\n",
                "\n",
                "        self.total_processing_time = 0.0\n",
                "        self.processed_frame_count = 0\n",
                "        self.state = 'INITIALIZING'\n",
                "        self.selectable_detections, self.user_selections = [], []\n",
                "        self.is_drawing_roi, self.show_help = False, True\n",
                "        self.roi_start_point, self.roi_end_point, self.new_manual_box = None, None, None\n",
                "        \n",
                "        self.YOLO_CLASSES = {\n",
                "            0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', \n",
                "            5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light',\n",
                "            10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench',\n",
                "            14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow',\n",
                "            20: 'other'\n",
                "        }\n",
                "        \n",
                "        cv2.namedWindow(self.window_title, cv2.WINDOW_NORMAL)\n",
                "        cv2.resizeWindow(self.window_title, int(self.frame_width * size_multiplier), int(self.frame_height * size_multiplier))\n",
                "        \n",
                "        cv2.setMouseCallback(self.window_title, self._mouse_callback)\n",
                "        print(\"--- Video Player Initialized for Interactive Tracking ---\")\n",
                "\n",
                "    def _mouse_callback(self, event, x, y, flags, param):\n",
                "        if self.state != 'PAUSED_FOR_SELECTION': return\n",
                "\n",
                "        if event == cv2.EVENT_LBUTTONDOWN:\n",
                "            self.is_drawing_roi = True\n",
                "            self.roi_start_point, self.roi_end_point = (x, y), (x, y)\n",
                "        elif event == cv2.EVENT_MOUSEMOVE:\n",
                "            if self.is_drawing_roi: self.roi_end_point = (x, y)\n",
                "        elif event == cv2.EVENT_LBUTTONUP:\n",
                "            if self.is_drawing_roi:\n",
                "                self.is_drawing_roi = False\n",
                "                if self.roi_end_point and self.roi_start_point and abs(self.roi_start_point[0] - self.roi_end_point[0]) > 5:\n",
                "                    x1, y1, x2, y2 = self.roi_start_point[0], self.roi_start_point[1], self.roi_end_point[0], self.roi_end_point[1]\n",
                "                    self.new_manual_box = (min(x1, x2), min(y1, y2), max(x1, x2), max(y1, y2))\n",
                "                self.roi_start_point, self.roi_end_point = None, None\n",
                "        elif event == cv2.EVENT_RBUTTONDOWN:\n",
                "            removed_selection = False\n",
                "            for i, sel in reversed(list(enumerate(self.user_selections))):\n",
                "                bbox = sel.get('bbox') or (sel['x1'], sel['y1'], sel['x2'], sel['y2'])\n",
                "                if bbox[0] < x < bbox[2] and bbox[1] < y < bbox[3]:\n",
                "                    removed_item = self.user_selections.pop(i)\n",
                "                    if 'x1' in removed_item: self.selectable_detections.append(removed_item)\n",
                "                    removed_selection = True\n",
                "                    break\n",
                "            if not removed_selection:\n",
                "                for i, det in reversed(list(enumerate(self.selectable_detections))):\n",
                "                    if det['x1'] < x < det['x2'] and det['y1'] < y < det['y2']:\n",
                "                        self.user_selections.append(self.selectable_detections.pop(i))\n",
                "                        break\n",
                "\n",
                "    def _draw_pause_menu(self, frame):\n",
                "        s = self.ui_scale_factor\n",
                "        # Scaled values for fonts and layout\n",
                "        bg_height = int(240 * s)\n",
                "        title_scale, head_scale, text_scale = 1.8 * s, 1.0 * s, 0.9 * s\n",
                "        thick_main, thick_sub = max(1, int(3 * s)), max(1, int(2 * s))\n",
                "\n",
                "        overlay = frame.copy()\n",
                "        cv2.rectangle(overlay, (0, 0), (frame.shape[1], bg_height), (0, 0, 0), -1)\n",
                "        frame = cv2.addWeighted(overlay, 0.7, frame, 0.3, 0)\n",
                "        \n",
                "        cv2.putText(frame, \"PAUSED - SELECTION MODE\", (int(25*s), int(60*s)), cv2.FONT_HERSHEY_TRIPLEX, title_scale, (0, 255, 255), thick_main)\n",
                "        cv2.putText(frame, \"Mouse Controls:\", (int(25*s), int(115*s)), cv2.FONT_HERSHEY_SIMPLEX, head_scale, (255, 255, 255), thick_main)\n",
                "        cv2.putText(frame, \"- Left-Click & Drag: Draw a new box to track\", (int(35*s), int(145*s)), cv2.FONT_HERSHEY_SIMPLEX, text_scale, (255, 255, 255), thick_sub)\n",
                "        cv2.putText(frame, \"- Right-Click: Select (Red) / Deselect (Green)\", (int(35*s), int(170*s)), cv2.FONT_HERSHEY_SIMPLEX, text_scale, (255, 255, 255), thick_sub)\n",
                "        cv2.putText(frame, \"Keyboard: C: Confirm | H: Toggle Help | Space: Pause | Q: Quit\", (int(25*s), int(210*s)), cv2.FONT_HERSHEY_SIMPLEX, text_scale, (255, 255, 255), thick_sub)\n",
                "        return frame\n",
                "    \n",
                "    def _get_numeric_input(self, frame):\n",
                "        s = self.ui_scale_factor\n",
                "        # Scaled values for fonts and layout\n",
                "        title_scale, text_scale = 1.8 * s, 1.2 * s\n",
                "        thick_main, thick_sub = max(1, int(4 * s)), max(1, int(3 * s))\n",
                "        y_offset_start, y_offset_inc = int(120*s), int(45*s)\n",
                "\n",
                "        num_input = \"\"\n",
                "        while True:\n",
                "            frame_copy, overlay = frame.copy(), frame.copy()\n",
                "            cv2.rectangle(overlay, (0, 0), (frame_copy.shape[1], frame_copy.shape[0]), (0, 0, 0), -1)\n",
                "            frame_copy = cv2.addWeighted(overlay, 0.85, frame_copy, 0.15, 0)\n",
                "            \n",
                "            current_selection_id = -1\n",
                "            try:\n",
                "                if num_input: current_selection_id = int(num_input)\n",
                "            except ValueError: pass\n",
                "\n",
                "            cv2.putText(frame_copy, \"Enter Class ID & Press Enter:\", (int(50*s), int(65*s)), cv2.FONT_HERSHEY_TRIPLEX, title_scale, (0, 255, 255), thick_main)\n",
                "            y_offset = y_offset_start\n",
                "            for i, name in self.YOLO_CLASSES.items():\n",
                "                if y_offset < frame.shape[0] - 30:\n",
                "                    color = (0, 255, 0) if i == current_selection_id else (255, 255, 255)\n",
                "                    thickness = thick_main if i == current_selection_id else thick_sub\n",
                "                    cv2.putText(frame_copy, f\"{i}: {name}\", (int(50*s), y_offset), cv2.FONT_HERSHEY_SIMPLEX, text_scale, color, thickness)\n",
                "                    y_offset += y_offset_inc\n",
                "            \n",
                "            cv2.imshow(self.window_title, frame_copy)\n",
                "            key = cv2.waitKey(0)\n",
                "            if key == 13:\n",
                "                try:\n",
                "                    if num_input and int(num_input) in self.YOLO_CLASSES: return int(num_input)\n",
                "                    else: print(f\"Error: Invalid ID. Please try again.\"); num_input = \"\"\n",
                "                except ValueError: print(\"Error: Invalid input.\"); num_input = \"\"\n",
                "            elif key == 8: num_input = num_input[:-1]\n",
                "            elif ord('0') <= key <= ord('9'): num_input += chr(key)\n",
                "            elif key == 27: return None\n",
                "\n",
                "    def play(self, tracker):\n",
                "        frame_idx = -1 # Start at -1 to handle loop logic correctly\n",
                "        temp_frame = None\n",
                "\n",
                "        while True:\n",
                "            loop_start_time = time.perf_counter()\n",
                "\n",
                "            # --- Unified Frame Loading ---\n",
                "            ret, frame = False, None\n",
                "            if self.state in ['INITIALIZING', 'PLAYING']:\n",
                "                frame_idx += 1\n",
                "                if self.source_type == 'video':\n",
                "                    ret, frame = self.cap.read()\n",
                "                elif self.source_type == 'images':\n",
                "                    if frame_idx < len(self.image_files):\n",
                "                        frame = cv2.imread(self.image_files[frame_idx])\n",
                "                        ret = frame is not None\n",
                "                if ret: temp_frame = frame.copy()\n",
                "                else: break\n",
                "            else: # Paused state\n",
                "                frame = temp_frame.copy()\n",
                "\n",
                "            # --- State Machine ---\n",
                "            display_frame = frame.copy()\n",
                "            if self.state == 'INITIALIZING' and frame_idx >= 1:\n",
                "                self.state = 'PAUSED_FOR_SELECTION'\n",
                "                self.selectable_detections = tracker.detect(display_frame)\n",
                "            elif self.state == 'PLAYING':\n",
                "                display_frame = tracker.process_frame(display_frame)\n",
                "            elif self.state == 'PAUSED_FOR_SELECTION':\n",
                "                if self.show_help: display_frame = self._draw_pause_menu(display_frame)\n",
                "                for det in self.selectable_detections: cv2.rectangle(display_frame, (det['x1'], det['y1']), (det['x2'], det['y2']), (0, 0, 255), 2)\n",
                "                for sel in self.user_selections:\n",
                "                    bbox = sel.get('bbox') or (sel['x1'], sel['y1'], sel['x2'], sel['y2'])\n",
                "                    cv2.rectangle(display_frame, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (0, 255, 0), 3)\n",
                "                if self.is_drawing_roi and self.roi_start_point and self.roi_end_point:\n",
                "                    cv2.rectangle(display_frame, self.roi_start_point, self.roi_end_point, (255, 255, 0), 2)\n",
                "                if self.new_manual_box:\n",
                "                    class_id = self._get_numeric_input(display_frame)\n",
                "                    if class_id is not None:\n",
                "                        self.user_selections.append({'bbox': self.new_manual_box, 'class_name': self.YOLO_CLASSES[class_id]})\n",
                "                    self.new_manual_box = None\n",
                "            \n",
                "            # --- Live FPS and Final Display ---\n",
                "            processing_time = time.perf_counter() - loop_start_time\n",
                "            live_fps = 1.0 / processing_time if processing_time > 0 else float('inf')\n",
                "            if self.state != 'PAUSED_FOR_SELECTION':\n",
                "                self.total_processing_time += processing_time\n",
                "                self.processed_frame_count += 1\n",
                "            \n",
                "            s = self.ui_scale_factor\n",
                "            cv2.putText(display_frame, f\"FPS: {live_fps:.1f}\", (int(20*s), int(40*s)), cv2.FONT_HERSHEY_SIMPLEX, 1.2*s, (0, 255, 0), max(1, int(2*s)))\n",
                "            cv2.imshow(self.window_title, display_frame)\n",
                "\n",
                "            wait_ms = 1\n",
                "            if self.target_fps != -1 and self.state == 'PLAYING':\n",
                "                target_duration = 1.0 / self.target_fps\n",
                "                if (delay_needed := target_duration - processing_time) > 0: wait_ms = int(delay_needed * 1000)\n",
                "            elif self.state == 'PAUSED_FOR_SELECTION': wait_ms = 20\n",
                "            \n",
                "            key = cv2.waitKey(wait_ms) & 0xFF\n",
                "            if key == ord('q'): break\n",
                "            elif key == ord('h'): self.show_help = not self.show_help\n",
                "            elif key == 32 and self.state == 'PLAYING':\n",
                "                self.state = 'PAUSED_FOR_SELECTION'\n",
                "                self.selectable_detections = tracker.detect(frame)\n",
                "                self.user_selections = list(tracker.tracked_objects)\n",
                "            elif key == ord('c') and self.state == 'PAUSED_FOR_SELECTION':\n",
                "                tracker.tracked_objects, tracker.next_track_id = [], 0\n",
                "                for sel in self.user_selections:\n",
                "                    bbox = sel.get('bbox') or (sel['x1'], sel['y1'], sel['x2'], sel['y2'])\n",
                "                    tracker.add_manual_track(temp_frame, bbox, sel['class_name'])\n",
                "                self.selectable_detections, self.user_selections, self.state = [], [], 'PLAYING'\n",
                "\n",
                "        if self.processed_frame_count > 0:\n",
                "            avg_fps = self.processed_frame_count / self.total_processing_time\n",
                "            print(f\"\\n--- Playback Finished ---\\nAverage Processing FPS: {avg_fps:.2f}\\n-------------------------\")\n",
                "        \n",
                "        self.release()\n",
                "\n",
                "    def release(self):\n",
                "        print(\"Releasing resources...\")\n",
                "        if self.cap and self.cap.isOpened(): self.cap.release()\n",
                "        cv2.destroyAllWindows()\n",
                "        for _ in range(5): cv2.waitKey(1)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ff4ec198",
            "metadata": {},
            "source": [
                "## Test Playback"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "id": "3adef366",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "--- Object Tracker Initialized (Manual Mode) ---\n",
                        "Target FPS set to 0. Using original video FPS: 30.00\n",
                        "--- Video Player Initialized for Interactive Tracking ---\n",
                        "\n",
                        "--- Playback Finished ---\n",
                        "Average Processing FPS: 12.37\n",
                        "-------------------------\n",
                        "Releasing resources...\n"
                    ]
                }
            ],
            "source": [
                "# VIDEO_PATH = './assets/OTB100/Bird1/img/'\n",
                "VIDEO_PATH = './assets/footage/person4.mp4'\n",
                "MODEL_PATH = './yolo11n.pt'\n",
                "TARGET_FPS = 0\n",
                "WINDOW_SIZE = 1\n",
                "\n",
                "try:\n",
                "    tracker = ObjectTracker(model, 'CSRT', track_classes=['person'])\n",
                "    player = VideoPlayer(\n",
                "        source=VIDEO_PATH,\n",
                "        target_fps=TARGET_FPS,\n",
                "        size_multiplier=WINDOW_SIZE,\n",
                "        window_title=\"High-Performance Player\"\n",
                "    )\n",
                "    player.play(tracker)\n",
                "except IOError as e:\n",
                "    print(e)\n",
                "except Exception as e:\n",
                "    print(f\"An unexpected error occurred: {e}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "signalenv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
