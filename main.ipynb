{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "c52d8f99",
            "metadata": {},
            "source": [
                "<div align=\"center\">\n",
                "  <a href=\"http://www.sharif.edu/\">\n",
                "    <img src=\"https://cdn.freebiesupply.com/logos/large/2x/sharif-logo-png-transparent.png\" alt=\"SUT Logo\" width=\"140\">\n",
                "  </a>\n",
                "  \n",
                "  # Sharif University of Technology\n",
                "  ### Electrical Engineering Department\n",
                "\n",
                "  ## Signals and Systems\n",
                "  #### *Final Project - Spring 2025*\n",
                "</div>\n",
                "\n",
                "---\n",
                "\n",
                "<div align=\"center\">\n",
                "  <h1>\n",
                "    <b>Object Tracker</b>\n",
                "  </h1>\n",
                "  <p>\n",
                "    An object tracking system using YOLO for detection and various algorithms (KCF, CSRT, MOSSE) for tracking.\n",
                "  </p>\n",
                "</div>\n",
                "\n",
                "<br>\n",
                "\n",
                "| Professor                  |\n",
                "| :-------------------------: |\n",
                "| Dr. Mohammad Mehdi Mojahedian |\n",
                "\n",
                "<br>\n",
                "\n",
                "| Contributors              |\n",
                "| :-----------------------: |\n",
                "| **Amirreza Mousavi** |\n",
                "| **Mahdi Falahi** |\n",
                "| **Zahra Miladipour** |"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f6017895",
            "metadata": {},
            "source": [
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "70e8b3de",
            "metadata": {},
            "source": [
                "## Imports"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0d3b8cc9",
            "metadata": {},
            "outputs": [],
            "source": [
                "import cv2\n",
                "import numpy as np\n",
                "from ultralytics import YOLO\n",
                "import time\n",
                "import torch\n",
                "from scipy.optimize import linear_sum_assignment\n",
                "from scipy import fft, linalg\n",
                "import os\n",
                "import torchreid\n",
                "from PIL import Image\n",
                "from sklearn.metrics.pairwise import cosine_similarity\n",
                "from collections import deque\n",
                "from numpy import linalg"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ac40b6c5",
            "metadata": {},
            "source": [
                "## Custom Kcf"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "dfd156f1",
            "metadata": {},
            "outputs": [],
            "source": [
                "class KCFParams:\n",
                "    \"\"\"\n",
                "    A data class to hold all configurable parameters for the KCF tracker.\n",
                "    \"\"\"\n",
                "    def __init__(self):\n",
                "        self.detect_thresh = 0.1\n",
                "        self.sigma = 0.1\n",
                "        self.lambda_ = 0.0001\n",
                "        self.interp_factor = 0.1\n",
                "        self.output_sigma_factor = 1.0 / 16.0\n",
                "        self.resize = True\n",
                "        self.max_patch_size = 80 * 80\n",
                "        self.split_coeff = True\n",
                "        self.wrap_kernel = False\n",
                "        self.desc_npca = 'GRAY'\n",
                "        self.desc_pca = 'CN'\n",
                "        self.compress_feature = True\n",
                "        self.compressed_size = 2\n",
                "        self.pca_learning_rate = 0.15\n",
                "\n",
                "class KCFLOW:\n",
                "    \"\"\"\n",
                "    Python implementation of the Kernelized Correlation Filter (KCF) tracker with optional GPU acceleration.\n",
                "    \"\"\"\n",
                "    def __init__(self, parameters=KCFParams(), use_gpu=True, verbose=False):\n",
                "        self.params = parameters\n",
                "        self.use_gpu = use_gpu\n",
                "        self.verbose = verbose\n",
                "        self.device = torch.device('cuda' if self.use_gpu and torch.cuda.is_available() else 'cpu')\n",
                "        self.roi = None\n",
                "        self.frame = 0\n",
                "        self.resize_image = False\n",
                "        self.output_sigma = 0.0\n",
                "        self.yf = None\n",
                "        self.alphaf = None\n",
                "        self.alphaf_den = None\n",
                "        self.z = None\n",
                "        self.x = None\n",
                "        self.hann = None\n",
                "        self.features_pca = []\n",
                "        self.features_npca = []\n",
                "        self.descriptors_pca = []\n",
                "        self.descriptors_npca = []\n",
                "        self.proj_mtx = None\n",
                "        self.old_cov_mtx = None\n",
                "        self.use_custom_extractor_pca = False\n",
                "        self.use_custom_extractor_npca = False\n",
                "        self.extractor_pca = []\n",
                "        self.extractor_npca = []\n",
                "        self.X = {}\n",
                "        self.Z = {}\n",
                "        self.Zc = {}\n",
                "        self.k = None\n",
                "        self.kf = None\n",
                "        self.kf_lambda = None\n",
                "        self.new_alphaf = None\n",
                "        self.response = None\n",
                "        self._color_names_table = None\n",
                "        self.model_width = None\n",
                "        self.model_height = None\n",
                "        self.size_smoothing_factor = 0.4  # Increased from 0.25 to dampen size changes\n",
                "        self.prev_bbox_size = None\n",
                "        self.initial_roi_width = None\n",
                "        self.initial_roi_height = None\n",
                "        self.max_roi_scale_factor = 2.5  # Adjustable; prevents excessive growth\n",
                "\n",
                "        if self.use_gpu and self.verbose:\n",
                "            print(\"GPU acceleration enabled. Device:\", self.device)\n",
                "\n",
                "    def init(self, image, boundingBox):\n",
                "        if self.verbose:\n",
                "            print(f\"Initializing TrackerKCF with boundingBox={boundingBox}\")\n",
                "        if len(image.shape) == 2 or image.shape[2] == 1:\n",
                "            self.params.desc_pca = self.params.desc_pca.replace('CN', '')\n",
                "            self.params.desc_npca = self.params.desc_npca.replace('CN', '')\n",
                "\n",
                "        self.frame = 0\n",
                "        x, y, w, h = boundingBox\n",
                "        img_to_process = image\n",
                "        if self.params.resize and w * h > self.params.max_patch_size:\n",
                "            self.resize_image = True\n",
                "            x /= 2.0\n",
                "            y /= 2.0\n",
                "            w /= 2.0\n",
                "            h /= 2.0\n",
                "            img_to_process = cv2.resize(image, (image.shape[1] // 2, image.shape[0] // 2), interpolation=cv2.INTER_LINEAR)\n",
                "        if self.verbose:\n",
                "            print(f\"resize_image={self.resize_image}\")\n",
                "\n",
                "        roi_x = x - w / 2\n",
                "        roi_y = y - h / 2\n",
                "        roi_w = w * 2\n",
                "        roi_h = h * 2\n",
                "        self.roi = (roi_x, roi_y, roi_w, roi_h)\n",
                "        if self.verbose:\n",
                "            print(f\"roi={self.roi}\")\n",
                "        self.model_width = int(roi_w)\n",
                "        self.model_height = int(roi_h)\n",
                "        self.initial_roi_width = self.model_width\n",
                "        self.initial_roi_height = self.model_height\n",
                "        self.prev_bbox_size = (w, h)\n",
                "        if self.verbose:\n",
                "            print(f\"model_width={self.model_width}, model_height={self.model_height}\")\n",
                "            print(f\"prev_bbox_size={self.prev_bbox_size}\")\n",
                "\n",
                "        output_sigma_val = np.sqrt(w * h) * self.params.output_sigma_factor\n",
                "        self.output_sigma = -0.5 / (output_sigma_val * output_sigma_val)\n",
                "\n",
                "        win_size = (self.model_width, self.model_height)\n",
                "        self.hann = self._createHanningWindow(win_size)\n",
                "        if self.verbose:\n",
                "            print(f\"hann shape={self._get_shape(self.hann)} type={type(self.hann)}\")\n",
                "\n",
                "        sz_y, sz_x = self.model_height, self.model_width\n",
                "        j, i = np.meshgrid(np.arange(sz_x), np.arange(sz_y))\n",
                "        cy, cx = sz_y // 2, sz_x // 2\n",
                "        dist_sq = (i - cy + 1)**2 + (j - cx + 1)**2\n",
                "        y = np.exp(self.output_sigma * dist_sq)\n",
                "        self.yf = self._fft2(y)\n",
                "        if self.verbose:\n",
                "            print(f\"yf shape={self._get_shape(self.yf)} type={type(self.yf)}\")\n",
                "\n",
                "        if 'GRAY' in self.params.desc_npca.upper(): self.descriptors_npca.append('GRAY')\n",
                "        if 'CN' in self.params.desc_npca.upper(): self.descriptors_npca.append('CN')\n",
                "        if self.use_custom_extractor_npca: self.descriptors_npca.append('CUSTOM')\n",
                "\n",
                "        if 'GRAY' in self.params.desc_pca.upper(): self.descriptors_pca.append('GRAY')\n",
                "        if 'CN' in self.params.desc_pca.upper(): self.descriptors_pca.append('CN')\n",
                "        if self.use_custom_extractor_pca: self.descriptors_pca.append('CUSTOM')\n",
                "\n",
                "        features_pca, features_npca = self._extractFeatures(img_to_process, self.roi, resize_size=(self.model_width, self.model_height))\n",
                "        if not features_pca and not features_npca:\n",
                "            raise ValueError(\"No valid features extracted during initialization\")\n",
                "\n",
                "        if features_npca: self.Z[1] = self._stack(features_npca)\n",
                "        if features_pca: self.Z[0] = self._stack(features_pca)\n",
                "        if self.verbose:\n",
                "            if 0 in self.Z: print(f\"Z[0] shape={self._get_shape(self.Z[0])} type={type(self.Z[0])}\")\n",
                "            if 1 in self.Z: print(f\"Z[1] shape={self._get_shape(self.Z[1])} type={type(self.Z[1])}\")\n",
                "        \n",
                "        if self.params.compress_feature and self.Z.get(0) is not None:\n",
                "            self._updateProjectionMatrix(self.Z[0], self.params.pca_learning_rate, self.params.compressed_size)\n",
                "\n",
                "        x_parts = [self.Z.get(0), self.Z.get(1)]\n",
                "        if self.verbose:\n",
                "            print(f\"x_parts types: {[type(p) for p in x_parts if p is not None]}\")\n",
                "        if self.params.compress_feature and x_parts[0] is not None:\n",
                "            x_parts[0] = self._compress(x_parts[0])\n",
                "            if self.verbose:\n",
                "                print(f\"After compress, x_parts[0] type={type(x_parts[0])} shape={self._get_shape(x_parts[0])}\")\n",
                "        self.x = self._stack([p for p in x_parts if p is not None])\n",
                "        if self.verbose:\n",
                "            print(f\"x shape={self._get_shape(self.x)} type={type(self.x)}\")\n",
                "\n",
                "        k = self._denseGaussKernel(self.params.sigma, self.x)\n",
                "        if self.verbose:\n",
                "            print(f\"k shape={self._get_shape(k)} type={type(k)}\")\n",
                "        kf = self._fft2(k)\n",
                "        if self.verbose:\n",
                "            print(f\"kf shape={self._get_shape(kf)} type={type(kf)}\")\n",
                "        kf_lambda = kf + self.params.lambda_\n",
                "        if self.params.split_coeff:\n",
                "            self.alphaf = self._pixelWiseMult(self.yf, kf)\n",
                "            self.alphaf_den = self._pixelWiseMult(kf, kf_lambda)\n",
                "            if self.verbose:\n",
                "                print(f\"alphaf shape={self._get_shape(self.alphaf)} type={type(self.alphaf)}\")\n",
                "                print(f\"alphaf_den shape={self._get_shape(self.alphaf_den)} type={type(self.alphaf_den)}\")\n",
                "        else:\n",
                "            self.alphaf = self.yf / (kf_lambda + 1e-10)\n",
                "            if self.verbose:\n",
                "                print(f\"alphaf shape={self._get_shape(self.alphaf)} type={type(self.alphaf)}\")\n",
                "\n",
                "    def update(self, image):\n",
                "        if self.verbose:\n",
                "            print(\"Entering update\")\n",
                "        img_to_process = image\n",
                "        if self.resize_image:\n",
                "            img_to_process = cv2.resize(image, (image.shape[1] // 2, image.shape[0] // 2), interpolation=cv2.INTER_LINEAR)\n",
                "        if self.verbose:\n",
                "            print(f\"img_to_process shape={img_to_process.shape}\")\n",
                "\n",
                "        if self.frame > 0:\n",
                "            features_pca, features_npca = self._extractFeatures(img_to_process, self.roi, resize_size=(self.model_width, self.model_height))\n",
                "            if not features_pca and not features_npca:\n",
                "                return False, None\n",
                "\n",
                "            if features_npca: self.X[1] = self._stack(features_npca)\n",
                "            if features_pca: self.X[0] = self._stack(features_pca)\n",
                "            if self.verbose:\n",
                "                if 0 in self.X: print(f\"X[0] shape={self._get_shape(self.X[0])} type={type(self.X[0])}\")\n",
                "                if 1 in self.X: print(f\"X[1] shape={self._get_shape(self.X[1])} type={type(self.X[1])}\")\n",
                "\n",
                "            if self.params.compress_feature and self.X.get(0) is not None:\n",
                "                self.X[0] = self._compress(self.X[0])\n",
                "                self.Zc[0] = self._compress(self.Z[0])\n",
                "            else:\n",
                "                self.Zc[0] = self.Z.get(0)\n",
                "            self.Zc[1] = self.Z.get(1)\n",
                "            if self.verbose:\n",
                "                if 0 in self.Zc: print(f\"Zc[0] shape={self._get_shape(self.Zc[0])} type={type(self.Zc[0])}\")\n",
                "                if 1 in self.Zc: print(f\"Zc[1] shape={self._get_shape(self.Zc[1])} type={type(self.Zc[1])}\")\n",
                "\n",
                "            x_parts = [self.X.get(0), self.X.get(1)]\n",
                "            if self.verbose:\n",
                "                print(f\"x_parts types in update: {[type(p) for p in x_parts if p is not None]}\")\n",
                "            z_parts = [self.Zc.get(0), self.Zc.get(1)]\n",
                "            if self.verbose:\n",
                "                print(f\"z_parts types in update: {[type(p) for p in z_parts if p is not None]}\")\n",
                "            x = self._stack([p for p in x_parts if p is not None])\n",
                "            z = self._stack([p for p in z_parts if p is not None])\n",
                "            if self.verbose:\n",
                "                print(f\"x shape in update={self._get_shape(x)} type={type(x)}\")\n",
                "                print(f\"z shape in update={self._get_shape(z)} type={type(z)}\")\n",
                "            \n",
                "            k = self._denseGaussKernel(self.params.sigma, x, z)\n",
                "            if self.verbose:\n",
                "                print(f\"k shape in update={self._get_shape(k)} type={type(k)}\")\n",
                "            \n",
                "            kf = self._fft2(k)\n",
                "            if self.verbose:\n",
                "                print(f\"kf shape in update={self._get_shape(kf)} type={type(kf)}\")\n",
                "            self.response = self._calcResponse(self.alphaf, kf)\n",
                "            if self.verbose:\n",
                "                print(f\"response shape={self._get_shape(self.response)} type={type(self.response)}\")\n",
                "            \n",
                "            if self.use_gpu:\n",
                "                response_np = self.response.cpu().numpy()\n",
                "            else:\n",
                "                response_np = self.response\n",
                "            peak_y, peak_x = np.unravel_index(np.argmax(response_np), response_np.shape)\n",
                "            \n",
                "            peak_value = response_np[peak_y, peak_x]\n",
                "            if peak_value < self.params.detect_thresh:\n",
                "                return False, None\n",
                "\n",
                "            disp_y = peak_y - response_np.shape[0] // 2 + 1\n",
                "            disp_x = peak_x - response_np.shape[1] // 2 + 1\n",
                "\n",
                "            self.roi = (self.roi[0] + disp_x, self.roi[1] + disp_y, self.roi[2], self.roi[3])\n",
                "            if self.verbose:\n",
                "                print(f\"Updated roi after displacement={self.roi}\")\n",
                "\n",
                "            # Scale search\n",
                "            current_physical_w = self.roi[2]\n",
                "            current_physical_h = self.roi[3]\n",
                "            scales = np.arange(0.97, 1.031, 0.015)  # Fewer scales for efficiency\n",
                "            peaks = []\n",
                "            z_for_kernel = self._stack([p for p in [self.Zc.get(0), self.Zc.get(1)] if p is not None])\n",
                "            if self.verbose:\n",
                "                print(f\"z_for_kernel shape={self._get_shape(z_for_kernel)} type={type(z_for_kernel)}\")\n",
                "            for s in scales:\n",
                "                temp_physical_w = current_physical_w * s\n",
                "                temp_physical_h = current_physical_h * s\n",
                "                center_x = self.roi[0] + current_physical_w / 2\n",
                "                center_y = self.roi[1] + current_physical_h / 2\n",
                "                temp_roi_x = center_x - temp_physical_w / 2\n",
                "                temp_roi_y = center_y - temp_physical_h / 2\n",
                "                temp_roi = (temp_roi_x, temp_roi_y, temp_physical_w, temp_physical_h)\n",
                "                if self.verbose:\n",
                "                    print(f\"Scale {s}: temp_roi={temp_roi}\")\n",
                "                features_pca, features_npca = self._extractFeatures(img_to_process, temp_roi, resize_size=(self.model_width, self.model_height))\n",
                "                if not features_pca and not features_npca:\n",
                "                    peaks.append(-np.inf)\n",
                "                    continue\n",
                "                x_temp = {}\n",
                "                if features_npca: x_temp[1] = self._stack(features_npca)\n",
                "                if features_pca: x_temp[0] = self._stack(features_pca)\n",
                "                if self.params.compress_feature and x_temp.get(0) is not None:\n",
                "                    x_temp[0] = self._compress(x_temp.get(0))\n",
                "                x = self._stack([p for p in [x_temp.get(0), x_temp.get(1)] if p is not None])\n",
                "                if self.verbose:\n",
                "                    print(f\"x shape for scale {s}={self._get_shape(x)} type={type(x)}\")\n",
                "                k = self._denseGaussKernel(self.params.sigma, x, z_for_kernel)\n",
                "                kf = self._fft2(k)\n",
                "                response = self._calcResponse(self.alphaf, kf)\n",
                "                if self.use_gpu:\n",
                "                    peak = torch.max(response).item()\n",
                "                else:\n",
                "                    peak = np.max(response)\n",
                "                peaks.append(peak)\n",
                "                if self.verbose:\n",
                "                    print(f\"Peak for scale {s}: {peak}\")\n",
                "            best_idx = np.argmax(peaks)\n",
                "            best_s = scales[best_idx]\n",
                "            if peak_value < self.params.detect_thresh * 1.5:  # Low confidence; skip scale adaptation\n",
                "                best_s = 1.0\n",
                "                if self.verbose:\n",
                "                    print(\"Low confidence; skipping scale adaptation\")\n",
                "            if self.verbose:\n",
                "                print(f\"Best scale: {best_s}\")\n",
                "            current_object_w = current_physical_w / 2\n",
                "            current_object_h = current_physical_h / 2\n",
                "            new_object_w = current_object_w * best_s\n",
                "            new_object_h = current_object_h * best_s\n",
                "            if self.prev_bbox_size is not None:\n",
                "                prev_w, prev_h = self.prev_bbox_size\n",
                "                max_w_change = max(5, prev_w * self.size_smoothing_factor)\n",
                "                max_h_change = max(5, prev_h * self.size_smoothing_factor)\n",
                "                delta_w = new_object_w - prev_w\n",
                "                delta_h = new_object_h - prev_h\n",
                "                if abs(delta_w) > max_w_change:\n",
                "                    new_object_w = prev_w + np.sign(delta_w) * max_w_change\n",
                "                if abs(delta_h) > max_h_change:\n",
                "                    new_object_h = prev_h + np.sign(delta_h) * max_h_change\n",
                "            self.prev_bbox_size = (new_object_w, new_object_h)\n",
                "            if self.verbose:\n",
                "                print(f\"Updated prev_bbox_size={self.prev_bbox_size}\")\n",
                "            new_physical_w = new_object_w * 2\n",
                "            new_physical_h = new_object_h * 2\n",
                "            # Clamp to prevent unbounded growth\n",
                "            max_w = min(self.initial_roi_width * self.max_roi_scale_factor, img_to_process.shape[1] * 0.5)\n",
                "            max_h = min(self.initial_roi_height * self.max_roi_scale_factor, img_to_process.shape[0] * 0.5)\n",
                "            new_physical_w = min(max(new_physical_w, self.initial_roi_width * 0.5), max_w)\n",
                "            new_physical_h = min(max(new_physical_h, self.initial_roi_height * 0.5), max_h)\n",
                "            center_x = self.roi[0] + self.roi[2] / 2\n",
                "            center_y = self.roi[1] + self.roi[3] / 2\n",
                "            self.roi = (center_x - new_physical_w / 2, center_y - new_physical_h / 2, new_physical_w, new_physical_h)\n",
                "            if self.verbose:\n",
                "                print(f\"Updated roi after scale={self.roi}\")\n",
                "\n",
                "        features_pca, features_npca = self._extractFeatures(img_to_process, self.roi, resize_size=(self.model_width, self.model_height))\n",
                "        if not features_pca and not features_npca:\n",
                "            return False, None\n",
                "            \n",
                "        if features_npca: self.X[1] = self._stack(features_npca)\n",
                "        if features_pca: self.X[0] = self._stack(features_pca)\n",
                "        if self.verbose:\n",
                "            if 0 in self.X: print(f\"X[0] shape after update extract={self._get_shape(self.X[0])} type={type(self.X[0])}\")\n",
                "            if 1 in self.X: print(f\"X[1] shape after update extract={self._get_shape(self.X[1])} type={type(self.X[1])}\")\n",
                "        \n",
                "        interp = self.params.interp_factor\n",
                "        if self.X.get(0) is not None:\n",
                "            self.Z[0] = (1 - interp) * self.Z[0] + interp * self.X[0]\n",
                "        if self.X.get(1) is not None:\n",
                "            self.Z[1] = (1 - interp) * self.Z[1] + interp * self.X[1]\n",
                "\n",
                "        if self.params.compress_feature and self.Z.get(0) is not None:\n",
                "            self._updateProjectionMatrix(self.Z[0], self.params.pca_learning_rate, self.params.compressed_size)\n",
                "        \n",
                "        x_parts = [self.X.get(0), self.X.get(1)]\n",
                "        if self.verbose:\n",
                "            print(f\"x_parts types for model update: {[type(p) for p in x_parts if p is not None]}\")\n",
                "        if self.params.compress_feature and x_parts[0] is not None:\n",
                "            x_parts[0] = self._compress(x_parts[0])\n",
                "\n",
                "        x = self._stack([p for p in x_parts if p is not None])\n",
                "        if self.verbose:\n",
                "            print(f\"x shape for model update={self._get_shape(x)} type={type(x)}\")\n",
                "\n",
                "        k = self._denseGaussKernel(self.params.sigma, x)\n",
                "        if self.verbose:\n",
                "            print(f\"k shape for model update={self._get_shape(k)} type={type(k)}\")\n",
                "        kf = self._fft2(k)\n",
                "        if self.verbose:\n",
                "            print(f\"kf shape for model update={self._get_shape(kf)} type={type(kf)}\")\n",
                "        kf_lambda = kf + self.params.lambda_\n",
                "        if self.params.split_coeff:\n",
                "            new_alphaf = self._pixelWiseMult(self.yf, kf)\n",
                "            new_alphaf_den = self._pixelWiseMult(kf, kf_lambda)\n",
                "        else:\n",
                "            new_alphaf = self.yf / (kf_lambda + 1e-10)\n",
                "\n",
                "        self.alphaf = (1 - interp) * self.alphaf + interp * new_alphaf\n",
                "        if self.params.split_coeff:\n",
                "            self.alphaf_den = (1 - interp) * self.alphaf_den + interp * new_alphaf_den\n",
                "\n",
                "        rx, ry, rw, rh = self.roi\n",
                "        obj_w, obj_h = rw / 2, rh / 2\n",
                "        obj_x, obj_y = rx + rw / 2, ry + rh / 2\n",
                "        \n",
                "        if self.resize_image:\n",
                "            obj_x *= 2\n",
                "            obj_y *= 2\n",
                "            obj_w *= 2\n",
                "            obj_h *= 2\n",
                "\n",
                "        img_h, img_w = image.shape[:2]\n",
                "        left = max(0.0, obj_x - obj_w / 2)\n",
                "        top = max(0.0, obj_y - obj_h / 2)\n",
                "        right = min(float(img_w), obj_x + obj_w / 2)\n",
                "        bottom = min(float(img_h), obj_y + obj_h / 2)\n",
                "        final_bbox = (left, top, right - left, bottom - top)\n",
                "        if self.verbose:\n",
                "            print(f\"Final bbox={final_bbox}\")\n",
                "        \n",
                "        self.frame += 1\n",
                "        return True, final_bbox\n",
                "\n",
                "    def setFeatureExtractor(self, callback, pca_func=False):\n",
                "        if pca_func:\n",
                "            self.extractor_pca.append(callback)\n",
                "            self.use_custom_extractor_pca = True\n",
                "        else:\n",
                "            self.extractor_npca.append(callback)\n",
                "            self.use_custom_extractor_npca = True\n",
                "\n",
                "    def _get_shape(self, obj):\n",
                "        if self.use_gpu and torch.is_tensor(obj):\n",
                "            return obj.shape\n",
                "        else:\n",
                "            return obj.shape if hasattr(obj, 'shape') else None\n",
                "\n",
                "    def _stack(self, arrays):\n",
                "        if self.use_gpu:\n",
                "            tensors = []\n",
                "            for a in arrays:\n",
                "                if a is not None:\n",
                "                    if torch.is_tensor(a):\n",
                "                        tensors.append(a.to(self.device).float())\n",
                "                    else:\n",
                "                        tensors.append(torch.from_numpy(a).to(self.device).float())\n",
                "            return torch.dstack(tensors) if tensors else None\n",
                "        else:\n",
                "            return np.dstack(arrays) if arrays else None\n",
                "\n",
                "    def _to_tensor(self, arr):\n",
                "        if self.use_gpu and not torch.is_tensor(arr):\n",
                "            return torch.from_numpy(arr).to(self.device).float()\n",
                "        return arr\n",
                "\n",
                "    def _to_numpy(self, tensor):\n",
                "        if self.use_gpu and torch.is_tensor(tensor):\n",
                "            return tensor.cpu().numpy()\n",
                "        return tensor\n",
                "\n",
                "    def _fft2(self, src):\n",
                "        if self.use_gpu:\n",
                "            src = self._to_tensor(src)\n",
                "            return torch.fft.fft2(src, dim=(0, 1))\n",
                "        else:\n",
                "            return fft.fft2(src, axes=(0, 1))\n",
                "\n",
                "    def _ifft2(self, src):\n",
                "        if self.use_gpu:\n",
                "            result = torch.fft.ifft2(src, dim=(0, 1)).real\n",
                "            return result\n",
                "        else:\n",
                "            return np.real(fft.ifft2(src, axes=(0, 1)))\n",
                "\n",
                "    def _pixelWiseMult(self, s1, s2, conjB=False):\n",
                "        s1 = self._to_tensor(s1)\n",
                "        s2 = self._to_tensor(s2)\n",
                "        if conjB:\n",
                "            result = s1 * torch.conj(s2) if self.use_gpu else s1 * np.conj(s2)\n",
                "        else:\n",
                "            result = s1 * s2\n",
                "        return result\n",
                "\n",
                "    def _sumChannels(self, src):\n",
                "        src = self._to_tensor(src)\n",
                "        if self.use_gpu:\n",
                "            return torch.sum(src, dim=2) if src.dim() == 3 else src\n",
                "        else:\n",
                "            return np.sum(src, axis=2) if src.ndim == 3 else src\n",
                "\n",
                "    def _createHanningWindow(self, s):\n",
                "        hann = cv2.createHanningWindow(s, cv2.CV_32F)\n",
                "        if self.use_gpu:\n",
                "            return torch.from_numpy(hann).to(self.device).float()\n",
                "        return hann\n",
                "\n",
                "    def _shift(self, mat, dx, dy):\n",
                "        if self.use_gpu:\n",
                "            mat = self._to_tensor(mat)\n",
                "            return torch.roll(mat, shifts=(dy, dx), dims=(0, 1))\n",
                "        else:\n",
                "            return np.roll(mat, (dy, dx), axis=(0, 1))\n",
                "    \n",
                "    def _extractFeatures(self, image, roi, resize_size=None):\n",
                "        if self.verbose:\n",
                "            print(f\"Entering _extractFeatures with roi={roi}, resize_size={resize_size}\")\n",
                "        features_npca = []\n",
                "        features_pca = []\n",
                "\n",
                "        for d in self.descriptors_npca:\n",
                "            if d == 'CUSTOM':\n",
                "                continue\n",
                "            if self.verbose:\n",
                "                print(f\"Processing descriptor_npca: {d}\")\n",
                "            patch = self._get_patch(image, roi)\n",
                "            if patch is None:\n",
                "                if self.verbose:\n",
                "                    print(\"Patch is None for npca\")\n",
                "                continue\n",
                "            if self.verbose:\n",
                "                print(f\"Patch shape for npca {d}: {patch.shape}\")\n",
                "            if resize_size:\n",
                "                if self.use_gpu:\n",
                "                    patch_tensor = torch.from_numpy(patch).to(self.device).permute(2, 0, 1).unsqueeze(0).float()\n",
                "                    patch = torch.nn.functional.interpolate(patch_tensor, size=(resize_size[1], resize_size[0]), mode='bilinear', align_corners=False).squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
                "                else:\n",
                "                    patch = cv2.resize(patch, resize_size, interpolation=cv2.INTER_LINEAR)\n",
                "                if self.verbose:\n",
                "                    print(f\"Resized patch shape for npca {d}: {patch.shape}\")\n",
                "            if d == 'GRAY':\n",
                "                f = self._extractGray(patch, apply_hann=False)\n",
                "            elif d == 'CN':\n",
                "                f = self._extractCN(patch, apply_hann=False)\n",
                "            else:\n",
                "                f = None\n",
                "            if f is not None:\n",
                "                if self.verbose:\n",
                "                    print(f\"f shape for npca {d}: {f.shape}\")\n",
                "                if f.ndim == 2:\n",
                "                    f = f[:, :, np.newaxis]\n",
                "                    if self.verbose:\n",
                "                        print(f\"After newaxis for npca {d}: {f.shape}\")\n",
                "                hann_expanded = self._to_tensor(self.hann)[:, :, None] if self.use_gpu else self.hann[:, :, np.newaxis]\n",
                "                if self.verbose:\n",
                "                    print(f\"hann_expanded shape for npca {d}: {self._get_shape(hann_expanded)} type={type(hann_expanded)}\")\n",
                "                f_tensor = self._to_tensor(f)\n",
                "                if self.verbose:\n",
                "                    print(f\"f_tensor shape before multiply: {self._get_shape(f_tensor)} type={type(f_tensor)}\")\n",
                "                multiplied = f_tensor * hann_expanded\n",
                "                f = self._to_numpy(multiplied)\n",
                "                if self.verbose:\n",
                "                    print(f\"After hann multiply for npca {d}: {f.shape} type={type(f)}\")\n",
                "                features_npca.append(f)\n",
                "\n",
                "        for d in self.descriptors_pca:\n",
                "            if d == 'CUSTOM':\n",
                "                continue\n",
                "            if self.verbose:\n",
                "                print(f\"Processing descriptor_pca: {d}\")\n",
                "            patch = self._get_patch(image, roi)\n",
                "            if patch is None:\n",
                "                if self.verbose:\n",
                "                    print(\"Patch is None for pca\")\n",
                "                continue\n",
                "            if self.verbose:\n",
                "                print(f\"Patch shape for pca {d}: {patch.shape}\")\n",
                "            if resize_size:\n",
                "                if self.use_gpu:\n",
                "                    patch_tensor = torch.from_numpy(patch).to(self.device).permute(2, 0, 1).unsqueeze(0).float()\n",
                "                    patch = torch.nn.functional.interpolate(patch_tensor, size=(resize_size[1], resize_size[0]), mode='bilinear', align_corners=False).squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
                "                else:\n",
                "                    patch = cv2.resize(patch, resize_size, interpolation=cv2.INTER_LINEAR)\n",
                "                if self.verbose:\n",
                "                    print(f\"Resized patch shape for pca {d}: {patch.shape}\")\n",
                "            if d == 'GRAY':\n",
                "                f = self._extractGray(patch, apply_hann=False)\n",
                "            elif d == 'CN':\n",
                "                f = self._extractCN(patch, apply_hann=False)\n",
                "            else:\n",
                "                f = None\n",
                "            if f is not None:\n",
                "                if self.verbose:\n",
                "                    print(f\"f shape for pca {d}: {f.shape}\")\n",
                "                if f.ndim == 2:\n",
                "                    f = f[:, :, np.newaxis]\n",
                "                    if self.verbose:\n",
                "                        print(f\"After newaxis for pca {d}: {f.shape}\")\n",
                "                hann_expanded = self._to_tensor(self.hann)[:, :, None] if self.use_gpu else self.hann[:, :, np.newaxis]\n",
                "                if self.verbose:\n",
                "                    print(f\"hann_expanded shape for pca {d}: {self._get_shape(hann_expanded)} type={type(hann_expanded)}\")\n",
                "                f_tensor = self._to_tensor(f)\n",
                "                if self.verbose:\n",
                "                    print(f\"f_tensor shape before multiply: {self._get_shape(f_tensor)} type={type(f_tensor)}\")\n",
                "                multiplied = f_tensor * hann_expanded\n",
                "                f = self._to_numpy(multiplied)\n",
                "                if self.verbose:\n",
                "                    print(f\"After hann multiply for pca {d}: {f.shape} type={type(f)}\")\n",
                "                features_pca.append(f)\n",
                "\n",
                "        for extractor in self.extractor_npca:\n",
                "            if self.verbose:\n",
                "                print(\"Processing custom extractor_npca\")\n",
                "            feat = extractor(image, roi)\n",
                "            if feat is not None:\n",
                "                if self.verbose:\n",
                "                    print(f\"Custom feat shape npca: {feat.shape}\")\n",
                "                if feat.ndim == 2:\n",
                "                    feat = feat[:, :, np.newaxis]\n",
                "                    if self.verbose:\n",
                "                        print(f\"After newaxis custom npca: {feat.shape}\")\n",
                "                hann_expanded = self._to_tensor(self.hann)[:, :, None] if self.use_gpu else self.hann[:, :, np.newaxis]\n",
                "                if self.verbose:\n",
                "                    print(f\"hann_expanded shape custom npca: {self._get_shape(hann_expanded)} type={type(hann_expanded)}\")\n",
                "                feat_tensor = self._to_tensor(feat)\n",
                "                if self.verbose:\n",
                "                    print(f\"feat_tensor shape before multiply custom npca: {self._get_shape(feat_tensor)} type={type(feat_tensor)}\")\n",
                "                multiplied = feat_tensor * hann_expanded\n",
                "                feat = self._to_numpy(multiplied)\n",
                "                if self.verbose:\n",
                "                    print(f\"After hann multiply custom npca: {feat.shape} type={type(feat)}\")\n",
                "                features_npca.append(feat)\n",
                "        \n",
                "        for extractor in self.extractor_pca:\n",
                "            if self.verbose:\n",
                "                print(\"Processing custom extractor_pca\")\n",
                "            feat = extractor(image, roi)\n",
                "            if feat is not None:\n",
                "                if self.verbose:\n",
                "                    print(f\"Custom feat shape pca: {feat.shape}\")\n",
                "                if feat.ndim == 2:\n",
                "                    feat = feat[:, :, np.newaxis]\n",
                "                    if self.verbose:\n",
                "                        print(f\"After newaxis custom pca: {feat.shape}\")\n",
                "                hann_expanded = self._to_tensor(self.hann)[:, :, None] if self.use_gpu else self.hann[:, :, np.newaxis]\n",
                "                if self.verbose:\n",
                "                    print(f\"hann_expanded shape custom pca: {self._get_shape(hann_expanded)} type={type(hann_expanded)}\")\n",
                "                feat_tensor = self._to_tensor(feat)\n",
                "                if self.verbose:\n",
                "                    print(f\"feat_tensor shape before multiply custom pca: {self._get_shape(feat_tensor)} type={type(feat_tensor)}\")\n",
                "                multiplied = feat_tensor * hann_expanded\n",
                "                feat = self._to_numpy(multiplied)\n",
                "                if self.verbose:\n",
                "                    print(f\"After hann multiply custom pca: {feat.shape} type={type(feat)}\")\n",
                "                features_pca.append(feat)\n",
                "            \n",
                "        return features_pca, features_npca\n",
                "\n",
                "    def _get_patch(self, img, roi):\n",
                "        if self.verbose:\n",
                "            print(f\"Getting patch for roi={roi}\")\n",
                "        x, y, w, h = map(int, roi)\n",
                "        img_h, img_w = img.shape[:2]\n",
                "        vx, vy = max(0, x), max(0, y)\n",
                "        rb, bb = min(x + w, img_w), min(y + h, img_h)\n",
                "        vw, vh = rb - vx, bb - vy\n",
                "        if vw <= 0 or vh <= 0:\n",
                "            return None\n",
                "        sub = img[vy:bb, vx:rb]\n",
                "        top, bot = max(0, -y), max(0, y + h - img_h)\n",
                "        left, rgt = max(0, -x), max(0, x + w - img_w)\n",
                "        patch = cv2.copyMakeBorder(sub, top, bot, left, rgt, cv2.BORDER_REPLICATE)\n",
                "        if self.verbose:\n",
                "            print(f\"Returned patch shape={patch.shape}\")\n",
                "        return patch\n",
                "\n",
                "    def _extractGray(self, p, apply_hann=True):\n",
                "        if p is None or p.size == 0: return None\n",
                "        if self.use_gpu:\n",
                "            p_tensor = torch.from_numpy(p).to(self.device).float()\n",
                "            if p_tensor.dim() == 3:\n",
                "                f = torch.mean(p_tensor, dim=2)\n",
                "            else:\n",
                "                f = p_tensor\n",
                "            f = f / 255.0 - 0.5\n",
                "            if apply_hann:\n",
                "                hann = self._to_tensor(self.hann)\n",
                "                f = f * hann\n",
                "            return f.cpu().numpy()\n",
                "        else:\n",
                "            f = cv2.cvtColor(p, cv2.COLOR_BGR2GRAY) if p.ndim > 2 else p\n",
                "            f = f.astype(np.float32) / 255.0 - 0.5\n",
                "            if apply_hann:\n",
                "                f = f * self.hann\n",
                "            return f\n",
                "\n",
                "    def _extractCN(self, p, apply_hann=True):\n",
                "        if p is None or p.size == 0: return None\n",
                "        if self._color_names_table is None: self._loadColorNamesTable()\n",
                "        if self.use_gpu:\n",
                "            if isinstance(self._color_names_table, np.ndarray):\n",
                "                self._color_names_table = torch.from_numpy(self._color_names_table).to(self.device).float()\n",
                "            p_tensor = torch.from_numpy(p).to(self.device).long()\n",
                "            px = p_tensor.view(-1, 3) // 8\n",
                "            idx = px[:, 2] + 32 * px[:, 1] + 1024 * px[:, 0]\n",
                "            f = self._color_names_table[idx].view(p.shape[0], p.shape[1], -1)\n",
                "            if apply_hann:\n",
                "                hann = self._to_tensor(self.hann)[:, :, None]\n",
                "                f = f * hann\n",
                "            return f.cpu().numpy()\n",
                "        else:\n",
                "            px = p.reshape(-1, 3).astype(np.int32)\n",
                "            idx = (px[:, 2] // 8) + 32 * (px[:, 1] // 8) + 1024 * (px[:, 0] // 8)\n",
                "            f = self._color_names_table[idx].reshape(p.shape[0], p.shape[1], 10).astype(np.float32)\n",
                "            if apply_hann:\n",
                "                f = f * self.hann[:, :, np.newaxis]\n",
                "            return f\n",
                "\n",
                "    def _loadColorNamesTable(self, path=\"colornames.npy\"):\n",
                "        try:\n",
                "            self._color_names_table = np.load(path)\n",
                "        except IOError:\n",
                "            raise IOError(f\"Color table '{path}' not found. Ensure the file exists or generate it from the original ColorNames array.\")\n",
                "\n",
                "    def _denseGaussKernel(self, sigma, x, y=None):\n",
                "        if self.verbose:\n",
                "            print(\"Entering _denseGaussKernel\")\n",
                "        x = self._to_tensor(x)\n",
                "        if self.verbose:\n",
                "            print(f\"x shape in kernel={self._get_shape(x)} type={type(x)}\")\n",
                "        if y is None: \n",
                "            y = x\n",
                "        else:\n",
                "            y = self._to_tensor(y)\n",
                "            if self.verbose:\n",
                "                print(f\"y shape in kernel={self._get_shape(y)} type={type(y)}\")\n",
                "        xf = self._fft2(x)\n",
                "        yf = self._fft2(y)\n",
                "        if self.verbose:\n",
                "            print(f\"xf shape={self._get_shape(xf)} type={type(xf)}, yf shape={self._get_shape(yf)} type={type(yf)}\")\n",
                "        if self.use_gpu:\n",
                "            nx_sq = torch.sum(x ** 2).item()\n",
                "            ny_sq = torch.sum(y ** 2).item()\n",
                "        else:\n",
                "            nx_sq = np.sum(x**2)\n",
                "            ny_sq = np.sum(y**2)\n",
                "        if self.verbose:\n",
                "            print(f\"nx_sq={nx_sq}, ny_sq={ny_sq}\")\n",
                "        xyf = self._pixelWiseMult(xf, yf, conjB=True)\n",
                "        if self.verbose:\n",
                "            print(f\"xyf shape={self._get_shape(xyf)} type={type(xyf)}\")\n",
                "        xy_sum = self._sumChannels(xyf)\n",
                "        if self.verbose:\n",
                "            print(f\"xy_sum shape={self._get_shape(xy_sum)} type={type(xy_sum)}\")\n",
                "        xy = self._ifft2(xy_sum)\n",
                "        if self.verbose:\n",
                "            print(f\"xy shape={self._get_shape(xy)} type={type(xy)}\")\n",
                "        if self.params.wrap_kernel:\n",
                "            shift_dy = int(self._get_shape(x)[0] // 2)\n",
                "            shift_dx = int(self._get_shape(x)[1] // 2)\n",
                "            xy = self._shift(xy, shift_dx, shift_dy)\n",
                "        if self.use_gpu:\n",
                "            d = (nx_sq + ny_sq - 2 * xy) / x.numel()\n",
                "            k = torch.exp(torch.clamp(d, min=0) / (sigma**2 * -1))\n",
                "        else:\n",
                "            d = (nx_sq + ny_sq - 2 * xy) / x.size\n",
                "            k = np.exp(-np.maximum(0, d) / (sigma**2))\n",
                "        if self.verbose:\n",
                "            print(f\"k shape returned={self._get_shape(k)} type={type(k)}\")\n",
                "        return k\n",
                "\n",
                "    def _calcResponse(self, alphaf, kf):\n",
                "        if self.verbose:\n",
                "            print(\"Entering _calcResponse\")\n",
                "        alphaf = self._to_tensor(alphaf)\n",
                "        kf = self._to_tensor(kf)\n",
                "        if self.verbose:\n",
                "            print(f\"alphaf shape={self._get_shape(alphaf)} type={type(alphaf)}, kf shape={self._get_shape(kf)} type={type(kf)}\")\n",
                "        if not self.params.split_coeff:\n",
                "            spec = self._pixelWiseMult(alphaf, kf)\n",
                "            return self._ifft2(spec)\n",
                "        else:\n",
                "            spec = self._pixelWiseMult(alphaf, kf)\n",
                "            response_spec = spec / (self._to_tensor(self.alphaf_den) + 1e-10)\n",
                "            return self._ifft2(response_spec)\n",
                "\n",
                "    def _updateProjectionMatrix(self, src, pca_rate, compressed_sz):\n",
                "        if self.verbose:\n",
                "            print(\"Entering _updateProjectionMatrix\")\n",
                "        src = self._to_tensor(src)\n",
                "        if self.verbose:\n",
                "            print(f\"src shape={self._get_shape(src)} type={type(src)}\")\n",
                "        num_pixels = src.shape[0] * src.shape[1]\n",
                "        num_channels = src.shape[2] if len(src.shape) > 2 else 1\n",
                "        compressed_sz = min(compressed_sz, num_channels)\n",
                "        data = src.reshape(num_pixels, num_channels)\n",
                "        if self.use_gpu:\n",
                "            mean = torch.mean(data, dim=0)\n",
                "            data_nomean = data - mean\n",
                "            new_cov = (data_nomean.t() @ data_nomean) / (num_pixels - 1)\n",
                "        else:\n",
                "            mean = np.mean(data, axis=0)\n",
                "            data_nomean = data - mean\n",
                "            new_cov = (data_nomean.T @ data_nomean) / (num_pixels - 1)\n",
                "        if self.old_cov_mtx is None:\n",
                "            self.old_cov_mtx = new_cov\n",
                "        else:\n",
                "            self.old_cov_mtx = self._to_tensor(self.old_cov_mtx)\n",
                "            self.old_cov_mtx = (1 - pca_rate) * self.old_cov_mtx + pca_rate * new_cov\n",
                "        if self.use_gpu:\n",
                "            u, s, _ = torch.linalg.svd(self.old_cov_mtx, full_matrices=True)\n",
                "        else:\n",
                "            u, s, _ = linalg.svd(self.old_cov_mtx, full_matrices=True)\n",
                "        self.proj_mtx = u[:, :compressed_sz]\n",
                "        if self.use_gpu:\n",
                "            proj_vars = torch.diag(s[:compressed_sz])\n",
                "            stab_term = self.proj_mtx @ proj_vars @ self.proj_mtx.t()\n",
                "        else:\n",
                "            proj_vars = np.diag(s[:compressed_sz])\n",
                "            stab_term = self.proj_mtx @ proj_vars @ self.proj_mtx.T\n",
                "        self.old_cov_mtx = (1 - pca_rate) * self.old_cov_mtx + pca_rate * stab_term\n",
                "        if self.verbose:\n",
                "            print(f\"proj_mtx shape={self._get_shape(self.proj_mtx)} type={type(self.proj_mtx)}\")\n",
                "\n",
                "    def _compress(self, src):\n",
                "        if self.verbose:\n",
                "            print(f\"Compressing src shape={self._get_shape(src)} type={type(src)}\")\n",
                "        src = self._to_tensor(src)\n",
                "        self.proj_mtx = self._to_tensor(self.proj_mtx)\n",
                "        data = src.reshape(-1, src.shape[2] if len(src.shape) > 2 else 1)\n",
                "        if self.use_gpu:\n",
                "            compressed = data @ self.proj_mtx\n",
                "        else:\n",
                "            compressed = data @ self.proj_mtx\n",
                "        compressed_reshaped = compressed.reshape(src.shape[0], src.shape[1], -1)\n",
                "        if self.verbose:\n",
                "            print(f\"Compressed shape={self._get_shape(compressed_reshaped)} type={type(compressed_reshaped)}\")\n",
                "        if self.use_gpu:\n",
                "            return compressed_reshaped\n",
                "        else:\n",
                "            return self._to_numpy(compressed_reshaped)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "153643a7",
            "metadata": {},
            "source": [
                "## TrackingSystem Class"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "feb41a98",
            "metadata": {},
            "outputs": [],
            "source": [
                "class TrackingSystem:\n",
                "    def __init__(self, model='./yolo11n.pt', tracker_type='CKCF', base_detect_interval=24, conf_threshold=0.5, \n",
                "                 max_lost_frames=30, lost_track_buffer=60,\n",
                "                 use_kalman=True, track_classes=None, \n",
                "                 appearance_weight=0.6, match_cost_threshold=0.85, \n",
                "                 reid_cost_threshold=0.3, occlusion_iou_threshold=0.2,\n",
                "                 iou_gating_threshold=0.1, **kwargs):\n",
                "        \n",
                "        # --- Check Cuda Presence ---\n",
                "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
                "        print(f'object tracker running on {self.device}')\n",
                "\n",
                "        # --- Core Parameters ---\n",
                "        self.detect_interval = base_detect_interval\n",
                "        self.conf_threshold = conf_threshold\n",
                "        self.max_lost_frames = max_lost_frames\n",
                "        self.track_classes = track_classes if track_classes is not None else []\n",
                "        self.model = YOLO(model).to(self.device)\n",
                "        print('--- Yolo loaded successfully ---')\n",
                "        \n",
                "        # --- Cost & Matching Parameters ---\n",
                "        self.appearance_weight = appearance_weight\n",
                "        self.match_cost_threshold = match_cost_threshold\n",
                "        self.reid_cost_threshold = reid_cost_threshold\n",
                "        self.occlusion_iou_threshold = occlusion_iou_threshold\n",
                "        self.iou_gating_threshold = iou_gating_threshold\n",
                "        \n",
                "        # --- State Management ---\n",
                "        self.frame_idx = 0\n",
                "        self.tracked_objects = []\n",
                "        self.lost_tracks = deque(maxlen=lost_track_buffer)\n",
                "        self.next_track_id = 0\n",
                "        \n",
                "        # --- Kalman Filter ---\n",
                "        self.use_kalman = use_kalman\n",
                "\n",
                "        # --- Re-ID Models & Warm-up ---\n",
                "        self.reid_models = self._load_reid_models()\n",
                "        for config in self.reid_models.values():\n",
                "            reid_model = config['model']\n",
                "            dummy_input = torch.randn(1, 3, 256, 128).to(self.device)\n",
                "            with torch.no_grad():\n",
                "                reid_model(dummy_input)\n",
                "        print(\"--- osnet loaded successfuly ---\")\n",
                "\n",
                "        # --- Tracker Constructors ---\n",
                "        self.tracker_constructors = {\n",
                "            'CSRT': cv2.legacy.TrackerCSRT_create, 'KCF': cv2.legacy.TrackerKCF_create,\n",
                "            'MOSSE': cv2.legacy.TrackerMOSSE_create, 'MEDIAN_FLOW': cv2.legacy.TrackerMedianFlow_create,\n",
                "            \"CKCF\":KCFLOW\n",
                "            \n",
                "        }\n",
                "        if tracker_type.upper() not in self.tracker_constructors:\n",
                "            raise ValueError(f\"Invalid tracker type: {tracker_type}. Choose from {list(self.tracker_constructors.keys())}\")\n",
                "        self.tracker_type = tracker_type.upper()\n",
                "        print(f\"--- Object Tracker Initialized ---\")\n",
                "\n",
                "    def _load_reid_models(self):\n",
                "        \"\"\"Loads pre-trained Re-ID models for different object classes.\"\"\"\n",
                "        models = {}\n",
                "        person_model = torchreid.models.build_model(name='osnet_x1_0', num_classes=4101, pretrained=False)\n",
                "        torchreid.utils.load_pretrained_weights(person_model, 'osnet_x1_0_msmt17_256x128.pth')\n",
                "        person_model.to(self.device).eval()\n",
                "        person_transform, _ = torchreid.data.transforms.build_transforms(height=256, width=128, is_train=False)\n",
                "        models['person'] = {'model': person_model, 'transform': person_transform}\n",
                "        models['car'] = {'model': person_model, 'transform': person_transform}\n",
                "        return models\n",
                "\n",
                "    def _extract_embedding(self, frame, bbox, track):\n",
                "        \"\"\"Extracts a feature embedding from a single bounding box.\"\"\"\n",
                "        reid_config = track.get('reid_config')\n",
                "        if not reid_config: return None\n",
                "        model, transform = reid_config['model'], reid_config['transform']\n",
                "        \n",
                "        x1, y1, x2, y2 = [int(c) for c in bbox]\n",
                "        roi = frame[y1:y2, x1:x2]\n",
                "        if roi.size == 0: return None\n",
                "\n",
                "        roi_rgb = cv2.cvtColor(roi, cv2.COLOR_BGR2RGB)\n",
                "        image_tensor = transform(Image.fromarray(roi_rgb)).unsqueeze(0).to(self.device)\n",
                "        \n",
                "        with torch.no_grad():\n",
                "            embedding = model(image_tensor)\n",
                "        \n",
                "        return torch.nn.functional.normalize(embedding, p=2, dim=1).cpu().numpy().flatten()\n",
                "\n",
                "    def _extract_batch_embeddings(self, frame, detections):\n",
                "        \"\"\"Extracts embeddings for a batch of detections, grouped by class.\"\"\"\n",
                "        grouped_dets = {}\n",
                "        for i, det in enumerate(detections):\n",
                "            cls_name = det['class_name']\n",
                "            if cls_name in self.reid_models:\n",
                "                grouped_dets.setdefault(cls_name, []).append((i, det))\n",
                "\n",
                "        for cls_name, dets_with_indices in grouped_dets.items():\n",
                "            reid_config = self.reid_models[cls_name]\n",
                "            model, transform = reid_config['model'], reid_config['transform']\n",
                "            \n",
                "            rois_with_indices = []\n",
                "            for original_idx, det in dets_with_indices:\n",
                "                x1, y1, x2, y2 = det['x1'], det['y1'], det['x2'], det['y2']\n",
                "                roi = frame[y1:y2, x1:x2]\n",
                "                if roi.size > 0:\n",
                "                    roi_rgb = cv2.cvtColor(roi, cv2.COLOR_BGR2RGB)\n",
                "                    rois_with_indices.append({'roi': Image.fromarray(roi_rgb), 'idx': original_idx})\n",
                "            \n",
                "            if not rois_with_indices: continue\n",
                "\n",
                "            batch_tensor = torch.stack([transform(item['roi']) for item in rois_with_indices]).to(self.device)\n",
                "            with torch.no_grad():\n",
                "                batch_embeddings = model(batch_tensor)\n",
                "            \n",
                "            normalized_embeddings = torch.nn.functional.normalize(batch_embeddings, p=2, dim=1).cpu().numpy()\n",
                "\n",
                "            for item, emb in zip(rois_with_indices, normalized_embeddings):\n",
                "                detections[item['idx']]['embedding'] = emb\n",
                "\n",
                "    def process_frame(self, frame):\n",
                "        \"\"\"Main processing function for each frame.\"\"\"\n",
                "        self._apply_boundary_conditions(frame.shape)\n",
                "\n",
                "        newly_lost = [t for t in self.tracked_objects if t['lost_frames'] >= self.max_lost_frames]\n",
                "        for t in newly_lost:\n",
                "            t['state'] = 'LOST'\n",
                "            self.lost_tracks.append(t)\n",
                "        self.tracked_objects = [t for t in self.tracked_objects if t['lost_frames'] < self.max_lost_frames]\n",
                "\n",
                "        if self.use_kalman: self._predict_phase()\n",
                "        self._update_phase(frame)\n",
                "        \n",
                "        if self.frame_idx % self.detect_interval == 0:\n",
                "            self._match_and_update_phase(frame)\n",
                "\n",
                "        annotated_frame = self._drawing_phase(frame)\n",
                "        self.frame_idx += 1\n",
                "        return annotated_frame\n",
                "    \n",
                "    def _apply_boundary_conditions(self, frame_shape):\n",
                "        if not self.tracked_objects: return\n",
                "        \n",
                "        bboxes = np.array([t['bbox'] for t in self.tracked_objects])\n",
                "        visibility = self._get_box_visibility(bboxes, frame_shape)\n",
                "        \n",
                "        for i, track in enumerate(self.tracked_objects):\n",
                "            if visibility[i] < 0.5:\n",
                "                track['lost_frames'] = self.max_lost_frames\n",
                "\n",
                "    def _predict_phase(self):\n",
                "        for obj in self.tracked_objects:\n",
                "            obj['kf'].predict()\n",
                "            predicted_state = obj['kf'].statePost\n",
                "            cx, cy, w, h = predicted_state[:4]\n",
                "            obj['bbox'] = (int(cx - w/2), int(cy - h/2), int(cx + w/2), int(cy + h/2))\n",
                "\n",
                "    def _update_phase(self, frame):\n",
                "        for obj in self.tracked_objects:\n",
                "            if obj['state'] == 'OCCLUDED': continue\n",
                "            success, bbox = obj['tracker'].update(frame)\n",
                "            if success:\n",
                "                x1, y1, w, h = [int(v) for v in bbox]\n",
                "                obj['bbox'] = (x1, y1, x1 + w, y1 + h)\n",
                "                obj['lost_frames'] = 0 # Reset lost counter on successful short-term track\n",
                "                if self.use_kalman:\n",
                "                    measurement = np.array([x1 + w/2, y1 + h/2, w, h], dtype=np.float32)\n",
                "                    obj['kf'].correct(measurement)\n",
                "            else:\n",
                "                obj['lost_frames'] += 1\n",
                "\n",
                "    def _match_and_update_phase(self, frame):\n",
                "        detections = self.detect(frame)\n",
                "        if not detections: return\n",
                "        self._extract_batch_embeddings(frame, detections)\n",
                "        \n",
                "        # --- Stage 1: Match Active Tracks with Detections ---\n",
                "        if self.tracked_objects:\n",
                "            cost_matrix = self._build_cost_matrix(self.tracked_objects, detections)\n",
                "            track_indices, det_indices = linear_sum_assignment(cost_matrix)\n",
                "            \n",
                "            matched_track_indices = set()\n",
                "            matched_det_indices = set()\n",
                "            for t_idx, d_idx in zip(track_indices, det_indices):\n",
                "                if cost_matrix[t_idx, d_idx] < self.match_cost_threshold:\n",
                "                    self._update_matched_track(frame, self.tracked_objects[t_idx], detections[d_idx])\n",
                "                    matched_track_indices.add(t_idx)\n",
                "                    matched_det_indices.add(d_idx)\n",
                "            \n",
                "            unmatched_track_indices = set(range(len(self.tracked_objects))) - matched_track_indices\n",
                "            for t_idx in unmatched_track_indices:\n",
                "                self._handle_unmatched_track(t_idx, matched_track_indices)\n",
                "        else:\n",
                "            matched_det_indices = set()\n",
                "        \n",
                "        # --- Stage 2: Re-identify Lost Tracks with Unmatched Detections ---\n",
                "        unmatched_dets = [d for i, d in enumerate(detections) if i not in matched_det_indices]\n",
                "        if unmatched_dets and self.lost_tracks:\n",
                "            reid_cost_matrix = self._build_cost_matrix(list(self.lost_tracks), unmatched_dets, only_appearance=True)\n",
                "            lost_indices, reid_det_indices = linear_sum_assignment(reid_cost_matrix)\n",
                "\n",
                "            revived_lost_indices = set()\n",
                "            for lt_idx, d_idx in zip(lost_indices, reid_det_indices):\n",
                "                if reid_cost_matrix[lt_idx, d_idx] < self.reid_cost_threshold:\n",
                "                    revived_track = self.lost_tracks[lt_idx]\n",
                "                    detection = unmatched_dets[d_idx]\n",
                "                    \n",
                "                    self._update_matched_track(frame, revived_track, detection)\n",
                "                    self.tracked_objects.append(revived_track)\n",
                "                    revived_lost_indices.add(lt_idx)\n",
                "            \n",
                "            self.lost_tracks = deque([t for i, t in enumerate(self.lost_tracks) if i not in revived_lost_indices], maxlen=self.lost_tracks.maxlen)\n",
                "    \n",
                "    def _build_cost_matrix(self, tracks, detections, only_appearance=False):\n",
                "        \"\"\"Builds the cost matrix using vectorized GPU operations.\"\"\"\n",
                "        num_tracks = len(tracks)\n",
                "        num_dets = len(detections)\n",
                "        if num_tracks == 0 or num_dets == 0:\n",
                "            return np.empty((num_tracks, num_dets))\n",
                "\n",
                "        # --- Prepare data as tensors on the GPU ---\n",
                "        track_embeddings = torch.tensor(\n",
                "            np.array([t['embedding_gallery'][-1] for t in tracks if t['embedding_gallery']]),\n",
                "            device=self.device, dtype=torch.float32\n",
                "        )\n",
                "        det_embeddings = torch.tensor(\n",
                "            np.array([d['embedding'] for d in detections if 'embedding' in d]),\n",
                "            device=self.device, dtype=torch.float32\n",
                "        )\n",
                "\n",
                "        # --- Vectorized Appearance Cost (Cosine Distance on GPU) ---\n",
                "        # 1 - cosine_similarity = cosine distance\n",
                "        app_cost = 1 - (track_embeddings @ det_embeddings.T)\n",
                "        \n",
                "        if only_appearance:\n",
                "            return app_cost.cpu().numpy()\n",
                "\n",
                "        # --- Vectorized IoU Cost (on GPU) ---\n",
                "        track_bboxes = torch.tensor([t['bbox'] for t in tracks], device=self.device, dtype=torch.float32)\n",
                "        det_bboxes = torch.tensor(\n",
                "            [[d['x1'], d['y1'], d['x2'], d['y2']] for d in detections], \n",
                "            device=self.device, dtype=torch.float32\n",
                "        )\n",
                "        iou_matrix = self._calculate_iou(track_bboxes, det_bboxes)\n",
                "        iou_cost = 1 - iou_matrix\n",
                "\n",
                "        # --- Vectorized Class Mismatch Mask (on GPU) ---\n",
                "        track_classes = np.array([t['class_name'] for t in tracks])\n",
                "        det_classes = np.array([d['class_name'] for d in detections])\n",
                "        mismatch_mask = torch.tensor(track_classes[:, None] != det_classes, device=self.device)\n",
                "        \n",
                "        # --- Combine Costs ---\n",
                "        cost_matrix = (self.appearance_weight * app_cost) + ((1 - self.appearance_weight) * iou_cost)\n",
                "        cost_matrix[mismatch_mask] = 1e6  # Invalidate non-matching classes\n",
                "        cost_matrix[iou_matrix < self.iou_gating_threshold] = 1e6 # Apply IoU gating\n",
                "\n",
                "        return cost_matrix.cpu().numpy()\n",
                "\n",
                "    def _update_matched_track(self, frame, track, det):\n",
                "        x1, y1, x2, y2 = det['x1'], det['y1'], det['x2'], det['y2']\n",
                "        w, h = x2 - x1, y2 - y1\n",
                "        \n",
                "        track['bbox'] = (x1, y1, x2, y2)\n",
                "        track['lost_frames'] = 0\n",
                "        track['state'] = 'CONFIRMED'\n",
                "            \n",
                "        new_cv_tracker = self.tracker_constructors[self.tracker_type]()\n",
                "        new_cv_tracker.init(frame, (x1, y1, w, h))\n",
                "        track['tracker'] = new_cv_tracker\n",
                "\n",
                "        if 'embedding' in det and det['embedding'] is not None:\n",
                "            track['embedding_gallery'].append(det['embedding'])\n",
                "            \n",
                "        if self.use_kalman:\n",
                "            measurement = np.array([x1 + w/2, y1 + h/2, w, h], dtype=np.float32)\n",
                "            track['kf'].correct(measurement)\n",
                "            track['kf'].statePost[4:] = 0; track['kf'].statePre[4:] = 0\n",
                "\n",
                "    def _handle_unmatched_track(self, t_idx, matched_track_indices):\n",
                "        track = self.tracked_objects[int(t_idx)]\n",
                "        # Check for occlusion against currently tracked (matched) objects\n",
                "        if matched_track_indices:\n",
                "            matched_bboxes = np.array([self.tracked_objects[int(m_idx)]['bbox'] for m_idx in matched_track_indices])\n",
                "            ious = self._calculate_iou_numpy(np.array([track['bbox']]), matched_bboxes)\n",
                "            if np.max(ious) > self.occlusion_iou_threshold:\n",
                "                track['state'] = 'OCCLUDED'\n",
                "                return\n",
                "\n",
                "        track['lost_frames'] += 1\n",
                "        if track['state'] == 'OCCLUDED': track['state'] = 'CONFIRMED'\n",
                "\n",
                "    def add_manual_track(self, frame, bbox, class_name):\n",
                "        if class_name not in self.reid_models:\n",
                "            print(f\"Warning: No Re-ID model for class '{class_name}'.\")\n",
                "            return\n",
                "\n",
                "        x1, y1, x2, y2 = [int(c) for c in bbox]\n",
                "        w = x2 - x1\n",
                "        h = y2 - y1\n",
                "        if w <= 0 or h <= 0:\n",
                "            print(\"Warning: Invalid bounding box dimensions.\")\n",
                "            return\n",
                "\n",
                "        new_track = {\n",
                "            'id': self.next_track_id, 'class_name': class_name,\n",
                "            'bbox': (x1, y1, x2, y2), 'lost_frames': 0, 'state': 'CONFIRMED', \n",
                "            'embedding_gallery': deque(maxlen=20),\n",
                "            'reid_config': self.reid_models[class_name]\n",
                "        }\n",
                "\n",
                "        tracker = self.tracker_constructors[self.tracker_type]()\n",
                "        tracker.init(frame, (x1, y1, w, h))\n",
                "        new_track['tracker'] = tracker\n",
                "\n",
                "        embedding = self._extract_embedding(frame, new_track['bbox'], new_track)\n",
                "        if embedding is not None: new_track['embedding_gallery'].append(embedding)\n",
                "\n",
                "        if self.use_kalman:\n",
                "            new_track['kf'] = self._create_kalman_filter()\n",
                "            cx, cy = x1 + w/2, y1 + h/2\n",
                "            new_track['kf'].statePost = np.array([cx, cy, w, h, 0, 0, 0, 0], dtype=np.float32)\n",
                "        \n",
                "        self.tracked_objects.append(new_track)\n",
                "        self.next_track_id += 1\n",
                "                                \n",
                "    def _drawing_phase(self, frame):\n",
                "        frame_copy = frame.copy()\n",
                "        if not self.tracked_objects: return frame_copy\n",
                "\n",
                "        bboxes = np.array([obj['bbox'] for obj in self.tracked_objects])\n",
                "        visibilities = self._get_box_visibility(bboxes, frame.shape)\n",
                "\n",
                "        for i, obj in enumerate(self.tracked_objects):\n",
                "            if obj['state'] == 'OCCLUDED' or visibilities[i] < 0.7: continue\n",
                "            \n",
                "            color = (0, 255, 0)\n",
                "            x1, y1, x2, y2 = [int(c) for c in obj['bbox']]\n",
                "            label = f\"{obj['class_name']} {obj['id']}\"\n",
                "            cv2.rectangle(frame_copy, (x1, y1), (x2, y2), color, 2)\n",
                "            cv2.putText(frame_copy, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
                "        return frame_copy\n",
                "\n",
                "    # --- Utility Methods ---\n",
                "    def detect(self, frame):\n",
                "        results = self.model(frame, verbose=False)[0]\n",
                "        detections = []\n",
                "        for box in results.boxes:\n",
                "            conf = box.conf[0].item()\n",
                "            if conf > self.conf_threshold:\n",
                "                class_name = self.model.names[int(box.cls[0].item())]\n",
                "                if self.track_classes and class_name not in self.track_classes: continue\n",
                "                coords = box.xyxy[0].tolist()\n",
                "                detections.append({\n",
                "                    'class_name': class_name, 'x1': int(coords[0]), 'y1': int(coords[1]),\n",
                "                    'x2': int(coords[2]), 'y2': int(coords[3]), 'conf': conf\n",
                "                })\n",
                "        return detections\n",
                "    \n",
                "    def _get_box_visibility(self, bboxes, frame_shape):\n",
                "        frame_h, frame_w = frame_shape[:2]\n",
                "        x1, y1, x2, y2 = bboxes[:, 0], bboxes[:, 1], bboxes[:, 2], bboxes[:, 3]\n",
                "        \n",
                "        total_area = (x2 - x1) * (y2 - y1)\n",
                "        total_area[total_area <= 0] = 1e-6\n",
                "\n",
                "        visible_x1, visible_y1 = np.maximum(x1, 0), np.maximum(y1, 0)\n",
                "        visible_x2, visible_y2 = np.minimum(x2, frame_w), np.minimum(y2, frame_h)\n",
                "        \n",
                "        visible_w = np.maximum(0, visible_x2 - visible_x1)\n",
                "        visible_h = np.maximum(0, visible_y2 - visible_y1)\n",
                "        visible_area = visible_w * visible_h\n",
                "        return visible_area / total_area\n",
                "\n",
                "    def _calculate_iou(self, bboxes1, bboxes2):\n",
                "        \"\"\"Calculates IoU for two sets of bounding boxes using PyTorch tensors.\"\"\"\n",
                "        # Broadcasting to get intersection coordinates\n",
                "        xA = torch.maximum(bboxes1[:, 0].unsqueeze(1), bboxes2[:, 0])\n",
                "        yA = torch.maximum(bboxes1[:, 1].unsqueeze(1), bboxes2[:, 1])\n",
                "        xB = torch.minimum(bboxes1[:, 2].unsqueeze(1), bboxes2[:, 2])\n",
                "        yB = torch.minimum(bboxes1[:, 3].unsqueeze(1), bboxes2[:, 3])\n",
                "\n",
                "        interArea = torch.clamp(xB - xA, min=0) * torch.clamp(yB - yA, min=0)\n",
                "\n",
                "        boxAArea = (bboxes1[:, 2] - bboxes1[:, 0]) * (bboxes1[:, 3] - bboxes1[:, 1])\n",
                "        boxBArea = (bboxes2[:, 2] - bboxes2[:, 0]) * (bboxes2[:, 3] - bboxes2[:, 1])\n",
                "\n",
                "        iou = interArea / (boxAArea.unsqueeze(1) + boxBArea - interArea + 1e-6)\n",
                "        return iou\n",
                "\n",
                "    def _calculate_iou_numpy(self, bboxes1, bboxes2):\n",
                "        \"\"\"A NumPy version for CPU-bound occlusion checks.\"\"\"\n",
                "        xA = np.maximum(bboxes1[:, 0][:, np.newaxis], bboxes2[:, 0])\n",
                "        yA = np.maximum(bboxes1[:, 1][:, np.newaxis], bboxes2[:, 1])\n",
                "        xB = np.minimum(bboxes1[:, 2][:, np.newaxis], bboxes2[:, 2])\n",
                "        yB = np.minimum(bboxes1[:, 3][:, np.newaxis], bboxes2[:, 3])\n",
                "        interArea = np.maximum(0, xB - xA) * np.maximum(0, yB - yA)\n",
                "        boxAArea = (bboxes1[:, 2] - bboxes1[:, 0]) * (bboxes1[:, 3] - bboxes1[:, 1])\n",
                "        boxBArea = (bboxes2[:, 2] - bboxes2[:, 0]) * (bboxes2[:, 3] - bboxes2[:, 1])\n",
                "        return interArea / (boxAArea[:, np.newaxis] + boxBArea - interArea + 1e-6)\n",
                "    \n",
                "    def _create_kalman_filter(self):\n",
                "        kf = cv2.KalmanFilter(8, 4)\n",
                "        kf.transitionMatrix = np.array([[1,0,0,0,1,0,0,0],[0,1,0,0,0,1,0,0],[0,0,1,0,0,0,1,0],[0,0,0,1,0,0,0,1],\n",
                "                                         [0,0,0,0,1,0,0,0],[0,0,0,0,0,1,0,0],[0,0,0,0,0,0,1,0],[0,0,0,0,0,0,0,1]], np.float32)\n",
                "        kf.measurementMatrix = np.array([[1,0,0,0,0,0,0,0],[0,1,0,0,0,0,0,0],[0,0,1,0,0,0,0,0],[0,0,0,1,0,0,0,0]], np.float32)\n",
                "        kf.processNoiseCov = np.eye(8, dtype=np.float32) * 0.03\n",
                "        kf.processNoiseCov[4:, 4:] *= 10\n",
                "        kf.measurementNoiseCov = np.eye(4, dtype=np.float32) * 0.1\n",
                "        return kf"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "4eb94019",
            "metadata": {},
            "source": [
                "## VideoPlayer class"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c0e49fee",
            "metadata": {},
            "outputs": [],
            "source": [
                "class VideoPlayer:\n",
                "    def __init__(self, source, target_fps=24, size_multiplier=1.0, window_title=\"Video Playback\"):\n",
                "        self.window_title = window_title\n",
                "        self.source = source\n",
                "        self.target_fps = target_fps\n",
                "\n",
                "        if os.path.isdir(self.source):\n",
                "            self.source_type = 'images'\n",
                "            image_extensions = ('.jpg', '.jpeg', '.png', '.bmp', '.tif', '.tiff')\n",
                "            self.image_files = sorted([os.path.join(self.source, f) for f in os.listdir(self.source) if f.lower().endswith(image_extensions)])\n",
                "            if not self.image_files: raise ValueError(\"Source directory contains no supported image files.\")\n",
                "            first_frame = cv2.imread(self.image_files[0])\n",
                "            if first_frame is None: raise IOError(f\"Could not read the first image: {self.image_files[0]}\")\n",
                "            self.frame_height, self.frame_width = first_frame.shape[:2]\n",
                "            self.cap = None\n",
                "            self.original_fps = 30\n",
                "        else:\n",
                "            self.source_type = 'video'\n",
                "            self.cap = cv2.VideoCapture(self.source)\n",
                "            if not self.cap.isOpened(): raise IOError(f\"Could not open video file: {self.source}\")\n",
                "            self.frame_width = int(self.cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
                "            self.frame_height = int(self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
                "            self.original_fps = self.cap.get(cv2.CAP_PROP_FPS)\n",
                "\n",
                "        if self.target_fps == 0:\n",
                "            self.target_fps = self.original_fps\n",
                "            print(f\"Target FPS set to 0. Using original video FPS: {self.target_fps:.2f}\")\n",
                "\n",
                "        # --- New: Adaptive UI Scaling Factor ---\n",
                "        self.ui_scale_factor = max(0.5, min(self.frame_height, 2200.0) / 1080.0) # Base scale on 1080p, with a minimum\n",
                "\n",
                "        self.total_processing_time = 0.0\n",
                "        self.processed_frame_count = 0\n",
                "        self.state = 'INITIALIZING'\n",
                "        self.selectable_detections, self.user_selections = [], []\n",
                "        self.is_drawing_roi, self.show_help = False, True\n",
                "        self.roi_start_point, self.roi_end_point, self.new_manual_box = None, None, None\n",
                "        \n",
                "        self.YOLO_CLASSES = {\n",
                "            0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', \n",
                "            5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light',\n",
                "            10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench',\n",
                "            14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow',\n",
                "            20: 'other'\n",
                "        }\n",
                "        \n",
                "        cv2.namedWindow(self.window_title, cv2.WINDOW_NORMAL)\n",
                "        cv2.resizeWindow(self.window_title, int(self.frame_width * size_multiplier), int(self.frame_height * size_multiplier))\n",
                "        \n",
                "        cv2.setMouseCallback(self.window_title, self._mouse_callback)\n",
                "        print(\"--- Video Player Initialized for Interactive Tracking ---\")\n",
                "\n",
                "    def _mouse_callback(self, event, x, y, flags, param):\n",
                "        if self.state != 'PAUSED_FOR_SELECTION': return\n",
                "\n",
                "        if event == cv2.EVENT_LBUTTONDOWN:\n",
                "            self.is_drawing_roi = True\n",
                "            self.roi_start_point, self.roi_end_point = (x, y), (x, y)\n",
                "        elif event == cv2.EVENT_MOUSEMOVE:\n",
                "            if self.is_drawing_roi: self.roi_end_point = (x, y)\n",
                "        elif event == cv2.EVENT_LBUTTONUP:\n",
                "            if self.is_drawing_roi:\n",
                "                self.is_drawing_roi = False\n",
                "                if self.roi_end_point and self.roi_start_point and abs(self.roi_start_point[0] - self.roi_end_point[0]) > 5:\n",
                "                    x1, y1, x2, y2 = self.roi_start_point[0], self.roi_start_point[1], self.roi_end_point[0], self.roi_end_point[1]\n",
                "                    self.new_manual_box = (min(x1, x2), min(y1, y2), max(x1, x2), max(y1, y2))\n",
                "                self.roi_start_point, self.roi_end_point = None, None\n",
                "        elif event == cv2.EVENT_RBUTTONDOWN:\n",
                "            removed_selection = False\n",
                "            for i, sel in reversed(list(enumerate(self.user_selections))):\n",
                "                bbox = sel.get('bbox') or (sel['x1'], sel['y1'], sel['x2'], sel['y2'])\n",
                "                if bbox[0] < x < bbox[2] and bbox[1] < y < bbox[3]:\n",
                "                    removed_item = self.user_selections.pop(i)\n",
                "                    if 'x1' in removed_item: self.selectable_detections.append(removed_item)\n",
                "                    removed_selection = True\n",
                "                    break\n",
                "            if not removed_selection:\n",
                "                for i, det in reversed(list(enumerate(self.selectable_detections))):\n",
                "                    if det['x1'] < x < det['x2'] and det['y1'] < y < det['y2']:\n",
                "                        self.user_selections.append(self.selectable_detections.pop(i))\n",
                "                        break\n",
                "\n",
                "    def _draw_pause_menu(self, frame):\n",
                "        s = self.ui_scale_factor\n",
                "        # Scaled values for fonts and layout\n",
                "        bg_height = int(240 * s)\n",
                "        title_scale, head_scale, text_scale = 1.8 * s, 1.0 * s, 0.9 * s\n",
                "        thick_main, thick_sub = max(1, int(3 * s)), max(1, int(2 * s))\n",
                "\n",
                "        overlay = frame.copy()\n",
                "        cv2.rectangle(overlay, (0, 0), (frame.shape[1], bg_height), (0, 0, 0), -1)\n",
                "        frame = cv2.addWeighted(overlay, 0.7, frame, 0.3, 0)\n",
                "        \n",
                "        cv2.putText(frame, \"PAUSED - SELECTION MODE\", (int(25*s), int(60*s)), cv2.FONT_HERSHEY_TRIPLEX, title_scale, (0, 255, 255), thick_main)\n",
                "        cv2.putText(frame, \"Mouse Controls:\", (int(25*s), int(115*s)), cv2.FONT_HERSHEY_SIMPLEX, head_scale, (255, 255, 255), thick_main)\n",
                "        cv2.putText(frame, \"- Left-Click & Drag: Draw a new box to track\", (int(35*s), int(145*s)), cv2.FONT_HERSHEY_SIMPLEX, text_scale, (255, 255, 255), thick_sub)\n",
                "        cv2.putText(frame, \"- Right-Click: Select (Red) / Deselect (Green)\", (int(35*s), int(170*s)), cv2.FONT_HERSHEY_SIMPLEX, text_scale, (255, 255, 255), thick_sub)\n",
                "        cv2.putText(frame, \"Keyboard: C: Confirm | H: Toggle Help | Space: Pause | Q: Quit\", (int(25*s), int(210*s)), cv2.FONT_HERSHEY_SIMPLEX, text_scale, (255, 255, 255), thick_sub)\n",
                "        return frame\n",
                "    \n",
                "    def _get_numeric_input(self, frame):\n",
                "        s = self.ui_scale_factor\n",
                "        # Scaled values for fonts and layout\n",
                "        title_scale, text_scale = 1.8 * s, 1.2 * s\n",
                "        thick_main, thick_sub = max(1, int(4 * s)), max(1, int(3 * s))\n",
                "        y_offset_start, y_offset_inc = int(120*s), int(45*s)\n",
                "\n",
                "        num_input = \"\"\n",
                "        while True:\n",
                "            frame_copy, overlay = frame.copy(), frame.copy()\n",
                "            cv2.rectangle(overlay, (0, 0), (frame_copy.shape[1], frame_copy.shape[0]), (0, 0, 0), -1)\n",
                "            frame_copy = cv2.addWeighted(overlay, 0.85, frame_copy, 0.15, 0)\n",
                "            \n",
                "            current_selection_id = -1\n",
                "            try:\n",
                "                if num_input: current_selection_id = int(num_input)\n",
                "            except ValueError: pass\n",
                "\n",
                "            cv2.putText(frame_copy, \"Enter Class ID & Press Enter:\", (int(50*s), int(65*s)), cv2.FONT_HERSHEY_TRIPLEX, title_scale, (0, 255, 255), thick_main)\n",
                "            y_offset = y_offset_start\n",
                "            for i, name in self.YOLO_CLASSES.items():\n",
                "                if y_offset < frame.shape[0] - 30:\n",
                "                    color = (0, 255, 0) if i == current_selection_id else (255, 255, 255)\n",
                "                    thickness = thick_main if i == current_selection_id else thick_sub\n",
                "                    cv2.putText(frame_copy, f\"{i}: {name}\", (int(50*s), y_offset), cv2.FONT_HERSHEY_SIMPLEX, text_scale, color, thickness)\n",
                "                    y_offset += y_offset_inc\n",
                "            \n",
                "            cv2.imshow(self.window_title, frame_copy)\n",
                "            key = cv2.waitKey(0)\n",
                "            if key == 13:\n",
                "                try:\n",
                "                    if num_input and int(num_input) in self.YOLO_CLASSES: return int(num_input)\n",
                "                    else: print(f\"Error: Invalid ID. Please try again.\"); num_input = \"\"\n",
                "                except ValueError: print(\"Error: Invalid input.\"); num_input = \"\"\n",
                "            elif key == 8: num_input = num_input[:-1]\n",
                "            elif ord('0') <= key <= ord('9'): num_input += chr(key)\n",
                "            elif key == 27: return None\n",
                "\n",
                "    def play(self, tracker):\n",
                "        frame_idx = -1 # Start at -1 to handle loop logic correctly\n",
                "        temp_frame = None\n",
                "\n",
                "        while True:\n",
                "            loop_start_time = time.perf_counter()\n",
                "\n",
                "            # --- Unified Frame Loading ---\n",
                "            ret, frame = False, None\n",
                "            if self.state in ['INITIALIZING', 'PLAYING']:\n",
                "                frame_idx += 1\n",
                "                if self.source_type == 'video':\n",
                "                    ret, frame = self.cap.read()\n",
                "                elif self.source_type == 'images':\n",
                "                    if frame_idx < len(self.image_files):\n",
                "                        frame = cv2.imread(self.image_files[frame_idx])\n",
                "                        ret = frame is not None\n",
                "                if ret: temp_frame = frame.copy()\n",
                "                else: break\n",
                "            else: # Paused state\n",
                "                frame = temp_frame.copy()\n",
                "\n",
                "            # --- State Machine ---\n",
                "            display_frame = frame.copy()\n",
                "            if self.state == 'INITIALIZING' and frame_idx >= 1:\n",
                "                self.state = 'PAUSED_FOR_SELECTION'\n",
                "                self.selectable_detections = tracker.detect(display_frame)\n",
                "            elif self.state == 'PLAYING':\n",
                "                display_frame = tracker.process_frame(display_frame)\n",
                "            elif self.state == 'PAUSED_FOR_SELECTION':\n",
                "                if self.show_help: display_frame = self._draw_pause_menu(display_frame)\n",
                "                for det in self.selectable_detections: cv2.rectangle(display_frame, (det['x1'], det['y1']), (det['x2'], det['y2']), (0, 0, 255), 2)\n",
                "                for sel in self.user_selections:\n",
                "                    bbox = sel.get('bbox') or (sel['x1'], sel['y1'], sel['x2'], sel['y2'])\n",
                "                    cv2.rectangle(display_frame, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (0, 255, 0), 3)\n",
                "                if self.is_drawing_roi and self.roi_start_point and self.roi_end_point:\n",
                "                    cv2.rectangle(display_frame, self.roi_start_point, self.roi_end_point, (255, 255, 0), 2)\n",
                "                if self.new_manual_box:\n",
                "                    class_id = self._get_numeric_input(display_frame)\n",
                "                    if class_id is not None:\n",
                "                        self.user_selections.append({'bbox': self.new_manual_box, 'class_name': self.YOLO_CLASSES[class_id]})\n",
                "                    self.new_manual_box = None\n",
                "            \n",
                "            # --- Live FPS and Final Display ---\n",
                "            processing_time = time.perf_counter() - loop_start_time\n",
                "            live_fps = 1.0 / processing_time if processing_time > 0 else float('inf')\n",
                "            if self.state != 'PAUSED_FOR_SELECTION':\n",
                "                self.total_processing_time += processing_time\n",
                "                self.processed_frame_count += 1\n",
                "            \n",
                "            s = self.ui_scale_factor\n",
                "            cv2.putText(display_frame, f\"FPS: {live_fps:.1f}\", (int(20*s), int(40*s)), cv2.FONT_HERSHEY_SIMPLEX, 1.2*s, (0, 255, 0), max(1, int(2*s)))\n",
                "            cv2.imshow(self.window_title, display_frame)\n",
                "\n",
                "            wait_ms = 1\n",
                "            if self.target_fps != -1 and self.state == 'PLAYING':\n",
                "                target_duration = 1.0 / self.target_fps\n",
                "                if (delay_needed := target_duration - processing_time) > 0: wait_ms = int(delay_needed * 1000)\n",
                "            elif self.state == 'PAUSED_FOR_SELECTION': wait_ms = 20\n",
                "            \n",
                "            key = cv2.waitKey(wait_ms) & 0xFF\n",
                "            if key == ord('q'): break\n",
                "            elif key == ord('h'): self.show_help = not self.show_help\n",
                "            elif key == 32 and self.state == 'PLAYING':\n",
                "                self.state = 'PAUSED_FOR_SELECTION'\n",
                "                self.selectable_detections = tracker.detect(frame)\n",
                "                self.user_selections = list(tracker.tracked_objects)\n",
                "            elif key == ord('c') and self.state == 'PAUSED_FOR_SELECTION':\n",
                "                tracker.tracked_objects, tracker.next_track_id = [], 0\n",
                "                for sel in self.user_selections:\n",
                "                    bbox = sel.get('bbox') or (sel['x1'], sel['y1'], sel['x2'], sel['y2'])\n",
                "                    tracker.add_manual_track(temp_frame, bbox, sel['class_name'])\n",
                "                self.selectable_detections, self.user_selections, self.state = [], [], 'PLAYING'\n",
                "\n",
                "        if self.processed_frame_count > 0:\n",
                "            avg_fps = self.processed_frame_count / self.total_processing_time\n",
                "            print(f\"\\n--- Playback Finished ---\\nAverage Processing FPS: {avg_fps:.2f}\\n-------------------------\")\n",
                "        \n",
                "        self.release()\n",
                "\n",
                "    def release(self):\n",
                "        print(\"Releasing resources...\")\n",
                "        if self.cap and self.cap.isOpened(): self.cap.release()\n",
                "        cv2.destroyAllWindows()\n",
                "        for _ in range(5): cv2.waitKey(1)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ff4ec198",
            "metadata": {},
            "source": [
                "## Realtime Playback"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3adef366",
            "metadata": {},
            "outputs": [],
            "source": [
                "# VIDEO_PATH = './assets/OTB100/human2/img/'\n",
                "VIDEO_PATH = './assets/footage/person4.mp4'\n",
                "MODEL_PATH = './yolo11n.pt'\n",
                "TARGET_FPS = 0 # 0: standard video fps / -1: max fps\n",
                "WINDOW_SIZE = .75\n",
                "\n",
                "try:\n",
                "    tracker = TrackingSystem(\n",
                "        tracker_type='ckcf',\n",
                "        track_classes=['person', 'car'],\n",
                "        use_kalman=False,\n",
                "        base_detect_interval=8000\n",
                "    )\n",
                "\n",
                "    player = VideoPlayer(\n",
                "        source=VIDEO_PATH,\n",
                "        target_fps=TARGET_FPS,\n",
                "        size_multiplier=WINDOW_SIZE,\n",
                "        window_title=\"Realtime Player\"\n",
                "    )\n",
                "    \n",
                "    player.play(tracker)\n",
                "except IOError as e:\n",
                "    print(e)\n",
                "except Exception as e:\n",
                "    print(f\"Error: {e}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "signalenv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
