{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23c8269e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vp\\AppData\\Local\\Temp\\ipykernel_24352\\2358188938.py:57: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  return int(pred[0]), int(pred[1]), int(pred[4]), int(pred[5])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 82\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ret:\n\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m preprocessed = \u001b[43mpreprocess_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m results = model(frame, verbose=\u001b[38;5;28;01mFalse\u001b[39;00m)[\u001b[32m0\u001b[39m]\n\u001b[32m     84\u001b[39m detections = []\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 63\u001b[39m, in \u001b[36mpreprocess_frame\u001b[39m\u001b[34m(frame)\u001b[39m\n\u001b[32m     61\u001b[39m f = np.fft.fft2(gray)\n\u001b[32m     62\u001b[39m fshift = np.fft.fftshift(f)\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m magnitude_spectrum = \u001b[32m20\u001b[39m * \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mabs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfshift\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cv2.normalize(magnitude_spectrum, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[32m0\u001b[39m, \u001b[32m255\u001b[39m, cv2.NORM_MINMAX).astype(np.uint8)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# object_tracking_kalman.py\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from filterpy.kalman import KalmanFilter\n",
    "from ultralytics import YOLO\n",
    "import itertools\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from collections import deque\n",
    "\n",
    "class SingleObjectTracker:\n",
    "    def __init__(self, tracker_id, bbox, frame):\n",
    "        self.id = tracker_id\n",
    "        self.kf = KalmanFilter(dim_x=6, dim_z=4)\n",
    "        self.kf.F = np.array([[1, 0, 1, 0, 0, 0],\n",
    "                              [0, 1, 0, 1, 0, 0],\n",
    "                              [0, 0, 1, 0, 0, 0],\n",
    "                              [0, 0, 0, 1, 0, 0],\n",
    "                              [0, 0, 0, 0, 1, 0],\n",
    "                              [0, 0, 0, 0, 0, 1]])\n",
    "\n",
    "        self.kf.H = np.array([[1, 0, 0, 0, 0, 0],\n",
    "                              [0, 1, 0, 0, 0, 0],\n",
    "                              [0, 0, 0, 0, 1, 0],\n",
    "                              [0, 0, 0, 0, 0, 1]])\n",
    "\n",
    "        self.kf.R *= 10\n",
    "        self.kf.P *= 10\n",
    "        self.kf.Q *= 0.01\n",
    "        x, y, w, h = bbox\n",
    "        self.kf.x[:6] = np.array([x, y, 0, 0, w, h]).reshape((6, 1))\n",
    "        self.missing_frames = 0\n",
    "        self.max_missing = 10\n",
    "        self.last_detection = bbox\n",
    "        self.color_signature = self.extract_color_signature(frame, bbox)\n",
    "\n",
    "    def extract_color_signature(self, frame, bbox):\n",
    "        x, y, w, h = bbox\n",
    "        roi = frame[y:y+h, x:x+w]\n",
    "        if roi.size == 0:\n",
    "            return np.array([0, 0, 0])\n",
    "        return np.mean(roi.reshape(-1, 3), axis=0)\n",
    "\n",
    "    def update(self, bbox=None, frame=None):\n",
    "        if bbox is not None:\n",
    "            x, y, w, h = bbox\n",
    "            self.kf.update(np.array([x, y, w, h]))\n",
    "            self.last_detection = bbox\n",
    "            if frame is not None:\n",
    "                self.color_signature = self.extract_color_signature(frame, bbox)\n",
    "            self.missing_frames = 0\n",
    "        else:\n",
    "            self.missing_frames += 1\n",
    "\n",
    "        self.kf.predict()\n",
    "        pred = self.kf.x\n",
    "        return int(pred[0]), int(pred[1]), int(pred[4]), int(pred[5])\n",
    "\n",
    "def preprocess_frame(frame):\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    f = np.fft.fft2(gray)\n",
    "    fshift = np.fft.fftshift(f)\n",
    "    magnitude_spectrum = 20 * np.log(np.abs(fshift) + 1)\n",
    "    return cv2.normalize(magnitude_spectrum, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "\n",
    "cap = cv2.VideoCapture(\"./assets/footage/person2.mp4\")\n",
    "model = YOLO(\"yolov8n.pt\")\n",
    "\n",
    "trackers = []\n",
    "id_counter = itertools.count()\n",
    "\n",
    "CONFIDENCE_THRESHOLD = 0.5\n",
    "MIN_AREA_THRESHOLD = 2000\n",
    "MAX_ASSIGN_DIST = 150\n",
    "COLOR_MATCH_THRESHOLD = 40\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    preprocessed = preprocess_frame(frame)\n",
    "    results = model(frame, verbose=False)[0]\n",
    "    detections = []\n",
    "\n",
    "    for det in results.boxes:\n",
    "        if det.conf[0] < CONFIDENCE_THRESHOLD:\n",
    "            continue\n",
    "        x1, y1, x2, y2 = map(int, det.xyxy[0])\n",
    "        w = x2 - x1\n",
    "        h = y2 - y1\n",
    "        area = w * h\n",
    "        if area < MIN_AREA_THRESHOLD:\n",
    "            continue\n",
    "        detections.append((x1, y1, w, h))\n",
    "\n",
    "    predicted_positions = [t.update() for t in trackers]\n",
    "    cost_matrix = []\n",
    "\n",
    "    for tx, ty, tw, th in predicted_positions:\n",
    "        row = []\n",
    "        for dx, dy, dw, dh in detections:\n",
    "            dist = np.hypot(dx - tx, dy - ty)\n",
    "            row.append(dist)\n",
    "        cost_matrix.append(row)\n",
    "\n",
    "    matched_det = set()\n",
    "    matched_trk = set()\n",
    "    if cost_matrix and detections:\n",
    "        cost_matrix = np.array(cost_matrix)\n",
    "        trk_inds, det_inds = linear_sum_assignment(cost_matrix)\n",
    "        for t, d in zip(trk_inds, det_inds):\n",
    "            if cost_matrix[t, d] < MAX_ASSIGN_DIST:\n",
    "                trackers[t].update(detections[d], frame)\n",
    "                matched_trk.add(t)\n",
    "                matched_det.add(d)\n",
    "\n",
    "    for i, tracker in enumerate(trackers):\n",
    "        if i not in matched_trk:\n",
    "            tracker.update(None)\n",
    "\n",
    "    # Match unassigned detections by color to lost trackers\n",
    "    lost_trackers = [t for t in trackers if t.missing_frames > 0 and t.missing_frames < t.max_missing]\n",
    "    for i, det in enumerate(detections):\n",
    "        if i in matched_det:\n",
    "            continue\n",
    "        x, y, w, h = det\n",
    "        roi = frame[y:y+h, x:x+w]\n",
    "        if roi.size == 0:\n",
    "            continue\n",
    "        det_color = np.mean(roi.reshape(-1, 3), axis=0)\n",
    "\n",
    "        for lost in lost_trackers:\n",
    "            color_dist = np.linalg.norm(lost.color_signature - det_color)\n",
    "            if color_dist < COLOR_MATCH_THRESHOLD:\n",
    "                lost.update(det, frame)\n",
    "                matched_det.add(i)\n",
    "                break\n",
    "\n",
    "    for i, det in enumerate(detections):\n",
    "        if i not in matched_det:\n",
    "            new_id = next(id_counter)\n",
    "            trackers.append(SingleObjectTracker(new_id, det, frame))\n",
    "\n",
    "    for tracker in trackers:\n",
    "        if tracker.missing_frames < tracker.max_missing:\n",
    "            x, y, w, h = tracker.update()\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, f\"ID {tracker.id}\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "    trackers = [t for t in trackers if t.missing_frames < t.max_missing]\n",
    "\n",
    "    cv2.imshow(\"Multi Object Tracking\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "signalenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
